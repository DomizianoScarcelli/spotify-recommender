{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/dev/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3wVgRfHYoOs",
        "outputId": "4007cddd-2403-4411-c4b1-f11ae95cf351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGHPv9OqY9MI"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from typing import Tuple\n",
        "from functools import reduce\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39857157-1889-47fd-b4d9-f47d3887ca50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88bb15d3-215a-4f1e-c553-8aeda073fb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=e939c7f73451d3c62d603447e97213f733b65ac2e689f8f991e891c6c0e82e9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b568cd33-8e4d-480a-a167-9967d5cc2249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432f4e1c-c791-4df2-cd27-465435b3d64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-08T10:36:29+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829fb859-bbf7-41da-c192-cfaf050a84ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://bf1c-35-231-212-214.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e63179-23fc-4566-9635-e3f8f352bf15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f8bfc5422c0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.app.startTime', '1686220359212'),\n",
              "  ('spark.driver.port', '44659'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.app.submitTime', '1686220358628'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.driver.host', '5bd51c57ed7e'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.app.id', 'local-1686220363250'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PcLYEWCRZ2Ns"
      },
      "outputs": [],
      "source": [
        "# slice_df = slice_df.limit(10_000).cache()\n",
        "NUM_PLAYLISTS = slice_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5zOoDDXEIBn"
      },
      "source": [
        "# Dataset Train-Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAhzcKNIQIR9"
      },
      "source": [
        "## Simple Train-Test split\n",
        "For the user-based and item-based collaborative filtering, the train-test split is done by splitting the songs inside each playlist with a ration of 80% train and 20% test. This means that the playlists are the same for the train and test, but the songs inside are different. In this way we can use the train test to recommend the songs, and use the test set to evaluate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qek8mLL4EK0M"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split as sklearn_split\n",
        "\n",
        "def train_test_split(playlist: Row) -> Tuple[Row, Row]:\n",
        "    train_rows, test_rows = sklearn_split(playlist.tracks, random_state=42)\n",
        "\n",
        "    playlist_train =  Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=train_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "\n",
        "    playlist_test = Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=test_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "    \n",
        "    return playlist_train, playlist_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3p9m_Jl6FDoA"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf, struct\n",
        "import shutil\n",
        "\n",
        "def divide_whole_dataset(playlist_df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
        "  train_test_split_udf = udf(train_test_split, returnType=ArrayType(StructType(playlist_df.schema.fields)))\n",
        "  divided_df = playlist_df.withColumn(\"divided\", train_test_split_udf(struct(*playlist_df.columns)))\n",
        "  train_test_df = divided_df.select(F.col('divided').getItem(0).alias('train'), F.col('divided').getItem(1).alias('test'))\n",
        "\n",
        "  train_df = train_test_df.select(\"train.*\")\n",
        "  test_df = train_test_df.select(\"test.*\")\n",
        "  return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YgXY1Gq1s7j4"
      },
      "outputs": [],
      "source": [
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "if os.path.exists(TRAIN_DF_PATH) and os.path.exists(TEST_DF_PATH):\n",
        "  train_df = spark.read.schema(playlist_schema).json(TRAIN_DF_PATH)\n",
        "  test_df = spark.read.schema(playlist_schema).json(TEST_DF_PATH)\n",
        "else:\n",
        "  # In order to avoid [PATH_ALREADY_EXISTS] errors. \n",
        "  if os.path.exists(TRAIN_DF_PATH):\n",
        "    shutil.rmtree(TRAIN_DF_PATH)\n",
        "  if os.path.exists(TEST_DF_PATH):\n",
        "    shutil.rmtree(TEST_DF_PATH)\n",
        "\n",
        "  train_df, test_df = divide_whole_dataset(slice_df)\n",
        "  train_df, test_df = train_df.cache(), test_df.cache()\n",
        "  train_df.write.mode(\"overwrite\").json(TRAIN_DF_PATH)\n",
        "  test_df.write.mode(\"overwrite\").json(TEST_DF_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pqje_w0Qk0C"
      },
      "source": [
        "## Neural Network Train-Test split\n",
        "Regarding the Neural Network approach, we cannot use the same train-test split. This because we need a training test that contains some playlists in order to train the model, and then we need a test set with different playlists in order to make the performance evaluation. The test set will also be split with the approach above, meaning some songs will be removes in order to evaluate the recommendations. This approach is needed in order to not evaluate the model with playlists that were in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aK4IK7EyRD-k"
      },
      "outputs": [],
      "source": [
        "def nn_train_test_split(playlists: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
        "    train_playlists, test_playlists = playlists.randomSplit([0.8, 0.2], 42)\n",
        "    return train_playlists, test_playlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "A50E9GgjRtF8"
      },
      "outputs": [],
      "source": [
        "NN_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_train_df-{NUM_PLAYLISTS}.json\")\n",
        "NN_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-{NUM_PLAYLISTS}.json\")\n",
        "NN_TEST_DF_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_TEST_DF_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "if os.path.exists(NN_TRAIN_DF_PATH) and os.path.exists(NN_TEST_DF_TRAIN_PATH) and os.path.exists(NN_TEST_DF_TEST_PATH) and os.path.exists(NN_TEST_DF_PATH):\n",
        "  nn_train_df = spark.read.schema(playlist_schema).json(NN_TRAIN_DF_PATH)\n",
        "  nn_test_df = spark.read.schema(playlist_schema).json(NN_TEST_DF_PATH)\n",
        "  nn_test_train_df = spark.read.schema(playlist_schema).json(NN_TEST_DF_TRAIN_PATH)\n",
        "  nn_test_test_df = spark.read.schema(playlist_schema).json(NN_TEST_DF_TEST_PATH)\n",
        "else:\n",
        "  # In order to avoid [PATH_ALREADY_EXISTS] errors. \n",
        "  if os.path.exists(NN_TRAIN_DF_PATH):\n",
        "    shutil.rmtree(NN_TRAIN_DF_PATH)\n",
        "  if os.path.exists(NN_TEST_DF_PATH):\n",
        "    shutil.rmtree(NN_TEST_DF_PATH)\n",
        "  if os.path.exists(NN_TEST_DF_TRAIN_PATH):\n",
        "    shutil.rmtree(NN_TEST_DF_TRAIN_PATH)\n",
        "  if os.path.exists(NN_TEST_DF_TEST_PATH):\n",
        "    shutil.rmtree(NN_TEST_DF_TEST_PATH)\n",
        "\n",
        "  nn_train_df, nn_test_df = nn_train_test_split(slice_df)\n",
        "  nn_test_train_df, nn_test_test_df = divide_whole_dataset(test_df)\n",
        "\n",
        "  nn_train_df, nn_test_df = nn_train_df.cache(), nn_test_df.cache()\n",
        "  test_train_df, test_test_df = nn_test_train_df.cache(), nn_test_test_df.cache()\n",
        "  nn_train_df.write.mode(\"overwrite\").json(NN_TRAIN_DF_PATH)\n",
        "  nn_test_df.write.mode(\"overwrite\").json(NN_TEST_DF_PATH)\n",
        "  nn_test_train_df.write.mode(\"overwrite\").json(NN_TEST_DF_TRAIN_PATH)\n",
        "  nn_test_test_df.write.mode(\"overwrite\").json(NN_TEST_DF_TEST_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N55cIvSCjlY"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-f-pu6DYvh"
      },
      "source": [
        "## Get the artists vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BMQE8T4XCng8"
      },
      "outputs": [],
      "source": [
        "def get_all_artists(playlist_df: DataFrame, filter_rare: bool = False) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Given a playlist dataframe, returns the dataframe containing the unique list of artist_uri that are present in all the playlists.\n",
        "  \"\"\"\n",
        "  if filter_rare:\n",
        "    all_songs = playlist_df.select(F.explode(\"tracks.artist_uri\").alias(\"artist_uri\")) \\\n",
        "            .groupBy(\"artist_uri\") \\\n",
        "            .agg(F.count(\"*\").alias(\"count\")) \\\n",
        "            .filter(F.col(\"count\") >= 3)\n",
        "  else:\n",
        "    all_songs = playlist_df.select(F.explode(\"tracks.artist_uri\").alias(\"artist_uri\")).distinct()\n",
        "  return all_songs\n",
        "\n",
        "def create_artists_pos_mapping(playlist_df: DataFrame, filter_rare: bool = False) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given the dataframe of artists_uri, it returns a mapping pos -> artist_uri in order to map each artist_uri to a position inside the embedding vector\n",
        "  \"\"\"\n",
        "  artists_df = get_all_artists(playlist_df)\n",
        "  artists_df.createOrReplaceTempView(\"ARTISTS\")\n",
        "  artists_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      ARTISTS\n",
        "  \"\"\")\n",
        "\n",
        "  artists_df = artists_df.sort(\"artist_uri\")\n",
        "\n",
        "  ARTIST_VECTOR_LENGTH = artists_df.count()\n",
        "\n",
        "  return artists_df, ARTIST_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "def create_artists_vector(playlist_df: DataFrame, artist_uri_to_id: dict, vector_length) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "        if row.artist_uri in artist_uri_to_id:\n",
        "          pos_list.add(artist_uri_to_id.get(row.artist_uri))\n",
        "        return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(vector_length + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(F.col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "def artist_pipeline(playlist_df: DataFrame, is_train: bool = True, is_train_train: bool = False) -> Tuple[DataFrame, int]:\n",
        "  train_string = \"train\" if is_train else \"test\"\n",
        "  if is_train:\n",
        "    train_train_string = \"\"\n",
        "  else:\n",
        "    train_train_string = \"train\" if is_train_train else \"test\"\n",
        "\n",
        "  ARTISTS_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-{train_string}-{train_train_string}-{NUM_PLAYLISTS}.json\")\n",
        "  \n",
        "  artists_slice_df = create_artists_vector(playlist_df,\n",
        "                                           artist_uri_to_id, \n",
        "                                           FILTERED_ARTIST_VECTOR_LENGTH).cache()\n",
        "  artists_slice_df.write.mode(\"overwrite\").json(ARTISTS_EMBEDDINGS)\n",
        "  \n",
        "  return artists_slice_df\n",
        "\n",
        "\n",
        "filtered_artist_mapping, FILTERED_ARTIST_VECTOR_LENGTH = create_artists_pos_mapping(slice_df, filter_rare=True)\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "with open(ARTIST_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "  f.write('%d' % FILTERED_ARTIST_VECTOR_LENGTH)\n",
        "\n",
        "artist_uri_to_id = filtered_artist_mapping.select('artist_uri', 'pos').rdd.collectAsMap()\n",
        "\n",
        "artist_slice_df_train = artist_pipeline(nn_train_df)\n",
        "\n",
        "artist_slice_df_test_train = artist_pipeline(nn_test_train_df, is_train=False, is_train_train=True)\n",
        "artist_slice_df_test_test = artist_pipeline(nn_test_test_df, is_train=False, is_train_train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EsUj1vODdXy"
      },
      "source": [
        "## Get the tracks vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ztcTMr3wDf2J"
      },
      "outputs": [],
      "source": [
        "def get_all_songs(playlist_df: DataFrame, filter_rare: bool = False) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a playlist dataframe, returns the dataframe containing the unique list of track_uri that are present in all the playlists.\n",
        "  \"\"\"\n",
        "  if filter_rare:\n",
        "    all_songs = playlist_df.select(F.explode(\"tracks.track_uri\").alias(\"track_uri\")) \\\n",
        "            .groupBy(\"track_uri\") \\\n",
        "            .agg(F.count(\"*\").alias(\"count\")) \\\n",
        "            .filter(F.col(\"count\") >= 5)\n",
        "  else:\n",
        "    all_songs = playlist_df.select(F.explode(\"tracks.track_uri\").alias(\"track_uri\")).distinct()\n",
        "  return all_songs\n",
        "\n",
        "def create_songs_pos_mapping(playlist_df: DataFrame, filter_rare: bool = False) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Given the dataframe of tracks_uris, it returns a mapping pos -> track_uri in order to map each track_uri to a position inside the embedding vector\n",
        "  \"\"\"\n",
        "  songs_df = get_all_songs(playlist_df, filter_rare)\n",
        "  songs_df.createOrReplaceTempView(\"SONGS_INFO\")\n",
        "\n",
        "  songs_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      SONGS_INFO\n",
        "  \"\"\")\n",
        "\n",
        "  RATING_VECTOR_LENGTH = songs_df.count()\n",
        "\n",
        "  return songs_df, RATING_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "#TODO: Since the .rdd is very slow, I can embed the position information of the track inside the track itself,\n",
        "# So then I can just do pos_list.add(row.rating_position) in a few miliseconds. \n",
        "def map_track_df_to_pos(playlist_df: DataFrame, track_uri_to_id: dict, vector_length: int) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "        if row.track_uri in track_uri_to_id:\n",
        "          pos_list.add(track_uri_to_id.get(row.track_uri))\n",
        "        return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(vector_length + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(F.col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "def track_pipeline(playlist_df: DataFrame, is_train: bool = True, is_train_train: bool = False, filter_rare: bool = False) -> Tuple[DataFrame, DataFrame, int]:\n",
        "  track_mapping = track_uri_to_id if not filter_rare else filtered_track_uri_to_id\n",
        "  vector_length = SONG_VECTOR_LENGTH if not filter_rare else FILTERED_SONG_VECTOR_LENGTH\n",
        "  song_embeddings_df = map_track_df_to_pos(playlist_df, \n",
        "                                       track_mapping, \n",
        "                                       vector_length).cache()\n",
        "\n",
        "  train_string = \"train\" if is_train else \"test\"\n",
        "  nn_string = \"nn-\" if filter_rare else \"\"\n",
        "  if is_train:\n",
        "    train_train_string = \"\"\n",
        "  else:\n",
        "    train_train_string = \"train\" if is_train_train else \"test\"\n",
        "\n",
        "  SONGS_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, f\"{nn_string}songs_embeddings-{train_string}-{train_train_string}-{NUM_PLAYLISTS}.json\")\n",
        "  song_embeddings_df.write.mode(\"overwrite\").json(SONGS_EMBEDDINGS)\n",
        "  \n",
        "  return song_embeddings_df\n",
        "\n",
        "\n",
        "SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "FILTERED_SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "song_mapping, SONG_VECTOR_LENGTH = create_songs_pos_mapping(slice_df)\n",
        "filtered_song_mapping, FILTERED_SONG_VECTOR_LENGTH = create_songs_pos_mapping(slice_df, filter_rare=True)\n",
        "with open(SONGS_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "  f.write('%d' % SONG_VECTOR_LENGTH)\n",
        "with open(FILTERED_SONGS_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "  f.write('%d' % FILTERED_SONG_VECTOR_LENGTH)\n",
        "\n",
        "track_uri_to_id = song_mapping.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "filtered_track_uri_to_id = filtered_song_mapping.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "FILTERED_SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"nn_songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "song_mapping.write.mode(\"overwrite\").json(SONGS_INFO_DF)\n",
        "filtered_song_mapping.write.mode(\"overwrite\").json(FILTERED_SONGS_INFO_DF)\n",
        "\n",
        "songs_slice_df_train = track_pipeline(train_df)\n",
        "songs_slice_df_test = track_pipeline(test_df, is_train=False)\n",
        "\n",
        "nn_songs_slice_df_train = track_pipeline(nn_train_df, filter_rare=True)\n",
        "#Create the test with also train and test for NN\n",
        "nn_songs_slice_df_test_train = track_pipeline(nn_test_df, is_train=False, is_train_train=True, filter_rare=True)\n",
        "nn_songs_slice_df_test_test = track_pipeline(nn_test_df, is_train=False, is_train_train=False, filter_rare=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6kcQSgwgX-93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60001425-9808-4ed5-e2ef-58a791be7a57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80002, 19998, 19998)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "nn_songs_slice_df_train.count(), nn_songs_slice_df_test_train.count(), nn_songs_slice_df_test_test.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1q5NS7aSrEc"
      },
      "source": [
        "## Get the Item-based dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7JxlnY0UJy9"
      },
      "outputs": [],
      "source": [
        "def create_playlists_pos_mapping(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Returns the dataframe that maps each playlist pid to a position, and the total number of playlists in the dataframe.\n",
        "  \"\"\"\n",
        "  playlist_df.createOrReplaceTempView(\"PLAYLISTS_INFO\")\n",
        "\n",
        "  playlist_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      PLAYLISTS_INFO\n",
        "  \"\"\")\n",
        "\n",
        "  playlist_df = playlist_df.select(\"pid\", \"pos\").sort(\"pid\").cache()\n",
        "  PLAYLIST_VECTOR_LENGTH = playlist_df.count()\n",
        "  return playlist_df, PLAYLIST_VECTOR_LENGTH\n",
        "\n",
        "playlist_map, PLAYLIST_VECTOR_LENGTH = create_playlists_pos_mapping(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k4bpr-OjUPw"
      },
      "outputs": [],
      "source": [
        "playlist_pid_to_id = playlist_map.rdd.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybAIbRP6T-uX"
      },
      "outputs": [],
      "source": [
        "def create_playlist_vector(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the track uris mapped to a list of playlist. The list of playlists is represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "    exploded_df = slice_df.select(\"tracks\", \"pid\").withColumn(\"track_uri\", F.explode(\"tracks.track_uri\"))\n",
        "    new_df = exploded_df.groupBy(\"track_uri\").agg(F.collect_list(\"pid\").alias(\"pid\"))\n",
        "\n",
        "    @F.udf(returnType=VectorUDT())\n",
        "    def extract_vector(row):\n",
        "      pos_list = set(playlist_pid_to_id.get(pid) for pid in row)\n",
        "      \n",
        "      return SparseVector(NUM_PLAYLISTS + 1, sorted(list(pos_list)), [1 for _ in pos_list]) #TODO: +1 because the positions of the track_uri_to_id start at 1 and not 0, so the first element is always 0\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = new_df.withColumn('embedding', extract_vector(F.col('pid'))).drop(\"pid\")\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "playlist_mapped = create_playlist_vector(playlist_df, playlist_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x76ocFdtl4TD"
      },
      "outputs": [],
      "source": [
        "# func = udf(lambda x: len(x.indices))\n",
        "# playlist_mapped.withColumn(\"count\", func(F.col('embedding'))).drop(\"embedding\").orderBy(F.col(\"count\").asc()).show(truncate=False) #This returns strange results, I think the embedding computation is wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw4NY0jkUDLw"
      },
      "outputs": [],
      "source": [
        "playlist_mapped.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-324eStVbv2"
      },
      "outputs": [],
      "source": [
        "PLAYLIST_MAP_PATH = os.path.join(SAVED_DFS_PATH, f\"playlist_map-{NUM_PLAYLISTS}.json\")\n",
        "PID_TO_ID_PATH = os.path.join(SAVED_DFS_PATH, f\"playlist_pid_to_id-{NUM_PLAYLISTS}.json\")\n",
        "playlist_mapped.write.json(PLAYLIST_MAP_PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KOojFseRjVnN",
        "X-s_eNiVaxhn",
        "VJeY9PpvaHUJ",
        "eAhzcKNIQIR9"
      ],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMv1OX9ROAoCVmuAoPm2PFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}