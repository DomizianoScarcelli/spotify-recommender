{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VJeY9PpvaHUJ",
        "h5zOoDDXEIBn"
      ],
      "authorship_tag": "ABX9TyPOXGVf6ZTccNkC9SchMHx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/item-based-cf/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N3wVgRfHYoOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65d63fc-4d02-411f-a74d-d748ca7a0ad2",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122545 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGHPv9OqY9MI"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49beca8-86be-489a-b746-28cb2c096491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a6a4e5-9cef-40d3-f5ef-198b1e8e3965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=0613a574c6726fa6552c07955b56455e13b9bf2a1059ce63e63d70fd46a64f88\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e503b9-05d7-4a85-d795-91a510d81aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "ad311152-bf52-41db-89b8-e7a304b8fca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-28T08:45:12+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "ERROR:pyngrok.process.ngrok:t=2023-05-28T08:45:12+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=b41bf8c7f0fc err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/ngrok-agent/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2QPcDVmsmNLLPmEZT9Jd2qY7dUo (34.125.120.213)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2023-05-28T08:45:12+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/ngrok-agent/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2QPcDVmsmNLLPmEZT9Jd2qY7dUo (34.125.120.213)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2023-05-28T08:45:12+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/ngrok-agent/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2QPcDVmsmNLLPmEZT9Jd2qY7dUo (34.125.120.213)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2023-05-28T08:45:12+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/ngrok-agent/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2QPcDVmsmNLLPmEZT9Jd2qY7dUo (34.125.120.213)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b6a85a590394>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Open a ngrok tunnel on the port 4050 where Spark is running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'4050'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tunnel with options: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             raise PyngrokNgrokError(\"The ngrok process errored on start: {}.\".format(ngrok_process.startup_error),\n\u001b[0m\u001b[1;32m    473\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/ngrok-agent/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2QPcDVmsmNLLPmEZT9Jd2qY7dUo (34.125.120.213)\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x15LhY-5a3Yi"
      },
      "outputs": [],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ea3692-f691-49f8-884a-00f8a27ae6b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7fafb20860b0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.port', '37591'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.app.id', 'local-1685263484911'),\n",
              "  ('spark.driver.host', 'ac3817da5f0a'),\n",
              "  ('spark.app.submitTime', '1685263482153'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.app.startTime', '1685263482392'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slice_df = slice_df.limit(10_000).cache()\n",
        "NUM_PLAYLISTS = slice_df.count()"
      ],
      "metadata": {
        "id": "PcLYEWCRZ2Ns"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Train-Test split"
      ],
      "metadata": {
        "id": "h5zOoDDXEIBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split as sklearn_split\n",
        "\n",
        "def train_test_split(playlist: Row) -> Tuple[Row, Row]:\n",
        "    train_rows, test_rows = sklearn_split(playlist.tracks, random_state=42)\n",
        "\n",
        "    playlist_train =  Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=train_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "\n",
        "    playlist_test = Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=test_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "    \n",
        "    return playlist_train, playlist_test"
      ],
      "metadata": {
        "id": "Qek8mLL4EK0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, struct\n",
        "import shutil\n",
        "\n",
        "def divide_whole_dataset(playlist_df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
        "  train_test_split_udf = udf(train_test_split, returnType=ArrayType(StructType(playlist_df.schema.fields)))\n",
        "  divided_df = playlist_df.withColumn(\"divided\", train_test_split_udf(struct(*playlist_df.columns)))\n",
        "  train_test_df = divided_df.select(col('divided').getItem(0).alias('train'), col('divided').getItem(1).alias('test'))\n",
        "\n",
        "  train_df = train_test_df.select(\"train.*\")\n",
        "  test_df = train_test_df.select(\"test.*\")\n",
        "  return train_df, test_df\n",
        "\n",
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "if os.path.exists(TRAIN_DF_PATH) and os.path.exists(TEST_DF_PATH):\n",
        "  train_df = spark.read.schema(playlist_schema).json(TRAIN_DF_PATH)\n",
        "  test_df = spark.read.schema(playlist_schema).json(TEST_DF_PATH)\n",
        "else:\n",
        "  # In order to avoid [PATH_ALREADY_EXISTS] errors. \n",
        "  if os.path.exists(TRAIN_DF_PATH):\n",
        "    shutil.rmtree(TRAIN_DF_PATH)\n",
        "  if os.path.exists(TEST_DF_PATH):\n",
        "    shutil.rmtree(TEST_DF_PATH)\n",
        "\n",
        "  train_df, test_df = divide_whole_dataset(slice_df)\n",
        "  train_df, test_df = train_df.cache(), test_df.cache()\n",
        "  train_df.write.mode(\"overwrite\").json(TRAIN_DF_PATH)\n",
        "  test_df.write.mode(\"overwrite\").json(TEST_DF_PATH)"
      ],
      "metadata": {
        "id": "3p9m_Jl6FDoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "7N55cIvSCjlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import col, udf, explode\n",
        "from pyspark.sql.types import IntegerType, ArrayType\n",
        "from functools import reduce\n",
        "import pickle"
      ],
      "metadata": {
        "id": "OAQzR1izCtyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the artists vector"
      ],
      "metadata": {
        "id": "KW-f-pu6DYvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_artists(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Given a playlist dataframe, returns the dataframe containing the unique list of artist_uri that are present in all the playlists.\n",
        "  \"\"\"\n",
        "  all_songs = playlist_df.select(explode(\"tracks.artist_uri\").alias(\"artist_uri\")).distinct()\n",
        "  return all_songs\n",
        "\n",
        "def create_artists_pos_mapping(playlist_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given the dataframe of artists_uri, it returns a mapping pos -> artist_uri in order to map each artist_uri to a position inside the embedding vector\n",
        "  \"\"\"\n",
        "  artists_df = get_all_artists(playlist_df)\n",
        "  artists_df.createOrReplaceTempView(\"ARTISTS\")\n",
        "  artists_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      ARTISTS\n",
        "  \"\"\")\n",
        "\n",
        "  artists_df = artists_df.sort(\"artist_uri\")\n",
        "\n",
        "  ARTIST_VECTOR_LENGTH = artists_df.count()\n",
        "\n",
        "  return artists_df, ARTIST_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "def create_artists_vector(playlist_df: DataFrame, mapping: DataFrame, artist_uri_to_id: dict, vector_length) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(artist_uri_to_id.get(row.artist_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(vector_length + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "def artist_pipeline(playlist_df: DataFrame, is_train: bool = True) -> Tuple[DataFrame, int]:\n",
        "  artists_df, ARTIST_VECTOR_LENGTH = create_artists_pos_mapping(train_df)\n",
        "  artist_uri_to_id = artists_df.select('artist_uri', 'pos').rdd.collectAsMap() # TODO: Pass it as a parameter maybe?\n",
        "\n",
        "  train_string = \"train\" if is_train else \"test\"\n",
        "\n",
        "  ARTISTS_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, f\"artists_embeddings-{train_string}-{NUM_PLAYLISTS}.json\")\n",
        "  ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"artist_vector_length-{train_string}-{NUM_PLAYLISTS}.txt\")\n",
        "  with open(ARTIST_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "    f.write('%d' % ARTIST_VECTOR_LENGTH)\n",
        "  artists_slice_df = create_artists_vector(train_df, \n",
        "                                           artists_df, \n",
        "                                           artist_uri_to_id, \n",
        "                                           ARTIST_VECTOR_LENGTH).cache()\n",
        "  artists_slice_df.write.mode(\"overwrite\").json(ARTISTS_EMBEDDINGS)\n",
        "  \n",
        "  return artists_slice_df, ARTIST_VECTOR_LENGTH\n",
        "\n",
        "artist_slice_df_train, ARTIST_VECTOR_LENGTH_TRAIN = artist_pipeline(train_df)\n",
        "artist_slice_df_test, ARTIST_VECTOR_LENGTH_TEST = artist_pipeline(test_df, is_train=False)"
      ],
      "metadata": {
        "id": "BMQE8T4XCng8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the tracks vector"
      ],
      "metadata": {
        "id": "5EsUj1vODdXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_songs(playlist_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a playlist dataframe, returns the dataframe containing the unique list of track_uri that are present in all the playlists.\n",
        "  \"\"\"\n",
        "  all_songs = playlist_df.select(explode(\"tracks.track_uri\").alias(\"track_uri\")).distinct()\n",
        "  return all_songs\n",
        "\n",
        "def create_songs_pos_mapping(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Given the dataframe of tracks_uris, it returns a mapping pos -> track_uri in order to map each track_uri to a position inside the embedding vector\n",
        "  \"\"\"\n",
        "  songs_df = get_all_songs(playlist_df)\n",
        "  songs_df.createOrReplaceTempView(\"SONGS_INFO\")\n",
        "\n",
        "  songs_info_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      SONGS_INFO\n",
        "  \"\"\")\n",
        "\n",
        "  songs_df = songs_info_df.sort(\"track_uri\")\n",
        "  RATING_VECTOR_LENGTH = songs_df.count()\n",
        "\n",
        "  return songs_df, songs_info_df, RATING_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "#TODO: Since the .rdd is very slow, I can embed the position information of the track inside the track itself,\n",
        "# So then I can just do pos_list.add(row.rating_position) in a few miliseconds. \n",
        "def map_track_df_to_pos(playlist_df: DataFrame, mapping: DataFrame, track_uri_to_id: dict, vector_length: int) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(track_uri_to_id.get(row.track_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(vector_length + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "def track_pipeline(playlist_df: DataFrame, is_train: bool = True) -> Tuple[DataFrame, DataFrame, int]:\n",
        "  songs_df, songs_info_df, RATING_VECTOR_LENGTH = create_songs_pos_mapping(train_df)\n",
        "\n",
        "  track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "  songs_slice_df = map_track_df_to_pos(train_df, \n",
        "                                       songs_df, \n",
        "                                       track_uri_to_id, \n",
        "                                       RATING_VECTOR_LENGTH).cache()\n",
        "\n",
        "  train_string = \"train\" if is_train else \"test\"\n",
        "\n",
        "  SONGS_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-{train_string}-{NUM_PLAYLISTS}.json\")\n",
        "  SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{train_string}-{NUM_PLAYLISTS}.json\")\n",
        "  RATING_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{train_string}-{NUM_PLAYLISTS}.txt\")\n",
        "  with open(RATING_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "    f.write('%d' % RATING_VECTOR_LENGTH)\n",
        "  songs_slice_df.write.mode(\"overwrite\").json(SONGS_EMBEDDINGS)\n",
        "  songs_info_df.write.mode(\"overwrite\").json(SONGS_INFO_DF)\n",
        "  return songs_slice_df, songs_info_df, RATING_VECTOR_LENGTH\n",
        "\n",
        "songs_slice_df_train, songs_info_df_train, SONGS_VECTOR_LENGTH_TRAIN = track_pipeline(train_df)\n",
        "songs_slice_df_train, songs_info_df_train, SONGS_VECTOR_LENGTH_TEST = track_pipeline(test_df, is_train=False)"
      ],
      "metadata": {
        "id": "ztcTMr3wDf2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Item-based dataframe"
      ],
      "metadata": {
        "id": "J1q5NS7aSrEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slice_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F3TVvogSvYx",
        "outputId": "87e590d3-a23e-4ef7-8de5-b5895a4e54f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|          name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+--------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|         Ratch|        false|45000| 1508976000|        88|        70|            1|[{0, Beyoncé, spo...|       50|   20047039|         48|\n",
            "|  slow it down|        false|45001| 1505952000|        80|        77|            1|[{0, Twinbed, spo...|       20|   20365984|         65|\n",
            "|    Phat Beats|        false|45002| 1466640000|        24|        15|            5|[{0, Baths, spoti...|       16|    5127143|         14|\n",
            "|           ✌🏽|        false|45003| 1509148800|        77|        63|            3|[{0, Owl City, sp...|       50|   17201663|         54|\n",
            "|          💘💘|        false|45004| 1479081600|       102|        78|            1|[{0, Rihanna, spo...|       35|   23364932|         56|\n",
            "|   Summer 2015|         true|45005| 1459555200|       126|       114|            7|[{0, Tiësto, spot...|       22|   30131677|         99|\n",
            "|Summer Country|        false|45006| 1492560000|       246|       166|            1|[{0, Kenny Chesne...|       13|   52836874|         91|\n",
            "|          Jazz|        false|45007| 1478044800|        15|        14|            1|[{0, Freddie Hubb...|        3|    6792929|         13|\n",
            "|       Hip Hop|        false|45008| 1503619200|       160|       113|            1|[{0, Fugees, spot...|       39|   37111155|         64|\n",
            "|    dance jams|        false|45009| 1469750400|        19|        16|            1|[{0, Justin Biebe...|        5|    4201172|         16|\n",
            "|   Inspiration|        false|45010| 1496188800|        30|        30|            3|[{0, The Staves, ...|        7|    7466099|         27|\n",
            "|           <33|        false|45011| 1504396800|        18|        16|            5|[{0, KYLE, spotif...|       10|    4106160|         15|\n",
            "|    Christmas |        false|45012| 1387756800|        42|        20|            1|[{0, Louis Armstr...|        2|    9704283|         26|\n",
            "|   Summer 2k17|        false|45013| 1500249600|        52|        51|            3|[{0, Drake, spoti...|       30|   11619016|         50|\n",
            "|         sleep|        false|45014| 1487030400|       164|       111|            1|[{0, TK N Cash, s...|       37|   38449124|         84|\n",
            "|           SKA|        false|45015| 1507593600|        50|        44|            1|[{0, The Slackers...|       15|    9435544|         34|\n",
            "|       Groovin|        false|45016| 1505347200|        88|        80|            1|[{0, of Montreal,...|       36|   23097679|         68|\n",
            "|     Play ME!!|        false|45017| 1477353600|        42|        40|            1|[{0, Jessie J, sp...|       15|    9737901|         35|\n",
            "|   Alternative|        false|45018| 1509408000|        47|        43|            1|[{0, All Time Low...|       36|   10622933|         35|\n",
            "|Rap En Español|        false|45019| 1503100800|       138|        25|            1|[{0, Cartel De Sa...|        4|   32636526|         11|\n",
            "+--------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_playlists_pos_mapping(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Returns the dataframe that maps each playlist pid to a position, and the total number of playlists in the dataframe.\n",
        "  \"\"\"\n",
        "  playlist_df.createOrReplaceTempView(\"PLAYLISTS_INFO\")\n",
        "\n",
        "  playlist_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      PLAYLISTS_INFO\n",
        "  \"\"\")\n",
        "\n",
        "  playlist_df = playlist_df.select(\"pos\", \"pid\").sort(\"pid\").cache()\n",
        "  PLAYLIST_VECTOR_LENGTH = playlist_df.count()\n",
        "  return playlist_df, PLAYLIST_VECTOR_LENGTH\n",
        "\n",
        "playlist_map, PLAYLIST_VECTOR_LENGTH = create_playlists_pos_mapping(slice_df)"
      ],
      "metadata": {
        "id": "F7JxlnY0UJy9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "playlist_map.show(), PLAYLIST_VECTOR_LENGTH"
      ],
      "metadata": {
        "id": "8hnYVPuOUwXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78bca4ff-6418-45e6-f4f8-b8fbdebf9bd0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "|  pos|pid|\n",
            "+-----+---+\n",
            "|30001|  0|\n",
            "|30002|  1|\n",
            "|30003|  2|\n",
            "|30004|  3|\n",
            "|30005|  4|\n",
            "|30006|  5|\n",
            "|30007|  6|\n",
            "|30008|  7|\n",
            "|30009|  8|\n",
            "|30010|  9|\n",
            "|30011| 10|\n",
            "|30012| 11|\n",
            "|30013| 12|\n",
            "|30014| 13|\n",
            "|30015| 14|\n",
            "|30016| 15|\n",
            "|30017| 16|\n",
            "|30018| 17|\n",
            "|30019| 18|\n",
            "|30020| 19|\n",
            "+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 100000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "# new_df.show()"
      ],
      "metadata": {
        "id": "gY-h6G0Chz0i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "track_uri_to_id = playlist_map.rdd.collectAsMap()\n",
        "track_uri_to_id"
      ],
      "metadata": {
        "id": "-k4bpr-OjUPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_playlist_vector(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "    exploded_df = slice_df.select(\"tracks\", \"pid\").withColumn(\"track_uri\", F.explode(\"tracks.track_uri\"))\n",
        "    new_df = exploded_df.groupBy(\"track_uri\").agg(F.collect_list(\"pid\").alias(\"pid\"))\n",
        "\n",
        "    @F.udf(returnType=VectorUDT())\n",
        "    def extract_vector(row):\n",
        "      pos_list = set(track_uri_to_id.get(pid) for pid in row)\n",
        "      \n",
        "      return SparseVector(NUM_PLAYLISTS, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = new_df.withColumn('embedding', extract_vector(F.col('pid'))).drop(\"pid\")\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "playlist_mapped = create_playlist_vector(playlist_df, playlist_map)"
      ],
      "metadata": {
        "id": "ybAIbRP6T-uX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func = udf(lambda x: len(x.indices))\n",
        "playlist_mapped.withColumn(\"count\", func(F.col('embedding'))).drop(\"embedding\").orderBy(col(\"count\").desc()).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "x76ocFdtl4TD",
        "outputId": "4df45cb7-d0f3-4798-d3fe-3a2fc2241a9a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1d0a0d65e85e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplaylist_mapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    910\u001b[0m                 )\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-42-01ef52f882ab>\", line 12, in extract_vector\nTypeError: '<' not supported between instances of 'NoneType' and 'int'\n"
          ]
        }
      ]
    }
  ]
}