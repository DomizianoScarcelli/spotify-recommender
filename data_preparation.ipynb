{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h5zOoDDXEIBn"
      ],
      "authorship_tag": "ABX9TyMuZb8AsILdm6NAgmzdCs/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/nn-model/data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N3wVgRfHYoOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4d61cb-e625-425e-89f2-a0204c48f632",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGHPv9OqY9MI"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524b903e-c1d8-4ff3-a105-004bb3a09c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95254a5b-b546-4cdc-f4a1-098141ebbfa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cb34df-9014-4853-c98c-2790bd0e448d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e937cc6b-ae0f-4d54-cbce-8761c145cff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-26T09:37:52+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e3852d-d545-4b04-8696-7e979517a9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://139c-35-245-153-155.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f5b1d7-df87-4692-a110-3befe5b9e000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f69184bf3a0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.driver.port', '40863'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.app.startTime', '1685093856793'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.app.id', 'local-1685093858754'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.driver.host', '87ee632b5f6c'),\n",
              "  ('spark.app.submitTime', '1685093856545'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slice_df = slice_df.limit(10_000).cache()\n",
        "NUM_PLAYLISTS = slice_df.count()"
      ],
      "metadata": {
        "id": "PcLYEWCRZ2Ns"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Train-Test split"
      ],
      "metadata": {
        "id": "h5zOoDDXEIBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split as sklearn_split\n",
        "\n",
        "def train_test_split(playlist: Row) -> Tuple[Row, Row]:\n",
        "    train_rows, test_rows = sklearn_split(playlist.tracks, random_state=42)\n",
        "\n",
        "    playlist_train =  Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=train_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "\n",
        "    playlist_test = Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=test_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "    \n",
        "    return playlist_train, playlist_test"
      ],
      "metadata": {
        "id": "Qek8mLL4EK0M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, struct\n",
        "import shutil\n",
        "\n",
        "def divide_whole_dataset(playlist_df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
        "  train_test_split_udf = udf(train_test_split, returnType=ArrayType(StructType(playlist_df.schema.fields)))\n",
        "  divided_df = playlist_df.withColumn(\"divided\", train_test_split_udf(struct(*playlist_df.columns)))\n",
        "  train_test_df = divided_df.select(col('divided').getItem(0).alias('train'), col('divided').getItem(1).alias('test'))\n",
        "\n",
        "  train_df = train_test_df.select(\"train.*\")\n",
        "  test_df = train_test_df.select(\"test.*\")\n",
        "  return train_df, test_df\n",
        "\n",
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "if os.path.exists(TRAIN_DF_PATH) and os.path.exists(TEST_DF_PATH):\n",
        "  train_df = spark.read.schema(playlist_schema).json(TRAIN_DF_PATH)\n",
        "  test_df = spark.read.schema(playlist_schema).json(TEST_DF_PATH)\n",
        "else:\n",
        "  # In order to avoid [PATH_ALREADY_EXISTS] errors. \n",
        "  if os.path.exists(TRAIN_DF_PATH):\n",
        "    shutil.rmtree(TRAIN_DF_PATH)\n",
        "  if os.path.exists(TEST_DF_PATH):\n",
        "    shutil.rmtree(TEST_DF_PATH)\n",
        "\n",
        "  train_df, test_df = divide_whole_dataset(slice_df)\n",
        "  train_df, test_df = train_df.cache(), test_df.cache()\n",
        "  train_df.write.mode(\"overwrite\").json(TRAIN_DF_PATH)\n",
        "  test_df.write.mode(\"overwrite\").json(TEST_DF_PATH)"
      ],
      "metadata": {
        "id": "3p9m_Jl6FDoA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "7N55cIvSCjlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import col, udf, explode\n",
        "from pyspark.sql.types import IntegerType, ArrayType\n",
        "from functools import reduce\n",
        "import pickle"
      ],
      "metadata": {
        "id": "OAQzR1izCtyG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the artists vector"
      ],
      "metadata": {
        "id": "KW-f-pu6DYvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_artists(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Given a playlist dataframe, returns the dataframe containing the unique list of artist_uri that are present in all the playlists.\n",
        "  \"\"\"\n",
        "  all_songs = playlist_df.select(explode(\"tracks.artist_uri\").alias(\"artist_uri\")).distinct()\n",
        "  return all_songs\n",
        "\n",
        "def create_artists_pos_mapping(playlist_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given the dataframe of artists_uri, it returns a mapping pos -> artist_uri in order to map each artist_uri to a position inside the embedding vector\n",
        "  \"\"\"\n",
        "  artists_df = get_all_artists(playlist_df)\n",
        "  artists_df.createOrReplaceTempView(\"ARTISTS\")\n",
        "  artists_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      ARTISTS\n",
        "  \"\"\")\n",
        "\n",
        "  artists_df = artists_df.sort(\"artist_uri\")\n",
        "\n",
        "  ARTIST_VECTOR_LENGTH = artists_df.count()\n",
        "\n",
        "  return artists_df, ARTIST_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "def create_artists_vector(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(artist_uri_to_id.get(row.artist_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(ARTIST_VECTOR_LENGTH + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "# Path to save the file\n",
        "\n",
        "artists_df, ARTIST_VECTOR_LENGTH = create_artists_pos_mapping(train_df)\n",
        "artist_uri_to_id = artists_df.select('artist_uri', 'pos').rdd.collectAsMap() # TODO: Pass it as a parameter maybe?\n",
        "\n",
        "ARTISTS_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, f\"artists_embeddings-{NUM_PLAYLISTS}.json\")\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "with open(ARTIST_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "  f.write('%d' % ARTIST_VECTOR_LENGTH)\n",
        "artists_slice_df = create_artists_vector(train_df, artists_df).cache()\n",
        "artists_slice_df.write.mode(\"overwrite\").json(ARTISTS_EMBEDDINGS)"
      ],
      "metadata": {
        "id": "BMQE8T4XCng8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the tracks vector"
      ],
      "metadata": {
        "id": "5EsUj1vODdXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_songs(playlist_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a playlist dataframe, returns the dataframe containing the unique list of track_uri that are present in all the playlists.\n",
        "  \"\"\"\n",
        "  all_songs = playlist_df.select(explode(\"tracks.track_uri\").alias(\"track_uri\")).distinct()\n",
        "  return all_songs\n",
        "\n",
        "def create_songs_pos_mapping(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  \"\"\"\n",
        "  Given the dataframe of tracks_uris, it returns a mapping pos -> track_uri in order to map each track_uri to a position inside the embedding vector\n",
        "  \"\"\"\n",
        "  songs_df = get_all_songs(playlist_df)\n",
        "  songs_df.createOrReplaceTempView(\"SONGS_INFO\")\n",
        "\n",
        "  songs_info_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      SONGS_INFO\n",
        "  \"\"\")\n",
        "\n",
        "  songs_df = songs_info_df.sort(\"track_uri\")\n",
        "  RATING_VECTOR_LENGTH = songs_df.count()\n",
        "\n",
        "  return songs_df, songs_info_df, RATING_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "#TODO: Since the .rdd is very slow, I can embed the position information of the track inside the track itself,\n",
        "# So then I can just do pos_list.add(row.rating_position) in a few miliseconds. \n",
        "def map_track_df_to_pos(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(track_uri_to_id.get(row.track_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(RATING_VECTOR_LENGTH + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "songs_df, songs_info_df, RATING_VECTOR_LENGTH = create_songs_pos_mapping(train_df)\n",
        "\n",
        "track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "songs_slice_df = map_track_df_to_pos(train_df, songs_df).cache()\n",
        "\n",
        "\n",
        "SONGS_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "RATING_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "with open(RATING_VECTOR_LENGTH_PATH, \"w\") as f:\n",
        "  f.write('%d' % RATING_VECTOR_LENGTH)\n",
        "songs_slice_df.write.mode(\"overwrite\").json(SONGS_EMBEDDINGS)\n",
        "songs_info_df.write.mode(\"overwrite\").json(SONGS_INFO_DF)"
      ],
      "metadata": {
        "id": "ztcTMr3wDf2J"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}