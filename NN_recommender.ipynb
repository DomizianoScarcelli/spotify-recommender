{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/nn-model/NN_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "d181d832-1d2c-4a0b-9380-b730cb68fd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install petastorm -qq\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oozTtW3om3Ab"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "from petastorm import make_batch_reader\n",
        "from petastorm.pytorch import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XG5sA53iP9z0"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MocM1cmqQpn5",
        "outputId": "9d094ad9-4b70-4f90-db97-6d3f1c29cf1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrnLYquoarPa",
        "outputId": "c5b91fd3-6faf-4fc8-ee8f-0015cfbc50b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3IEuiyDawo3",
        "outputId": "d29658a2-b129-4308-fed8-430c2dbc67b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qJN-lfta1ok",
        "outputId": "ca03c715-50cd-45ad-dcea-4b357314bd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-25T11:07:13+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x15LhY-5a3Yi",
        "outputId": "6d59d327-f832-40a5-fcdd-452ff3a408b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://c6a8-35-201-201-181.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlxfJiBSZ6ju",
        "outputId": "3e7ed4d1-bfef-4591-e4b7-85f262e2f418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f251c5e75e0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.driver.host', 'e8b508dba625'),\n",
              "  ('spark.app.submitTime', '1685012808364'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.app.id', 'local-1685012813955'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.driver.port', '43935'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true'),\n",
              "  ('spark.app.startTime', '1685012808868')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U21yG9qCLVMP"
      },
      "source": [
        "# Import pyspark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n8oOaZFLLZqM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "20f0bkGILXPu"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TPb3c8hxLbO5"
      },
      "outputs": [],
      "source": [
        "PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"playlist_embeddings_new.parquet\")\n",
        "JSON_PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"playlist_embeddings_NN.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F19_eMTNLcTY"
      },
      "outputs": [],
      "source": [
        "# if os.path.exists(JSON_PLAYLIST_EMBEDDINGS):\n",
        "#   mapped_slice_df = spark.read.schema(playlist_schema_mapped).json(JSON_PLAYLIST_EMBEDDINGS)\n",
        "# else:\n",
        "#   mapped_slice_df = spark.read.schema(playlist_schema_mapped).parquet(PLAYLIST_EMBEDDINGS)\n",
        "#   mapped_slice_df.write.json(JSON_PLAYLIST_EMBEDDINGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHOwXH6iLjB7"
      },
      "source": [
        "# Extract artist matrix $\\mathbf{A}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvzFsHlyL0JR"
      },
      "source": [
        "From the main dataframe `slice_df` that contains the information about playlists and their relative songs, I want to obtain an artist binary `SparseVector` $\\mathbf{a}$ for each playlist that describes the artists that are inside of that playlist.\n",
        "Stacked togheter, all the vectors make the artists matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JUhIHxpLLqqs"
      },
      "outputs": [],
      "source": [
        "slice_df = slice_df.limit(10_000).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XrXkOgpACq7B"
      },
      "outputs": [],
      "source": [
        "# slice_df.limit(1).cache().select(explode(\"tracks\")).select(\"col.*\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nJ-dlCcTFeTW"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import col, udf, explode\n",
        "from pyspark.sql.types import IntegerType, ArrayType\n",
        "from functools import reduce\n",
        "\n",
        "def get_all_artists(playlist_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "   all_songs = playlist_df.select(explode(\"tracks.artist_uri\").alias(\"artist_uri\")).distinct()\n",
        "   return all_songs\n",
        "\n",
        "def create_artists_pos_mapping(artists_df: DataFrame) -> DataFrame:\n",
        "  artists_df = get_all_artists(slice_df)\n",
        "  artists_df.createOrReplaceTempView(\"ARTISTS\")\n",
        "  artists_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      ARTISTS\n",
        "  \"\"\")\n",
        "\n",
        "  artists_df = artists_df.sort(\"artist_uri\")\n",
        "\n",
        "  ARTIST_VECTOR_LENGTH = artists_df.count()\n",
        "\n",
        "  return artists_df, ARTIST_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "#TODO: Since the .rdd is very slow, I can embed the position information of the track inside the track itself,\n",
        "# So then I can just do pos_list.add(row.rating_position) in a few miliseconds. \n",
        "def create_artists_vector(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(artist_uri_to_id.get(row.artist_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(ARTIST_VECTOR_LENGTH + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "artists_df, ARTIST_VECTOR_LENGTH = create_artists_pos_mapping(slice_df)\n",
        "artist_uri_to_id = artists_df.select('artist_uri', 'pos').rdd.collectAsMap() # TODO: Pass it as a parameter maybe?\n",
        "# if not os.path.exists(PLAYLIST_EMBEDDINGS):\n",
        "artists_slice_df = create_artists_vector(slice_df, artists_df).cache()\n",
        "  # mapped_slice_df.write.parquet(PLAYLIST_EMBEDDINGS)\n",
        "# else:\n",
        "  # mapped_slice_df = spark.read.schema(playlist_schema_mapped).parquet(PLAYLIST_EMBEDDINGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K5LUrFJIF9y9"
      },
      "outputs": [],
      "source": [
        "def get_all_songs(playlist_df: DataFrame) -> DataFrame:\n",
        "   all_songs = playlist_df.select(explode(\"tracks.track_uri\").alias(\"track_uri\")).distinct()\n",
        "   return all_songs\n",
        "\n",
        "def create_songs_pos_mapping(songs_df: DataFrame) -> Tuple[DataFrame, int]:\n",
        "  songs_df = get_all_songs(slice_df)\n",
        "  songs_df.createOrReplaceTempView(\"SONGS_INFO\")\n",
        "\n",
        "  songs_info_df = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      row_number() OVER (\n",
        "          PARTITION BY '' \n",
        "          ORDER BY '' \n",
        "      ) as pos,\n",
        "      *\n",
        "  FROM \n",
        "      SONGS_INFO\n",
        "  \"\"\")\n",
        "\n",
        "  songs_df = songs_info_df.sort(\"track_uri\")\n",
        "  RATING_VECTOR_LENGTH = songs_df.count()\n",
        "\n",
        "  return songs_df, RATING_VECTOR_LENGTH\n",
        "\n",
        "\n",
        "#TODO: Since the .rdd is very slow, I can embed the position information of the track inside the track itself,\n",
        "# So then I can just do pos_list.add(row.rating_position) in a few miliseconds. \n",
        "def map_track_df_to_pos(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(track_uri_to_id.get(row.track_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(RATING_VECTOR_LENGTH + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "songs_df, RATING_VECTOR_LENGTH = create_songs_pos_mapping(slice_df)\n",
        "track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap() # TODO: Pass it as a parameter maybe?\n",
        "mapped_slice_df = map_track_df_to_pos(slice_df, songs_df).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPOD60GQQGse"
      },
      "source": [
        "# Convert PySpark DataFrame into PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import udf, length\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "import torch\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "def convert_sparse_to_indices(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "  # SparseTupleType = ArrayType(\n",
        "  #   ArrayType(IntegerType())\n",
        "  # )\n",
        "\n",
        "  @udf(returnType=ArrayType(IntegerType()))\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    indices_list = item.indices.tolist()\n",
        "    padding_width = max_songs - len(indices_list)\n",
        "    return indices_list + [-1] * padding_width\n",
        "  \n",
        "  max_songs = mapped_slice_df.select(F.max(\"num_tracks\")).first()[0]\n",
        "  print(f\"Max number of songs: {max_songs}\")\n",
        "  df = df.withColumn(\"embedding_indices\", transform_array(col(\"tracks\"))).drop(\"tracks\")\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eIIqDVeN7cU0"
      },
      "outputs": [],
      "source": [
        "def padded_tensors_to_sparse_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    rows.append(sparse_tensor)\n",
        "  return torch.stack(rows)\n",
        "\n",
        "def padded_tensors_to_dense_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    dense = sparse_tensor.to_dense()\n",
        "    rows.append(dense)\n",
        "  unpadded = torch.stack(rows)\n",
        "  return unpadded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNak5bEo4cmV"
      },
      "source": [
        "In the paper they have two matrices,l et $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "y_pKgJx26s73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbbed63-d92a-4cf8-e696-8dc3d642d2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of songs: 250\n",
            "Max number of songs: 250\n"
          ]
        }
      ],
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
        "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
        "from torchvision import transforms\n",
        "from petastorm import TransformSpec\n",
        "\n",
        "CACHE = os.path.join(GDRIVE_DIR, \"/big_data/cache_5\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df = convert_sparse_to_indices(mapped_slice_df.select(\"tracks\"))\n",
        "songs_converter = make_spark_converter(pytorch_songs_df)\n",
        "\n",
        "pytorch_artists_df = convert_sparse_to_indices(artists_slice_df.select(\"tracks\"))\n",
        "artist_converter = make_spark_converter(pytorch_songs_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Av0W0P66lC"
      },
      "source": [
        "# PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.learning_rate = conf[\"lr\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.x_positions = torch.LongTensor()\n",
        "        self.x_ones = torch.FloatTensor()\n",
        "\n",
        "        self.y_positions = torch.LongTensor()\n",
        "        self.y_ones = torch.FloatTensor()\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "        nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "        self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden))\n",
        "        nn.init.zeros_(self.biases['encoder_b'])\n",
        "        self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input))\n",
        "        nn.init.zeros_(self.biases['decoder_b'])\n",
        "        self.d_params = [self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l2 = torch.norm(self.weights['encoder_h']) ** 2\n",
        "      decoder_b_l2 = torch.norm(self.biases['decoder_b']) ** 2\n",
        "      encoder_b_l2 = torch.norm(self.biases['encoder_b']) ** 2\n",
        "\n",
        "      l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2\n",
        "      return l2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        # TODO: Take sparse matrix representation instead of dense\n",
        "        # self.x = x_positxions\n",
        "        # self.x_ones = x_ones\n",
        "        # self.y_positions = y_positions\n",
        "        # self.y_ones = y_ones\n",
        "\n",
        "        self.x = x.t()\n",
        "        self.y = y.t()\n",
        "\n",
        "        # x_sparse = torch.sparse.FloatTensor(self.x_positions.t(), self.x_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        # self.x = x_sparse.to_dense()\n",
        "        # y_sparse = torch.sparse.FloatTensor(self.y_positions.t(), self.y_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        # self.y = y_sparse.to_dense()\n",
        "\n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p= 1 - self.input_keep_prob) # Maybe error\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       0.55 * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "            \n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l2 = torch.norm(self.weights['encoder_h']) ** 2\n",
        "      decoder_b_l2 = torch.norm(self.biases['decoder_b']) ** 2\n",
        "      encoder_b_l2 = torch.norm(self.biases['encoder_b']) ** 2\n",
        "      decoder_h_l2 = torch.norm(self.weights['decoder_h']) ** 2\n",
        "\n",
        "      l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2 + decoder_h_l2\n",
        "      return l2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSdJCqx44lgE"
      },
      "source": [
        "I'm trying to figure out what x and y are. From the source code, we can see this in the `data_readery.py:48` file:\n",
        "```python\n",
        "trk_positions = np.concatenate(trk_positions)\n",
        "art_positions = np.concatenate(art_positions)\n",
        "y_positions = np.concatenate((trk_positions, art_positions), 0)\n",
        "```\n",
        "So I can assume that y is just the concatenation of p_i and a_i.\n",
        "\n",
        "On the other hand, from `data_reader.py:250`, we can see:\n",
        "```python\n",
        "trk_positions = np.concatenate(trk_positions)\n",
        "art_positions = np.concatenate(art_positions)\n",
        "x_positions = np.concatenate((trk_positions, art_positions), 0)\n",
        "```\n",
        "\n",
        "So From this I assume `y` and `x` are the same thing, meanin the concatenation of A and P."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DVGpMdqs7dQQ"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters used in the paper\n",
        "NUM_PLAYLISTS = 10_000\n",
        "conf = {\n",
        "    'batch': 32,\n",
        "    'n_input': RATING_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH,\n",
        "    'hidden': 64,\n",
        "    'lr': 0.001, #original 0.001\n",
        "    'reg_lambda': 0.001,\n",
        "    'initval': 'NULL',\n",
        "    \"keep_prob\": 0.8,\n",
        "    \"input_keep_prob\": 0.8,\n",
        "    'save': './'\n",
        "}\n",
        "dae_model = DAE(conf)\n",
        "dae_model.init_weight()\n",
        "optimizer = optim.Adam(dae_model.d_params, lr=conf['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Z_BjjpshDD2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605,
          "referenced_widgets": [
            "1c64c0234093499ba40196134d2f9d55",
            "cee3781817d345a6b66bf2a93db850ba",
            "33b7a1a62c0340258a68e46633440e7d",
            "53759a95818a408ab6f9f1abc6e0f003",
            "14c080de9f5947618efad34924703f19",
            "c8444d6d799f474c9c3e7edb7f1f64ce",
            "a17f6c8b2f354a19a56c637e50664385",
            "4abc0d7fb3454588bc24c20ff2285a30",
            "ef4aa335c90147b4b96f7dacf35e424e",
            "5b53d12b3bdc4b7686d622137acdf321",
            "1fca707879074a72b8d46487106ffdc2"
          ]
        },
        "outputId": "34b2a82d-7710-46fe-c246-b493bfa33817"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training model:   0%|          | 0/3125.0 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c64c0234093499ba40196134d2f9d55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 81554.84375\n",
            "Loss: 10480.935546875\n",
            "Loss: 4093.239501953125\n",
            "Loss: 2651.362060546875\n",
            "Loss: 1879.93798828125\n",
            "Loss: 1749.5504150390625\n",
            "Loss: 1353.27978515625\n",
            "Loss: 1458.247314453125\n",
            "Loss: 1289.91650390625\n",
            "Loss: 1383.0006103515625\n",
            "Loss: 1192.9573974609375\n",
            "Loss: 1267.356201171875\n",
            "Loss: 1131.73486328125\n",
            "Loss: 1235.7421875\n",
            "Loss: 1063.07666015625\n",
            "Loss: 1160.0535888671875\n",
            "Loss: 1114.031005859375\n",
            "Loss: 1064.7171630859375\n",
            "Loss: 1153.3277587890625\n",
            "Loss: 1407.975830078125\n",
            "Loss: 1140.4365234375\n",
            "Loss: 1139.4649658203125\n",
            "Loss: 1240.2803955078125\n",
            "Loss: 1298.5384521484375\n",
            "Loss: 1132.6448974609375\n",
            "Loss: 1427.786376953125\n",
            "Loss: 1093.15087890625\n",
            "Loss: 1065.274658203125\n",
            "Loss: 1249.514892578125\n",
            "Loss: 1071.7154541015625\n",
            "Loss: 1227.5726318359375\n",
            "Loss: 1055.323974609375\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "NUM_EPOCHS = 10\n",
        "with songs_converter.make_torch_dataloader(num_epochs = NUM_EPOCHS) as songs_dataloader:\n",
        "  with artist_converter.make_torch_dataloader(num_epochs = NUM_EPOCHS) as artists_dataloader:\n",
        "    losses = []\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (RATING_VECTOR_LENGTH, )\n",
        "    # for epoch in tqdm(range(num_epochs), desc=\"Training...\"):\n",
        "    zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "    for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Training model\", total= (NUM_PLAYLISTS / 32) * NUM_EPOCHS):\n",
        "      padded_song_tensor = song[\"embedding_indices\"]\n",
        "      padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "      y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "      dae_model(x, y)\n",
        "      loss = dae_model.cost\n",
        "      dae_model.cost.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch_idx % 100 == 0:\n",
        "        print(f\"Loss: {loss}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X-s_eNiVaxhn"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNEHAsCa2IcSjcK5qKCh+/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c64c0234093499ba40196134d2f9d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cee3781817d345a6b66bf2a93db850ba",
              "IPY_MODEL_33b7a1a62c0340258a68e46633440e7d",
              "IPY_MODEL_53759a95818a408ab6f9f1abc6e0f003"
            ],
            "layout": "IPY_MODEL_14c080de9f5947618efad34924703f19"
          }
        },
        "cee3781817d345a6b66bf2a93db850ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8444d6d799f474c9c3e7edb7f1f64ce",
            "placeholder": "​",
            "style": "IPY_MODEL_a17f6c8b2f354a19a56c637e50664385",
            "value": "Training model: 100%"
          }
        },
        "33b7a1a62c0340258a68e46633440e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4abc0d7fb3454588bc24c20ff2285a30",
            "max": 3125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef4aa335c90147b4b96f7dacf35e424e",
            "value": 3125
          }
        },
        "53759a95818a408ab6f9f1abc6e0f003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b53d12b3bdc4b7686d622137acdf321",
            "placeholder": "​",
            "style": "IPY_MODEL_1fca707879074a72b8d46487106ffdc2",
            "value": " 3125/3125.0 [39:04&lt;00:00,  1.23it/s]"
          }
        },
        "14c080de9f5947618efad34924703f19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8444d6d799f474c9c3e7edb7f1f64ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17f6c8b2f354a19a56c637e50664385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4abc0d7fb3454588bc24c20ff2285a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4aa335c90147b4b96f7dacf35e424e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b53d12b3bdc4b7686d622137acdf321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fca707879074a72b8d46487106ffdc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}