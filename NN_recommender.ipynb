{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/dev/NN_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "def is_running_on_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "LOCAL = not is_running_on_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "42c169d8-dd98-4d5b-ce88-cd35f1b7ca4c"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    !pip install petastorm -qq\n",
        "    !pip install pyspark -qq\n",
        "    !pip install -U -q PyDrive -qq\n",
        "    !apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "oozTtW3om3Ab"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "if not LOCAL:\n",
        "    from google.colab import drive\n",
        "\n",
        "from typing import Tuple\n",
        "from functools import reduce\n",
        "import pickle\n",
        "import torch\n",
        "from petastorm import make_batch_reader\n",
        "from petastorm.pytorch import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XG5sA53iP9z0"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "if not LOCAL:\n",
        "    JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    GDRIVE_DIR = \"/content/drive\"\n",
        "    GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "    GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "    DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "    AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "    LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "    SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "    LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "    MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "    SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "else:\n",
        "    GDRIVE_DATA_DIR = os.path.abspath(\"./data\")\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "    JAVA_HOME = \"/opt/homebrew/opt/openjdk\"\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/19 02:25:55 WARN Utils: Your hostname, MacBook-Air-di-Domiziano.local resolves to a loopback address: 127.0.0.1; using 192.168.1.175 instead (on interface en0)\n",
            "23/06/19 02:25:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/06/19 02:25:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "#@title Create the session\n",
        "config = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JAu9mQsxTxHj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fGgc9DHjT09S"
      },
      "outputs": [],
      "source": [
        "NUM_PLAYLISTS = 100_000\n",
        "SONGS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_INFO_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "ARTISTS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-train-{NUM_PLAYLISTS}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The DF used for train (80% of the original) (playlist are different)\n",
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "# The DF used for testing (20% of the original) (playlist are different)\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "# The DF used for train in the NN model (can be filtered or not)\n",
        "NN_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_train_df-{NUM_PLAYLISTS}.json\")\n",
        "# The DF used for testing in the NN model (can be filtered or not)\n",
        "NN_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-{NUM_PLAYLISTS}.json\")\n",
        "# The partition in train test of the NN test set. (Same playlists, different songs)\n",
        "NN_TEST_DF_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_TEST_DF_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_EVAL_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-{NUM_PLAYLISTS}.json\")\n",
        "NN_EVAL_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_EVAL_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-test-{NUM_PLAYLISTS}.json\")\n",
        "# New one:\n",
        "ARTISTS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_EVAL = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_EVAL_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_EVAL_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "# The length of the artist vector length (Artist vectors are only used in the NN model)\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "# This may be filtered or not\n",
        "FILTERED_SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "SONGS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_TEST = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_SONGS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_SONGS_EMBEDDINGS_EVAL = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-{NUM_PLAYLISTS}.json\") #TODO: The logic to produce this still has to be coded.\n",
        "NN_SONGS_EMBEDDINGS_EVAL_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_EVAL_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "FILTERED_SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"nn_songs_info_df-{NUM_PLAYLISTS}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lo8gbiN1U1XJ"
      },
      "outputs": [],
      "source": [
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH)\n",
        "artists_embeddings = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH)\n",
        "song_mapping = spark.read.json(SONGS_INFO_DF_PATH)\n",
        "\n",
        "songs_embeddings_eval_train = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_EVAL_TRAIN)\n",
        "songs_embeddings_eval_test = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_EVAL_TEST)\n",
        "\n",
        "artists_embeddings_eval_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_EVAL_TRAIN)\n",
        "artists_embeddings_eval_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_EVAL_TEST)\n",
        "\n",
        "with open(ARTIST_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  ARTIST_VECTOR_LENGTH = int(content)\n",
        "with open(SONGS_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  SONGS_VECTOR_LENGTH = int(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY_szFyLTps4",
        "outputId": "764c5d24-4a9f-44d6-e349-6a2bca3bb948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               Mix|        false|24770| 1376352000|        67|        43|            1|(133477,[84,3091,...|       32|   16179675|         34|\n",
            "|         C H I L L|        false|84782| 1508889600|       105|        79|            2|(133477,[256,1951...|       74|   23647221|         59|\n",
            "|    COACHELLA 2013|        false| 3188| 1366675200|       207|        87|            1|(133477,[142,146,...|       16|   49566298|         41|\n",
            "|              Heat|        false| 3879| 1492992000|        27|        20|            2|(133477,[1171,372...|       11|    5580836|         18|\n",
            "|               Mix|        false|24303| 1446595200|        41|        32|            1|(133477,[565,1402...|       17|    9859435|         30|\n",
            "|       Wild Things|        false|24622| 1508803200|       108|        94|            1|(133477,[2,1753,2...|       55|   23296152|         88|\n",
            "|          espaÃ±ol |        false|35696| 1500422400|        30|        26|            1|(133477,[4659,609...|       18|    6338592|         19|\n",
            "|              hype|        false|24515| 1461369600|        34|        31|            2|(133477,[4138,524...|       18|    7390554|         28|\n",
            "|              jamz|        false| 3451| 1490832000|       143|       119|            1|(133477,[276,1439...|        9|   35734173|         87|\n",
            "| party party party|        false|24706| 1509062400|       111|        96|            4|(133477,[258,299,...|       14|   24131476|         70|\n",
            "|     !! setlist !!|        false|84989| 1468454400|        23|        10|            1|(133477,[569,1741...|        2|    4784783|          4|\n",
            "|     #boostyourrun|        false|24105| 1411603200|        21|        20|            1|(133477,[16791,20...|        2|    4762022|         20|\n",
            "|     #boostyourrun|        false|84725| 1401235200|        21|        20|            1|(133477,[284,4423...|        2|    5053337|         19|\n",
            "|             #mood|        false|84298| 1508198400|        36|        33|            1|(133477,[627,1763...|       25|    8333889|         30|\n",
            "|               '16|        false| 3337| 1478217600|        25|        25|            1|(133477,[1677,220...|       14|    5357192|         24|\n",
            "|               '17|        false|24690| 1508716800|       206|       157|            1|(133477,[926,1227...|       27|   47330512|        120|\n",
            "|           'Merica|        false| 3639| 1435968000|        26|        21|            1|(133477,[2730,299...|        3|    6908434|         17|\n",
            "|           'Merica|        false| 3953| 1468713600|        77|        69|            1|(133477,[13,1155,...|        3|   18653965|         51|\n",
            "|          'Merica!|        false|84876| 1404432000|        22|        22|            1|(133477,[3319,747...|        2|    5243969|         21|\n",
            "|               ---|        false|24893| 1484179200|        57|        41|            5|(133477,[4638,500...|       17|   13571222|         33|\n",
            "+------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              Alone|        false|20080| 1496880000|        50|        48|            1|(52469,[350,529,6...|       19|   11128916|         42|\n",
            "|       Country Hits|        false|14258| 1504828800|       130|        77|            1|(52469,[1,763,966...|       61|   28245005|         44|\n",
            "|              Peter|        false|14812| 1446595200|        65|        56|            1|(52469,[3,175,180...|       15|   14212644|         47|\n",
            "|          Rock.....|        false| 1938| 1500163200|        57|        39|            1|(52469,[1123,1220...|        5|   16766883|         19|\n",
            "|               punk|        false| 1180| 1504483200|        67|        45|            1|(52469,[180,285,1...|       21|   15393647|         26|\n",
            "|#CODGhosts Playlist|        false|14362| 1382918400|        50|        50|            1|(52469,[955,1254,...|        2|   13601217|         50|\n",
            "|             #Dance|        false|14103| 1463961600|        33|        28|            1|(52469,[98,100,52...|       12|    6901015|         24|\n",
            "|      #boostyourrun|        false|14325| 1425686400|        21|        21|            1|(52469,[6615,1991...|        2|    5002212|         20|\n",
            "|      #boostyourrun|        false|20976| 1399075200|        21|        21|            1|(52469,[933,1752,...|        2|    5400100|         19|\n",
            "|      #boostyourrun|        false|78200| 1402704000|        22|        22|            1|(52469,[5472,7432...|        2|    5825915|         21|\n",
            "|           #college|        false|78334| 1490400000|        21|        19|            2|(52469,[174,1308,...|        4|    5068086|         18|\n",
            "|               #tbt|        false| 1168| 1436832000|        29|        22|            1|(52469,[1744,1895...|        2|    6829177|         15|\n",
            "|             $andy$|        false|78110| 1457395200|       117|        63|            1|(52469,[93,867,95...|       14|   28982414|         40|\n",
            "|                '17|        false|20507| 1505433600|        38|        31|            2|(52469,[433,771,1...|       31|    8051141|         30|\n",
            "|                '17|        false|78571| 1509235200|        34|        29|            1|(52469,[94,1973,7...|       16|    7500239|         26|\n",
            "|            'Merica|        false|78912| 1499126400|       241|       237|            1|(52469,[10,175,40...|        6|   57441819|        217|\n",
            "|           'merica |        false|78916| 1499990400|        67|        45|            1|(52469,[44,763,76...|       21|   14868475|         35|\n",
            "|   **That New New**|        false|14379| 1432771200|        63|        52|            1|(52469,[14,93,953...|       21|   14353567|         47|\n",
            "|           *HANNAH*|        false|20194| 1506211200|        84|        22|            1|(52469,[5733,6189...|        5|   11964693|          6|\n",
            "|             *sigh*|        false|14217| 1509321600|       154|       101|            4|(52469,[715,955,1...|       25|   38055908|         64|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None, 52469, 681805)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "songs_embeddings.show(), artists_embeddings.show(), ARTIST_VECTOR_LENGTH, SONGS_VECTOR_LENGTH"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lPOD60GQQGse"
      },
      "source": [
        "# Convert PySpark DataFrame into PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "outputs": [],
      "source": [
        "def convert_sparse_to_indices(df: DataFrame, column_name: str) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "\n",
        "  @F.udf(returnType=ArrayType(IntegerType()))\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    indices_list = item.indices.tolist()\n",
        "    padding_width = max_songs - len(indices_list)\n",
        "    return indices_list + [-1] * padding_width\n",
        "  \n",
        "  max_songs = songs_embeddings.select(F.max(\"num_tracks\")).first()[0]\n",
        "  print(f\"Max number of songs: {max_songs}\")\n",
        "  df = df.withColumn(f\"{column_name}_indices\", transform_array(F.col(column_name))).drop(column_name)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eIIqDVeN7cU0"
      },
      "outputs": [],
      "source": [
        "def padded_tensors_to_sparse_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    rows.append(sparse_tensor)\n",
        "  return torch.stack(rows)\n",
        "\n",
        "def padded_tensors_to_dense_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    dense = sparse_tensor.to_dense()\n",
        "    rows.append(dense)\n",
        "  unpadded = torch.stack(rows)\n",
        "  return unpadded"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UNak5bEo4cmV"
      },
      "source": [
        "In the paper they have two matrices,l et $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pKgJx26s73",
        "outputId": "fe16bfd0-5d64-4cf5-d160-1e7dbea99d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The median size 3296798 B (< 50 MB) of the parquet files is too small. Total size: 26620687 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///Users/dov/Desktop/big-data-project/data/cache/20230619022601-appid-local-1687134356630-b600be38-77c8-4ef8-aad4-b034881bf1d5/part-00003-aba90c37-e728-45e2-98d3-14e71a7aeaba-c000.parquet, ...\n",
            "23/06/19 02:26:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "98465 98465 98465\n"
          ]
        }
      ],
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "\n",
        "CACHE = os.path.join(GDRIVE_DATA_DIR, \"cache\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df = convert_sparse_to_indices(songs_embeddings.select(\"tracks\", \"pid\"), column_name=\"tracks\")\n",
        "# songs_converter = make_spark_converter(pytorch_songs_df)\n",
        "pytorch_artists_df = convert_sparse_to_indices(artists_embeddings.withColumnRenamed(\"tracks\", \"artists\").select(\"pid\", \"artists\"), column_name=\"artists\")\n",
        "# artist_converter = make_spark_converter(pytorch_artists_df)\n",
        "songs_artists_df = pytorch_songs_df.join(pytorch_artists_df, on=\"pid\")\n",
        "pytorch_merged_dataloader = make_spark_converter(songs_artists_df)\n",
        "\n",
        "\n",
        "print(pytorch_songs_df.count(), pytorch_artists_df.count(), songs_artists_df.count()) #Everything good here, this is nice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# songs_embeddings.show(), artists_embeddings.show()\n",
        "# artists_embeddings = artists_embeddings.withColumnRenamed(\"tracks\", \"artists\").select(\"pid\", \"artists\")\n",
        "# df = songs_embeddings.join(artists_embeddings, on=\"pid\").show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the dataloader for the evaluation set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Av0W0P66lC"
      },
      "source": [
        "# PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = torch.device(\"mps\")\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "        self.z = None\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "        # encoder_h_l2 = (torch.sum(self.weights['encoder_h']) ** 2)/2\n",
        "        # decoder_b_l2 = (torch.sum(self.biases['decoder_b']) ** 2)/2\n",
        "        # encoder_b_l2 = (torch.sum(self.biases['encoder_b']) ** 2)/2\n",
        "\n",
        "        encoder_h_l1 = torch.sum(torch.abs(self.weights['encoder_h']))\n",
        "        decoder_b_l1 = torch.sum(torch.abs(self.biases['decoder_b']))\n",
        "        encoder_b_l1 = torch.sum(torch.abs(self.biases['encoder_b']))\n",
        "\n",
        "        # l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2\n",
        "        l1 = encoder_h_l1 + decoder_b_l1 + encoder_b_l1\n",
        "        return l1\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x.t()\n",
        "        self.y = y.t()\n",
        "        \n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p= 1 - self.input_keep_prob)\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "\n",
        "        self.z = encoder_op\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       0.55 * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "            \n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "    #   encoder_h_l2 = (torch.sum(self.weights['encoder_h']) ** 2)/2\n",
        "    #   decoder_b_l2 = (torch.sum(self.biases['decoder_b']) ** 2)/2\n",
        "    #   encoder_b_l2 = (torch.sum(self.biases['encoder_b']) ** 2)/2\n",
        "    #   decoder_h_l2 = (torch.sum(self.weights['decoder_h']) ** 2)/2\n",
        "      \n",
        "      encoder_h_l1 = torch.sum(torch.abs(self.weights['encoder_h']))\n",
        "      decoder_b_l1 = torch.sum(torch.abs(self.biases['decoder_b']))\n",
        "      encoder_b_l1 = torch.sum(torch.abs(self.biases['encoder_b']))\n",
        "      decoder_h_l1 = torch.sum(torch.abs(self.weights['decoder_h']))\n",
        "\n",
        "\n",
        "      l1 = encoder_h_l1 + decoder_b_l1 + encoder_b_l1 + decoder_h_l1\n",
        "\n",
        "    #   l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2 + decoder_h_l2\n",
        "\n",
        "      return l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aykoeJfhOUBV"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGpMdqs7dQQ",
        "outputId": "2001aebc-9e53-4b86-fb7d-2a2ba9463fed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DAE_tied()"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"best_params.pickle\")\n",
        "BEST_PARAMS_PATH_2 = os.path.join(SAVED_MODELS, \"best_params_reg.pickle\")\n",
        "\n",
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 32,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH,\n",
        "    'hidden': 100,\n",
        "    'lr': 0.001,\n",
        "    'reg_lambda': 0.0001,\n",
        "    'initval': \"NULL\",\n",
        "    \"keep_prob\": 0.8, \n",
        "    \"input_keep_prob\": 0.8, # This isn't used for now because of the .uniform()\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}\n",
        "pretrain_model = DAE_tied(conf)\n",
        "pretrain_model.init_weight()\n",
        "pretrain_model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mNPmA2YijSO1"
      },
      "outputs": [],
      "source": [
        "pretrain_optimizer = optim.Adam(pretrain_model.d_params, lr=conf[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jAqv6QBEiK9",
        "outputId": "c0716d37-5a7f-4966-c860-65e8041a4f74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps', index=0)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_model.weights['encoder_h'].device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the validation function that is invoked during the training in order to save the model parameters that optimize the performance evaluation on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "def k_prec(input: torch.Tensor, eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    batch_size = 32\n",
        "    counts = []\n",
        "    confidence_list = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        input_idx = torch.nonzero(input[i] == 1).squeeze().flatten()\n",
        "        num_input_songs = input_idx.shape[0]\n",
        "\n",
        "        ground_truth_idx = torch.nonzero(ground_truth[i] == 1).squeeze().flatten()\n",
        "        num_ground_truth_songs = ground_truth_idx.shape[0]\n",
        "\n",
        "        k = num_input_songs + num_ground_truth_songs \n",
        "\n",
        "        top_k_preds = eval_preds[i].topk(k, dim=0)\n",
        "        top_k_preds_idx = top_k_preds.indices.flatten().cpu()\n",
        "        confidences = top_k_preds.values.flatten()\n",
        "\n",
        "        already_in_playlist = np.intersect1d(top_k_preds_idx.detach().numpy(), input_idx.cpu().detach().numpy())\n",
        "\n",
        "        top_k_preds_idx = np.array([item for item in top_k_preds_idx if item not in already_in_playlist])\n",
        "\n",
        "        common_elements = np.intersect1d(top_k_preds_idx, ground_truth_idx.cpu().detach().numpy())\n",
        "        num_common_elements = len(common_elements)\n",
        "        counts.append(num_common_elements)\n",
        "        confidence_list.append(confidences)\n",
        "\n",
        "    return confidence_list, counts\n",
        "\n",
        "def ndcg(eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> float:\n",
        "    return 0\n",
        "\n",
        "def evaluate(input: torch.Tensor, eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    return k_prec(input, eval_preds, ground_truth)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the petastorm converters for validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n",
            "Max number of songs: 250\n",
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "pytorch_songs_eval_train_df = convert_sparse_to_indices(songs_embeddings_eval_train.withColumnRenamed(\"tracks\", \"train_tracks\").select(\"train_tracks\", \"pid\"), column_name=\"train_tracks\")\n",
        "pytorch_artists_eval_train_df = convert_sparse_to_indices(artists_embeddings_eval_train.withColumnRenamed(\"tracks\", \"train_artists\").select(\"pid\", \"train_artists\"), column_name=\"train_artists\")\n",
        "songs_artists_eval_train_df = pytorch_songs_eval_train_df.join(pytorch_artists_eval_train_df, on=\"pid\")\n",
        "\n",
        "pytorch_songs_eval_test_df = convert_sparse_to_indices(songs_embeddings_eval_test.withColumnRenamed(\"tracks\", \"test_tracks\").select(\"test_tracks\", \"pid\"), column_name=\"test_tracks\")\n",
        "pytorch_artists_eval_test_df = convert_sparse_to_indices(artists_embeddings_eval_test.withColumnRenamed(\"tracks\", \"test_artists\").select(\"pid\", \"test_artists\"), column_name=\"test_artists\")\n",
        "songs_artists_eval_test_df = pytorch_songs_eval_test_df.join(pytorch_artists_eval_test_df, on=\"pid\")\n",
        "\n",
        "eval_merged_df = songs_artists_eval_train_df.join(songs_artists_eval_test_df, on=\"pid\")\n",
        "\n",
        "counter = F.udf(lambda x: len([item for item in x if item != -1]), returnType=IntegerType())\n",
        "eval_merged_df = eval_merged_df\\\n",
        "    .withColumn(\"train_tracks_count\", counter(F.col(\"train_tracks_indices\")))\\\n",
        "    .withColumn(\"test_tracks_count\", counter(F.col(\"test_tracks_indices\")))\n",
        "eval_merged_df = eval_merged_df.filter(\"train_tracks_count > 100\")\n",
        "eval_merged_dataloader = make_spark_converter(eval_merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_merged_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model: DAE_tied) -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"\n",
        "    Given the model, performs an evaluation on the validation set.\n",
        "    \"\"\"\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    precs = []\n",
        "    confidence_list = []\n",
        "    tot_k = 0\n",
        "    model.eval()\n",
        "    with eval_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = 1) as eval_dataloader:\n",
        "        for batch_idx, row in enumerate(eval_dataloader):\n",
        "            with torch.no_grad():\n",
        "                if batch_idx == 1:\n",
        "                    break #TODO: faster but less generalized, remove for the final training\n",
        "                padded_eval_song_tensor = row[\"train_tracks_indices\"]\n",
        "                padded_eval_artist_tensor = row[\"train_artists_indices\"]\n",
        "                \n",
        "                song_dense = padded_tensors_to_dense_matrix(padded_eval_song_tensor, SONG_SHAPE)\n",
        "                artist_dense = padded_tensors_to_dense_matrix(padded_eval_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "                song_dense = song_dense.to(device)\n",
        "                artist_dense = artist_dense.to(device)\n",
        "\n",
        "                del padded_eval_song_tensor\n",
        "                del padded_eval_artist_tensor\n",
        "                \n",
        "                x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "                y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "                \n",
        "                model(x, y)\n",
        "\n",
        "                eval_preds = model.y_pred[:, :SONGS_VECTOR_LENGTH]\n",
        "\n",
        "                padded_eval_song_tensor_test = row[\"test_tracks_indices\"]\n",
        "                \n",
        "                ground_truth = padded_tensors_to_dense_matrix(padded_eval_song_tensor_test, SONG_SHAPE)\n",
        "\n",
        "                ground_truth = ground_truth.to(device)\n",
        "\n",
        "                k = torch.sum(ground_truth == 1, dim=1)\n",
        "                tot_k += torch.sum(k).item()\n",
        "                confidences, prec_list = evaluate(song_dense, eval_preds, ground_truth)\n",
        "                precs.extend(prec_list)\n",
        "                confidence_list.extend(confidences)\n",
        "\n",
        "        mean_prec: float = sum(precs) / tot_k\n",
        "        model.train()\n",
        "        return confidence_list, model.cost, mean_prec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_validation_step(model: DAE_tied, max_prec: float, save_path:str):\n",
        "    confidences, eval_loss, prec = validate(model)\n",
        "    \n",
        "    if prec > max_prec:\n",
        "        max_prec = prec\n",
        "        best_params = [param.cpu().detach().numpy() for param in model.d_params]\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(best_params, f)\n",
        "        print(f\"Best prec achieved: {prec}, parameters saved!\")\n",
        "    return confidences, max_prec"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing if the ordering is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tqdm.notebook import tqdm\n",
        "# import random\n",
        "# # os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "# NUM_EPOCHS = 5\n",
        "# with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "#     ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "#     SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "#     for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "#       # Pick random input_keep_prob between 0.5 and 0.8\n",
        "#     #   pretrain_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "#       pretrain_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "#       padded_song_tensor = row[\"tracks_indices\"]\n",
        "#       padded_artist_tensor = row[\"artists_indices\"]\n",
        "      \n",
        "#       song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)[:, 1:]\n",
        "#       artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)[:, 1:]\n",
        "\n",
        "#       song_indices = sorted(list(set(torch.nonzero(song_dense[0] == 1).squeeze().flatten().cpu().detach().tolist())))\n",
        "#       print(torch.nonzero(song_dense[0] == 1))\n",
        "#       songs_artists_df.show()\n",
        "#       artist_indices = torch.nonzero(artist_dense == 1).squeeze().flatten()\n",
        "\n",
        "#     #   print(song_indices)\n",
        "#     #   print(artist_indices)\n",
        "#       break\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GuqQMu_E0yaN"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jIg2DIDNjEKm"
      },
      "outputs": [],
      "source": [
        "min_loss = 2000\n",
        "max_prec = 0\n",
        "best_params = []\n",
        "losses = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvPuc5hqirK"
      },
      "source": [
        "Pretrain with `DAE_tied`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "800d36c1bd2f4519ad7215f62fe694b2",
            "025feace7e5548cebe8d42356ed1963e",
            "2e5ed18f6ed644898269df6699939a96",
            "210989b9506346409dcb494dc08a7c28",
            "bada53838d434c47bfd5185687d2e133",
            "4cc43c4c34974993ad879ab6789719dc",
            "3a7a8265d2f84b18b0556518474586cc",
            "56ff9818b0e24a4ca8fbe94afc45b797",
            "3af36fc968654a27826c33cbfa6a7b62",
            "1ecbd9fd2159484e9fd5f1ad4028717e",
            "c54d6d9cf7614f20b5f000c29c5a5b18"
          ]
        },
        "id": "Z_BjjpshDD2H",
        "outputId": "e1ab4815-5fa7-433f-b258-bb6317b2d657"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c8205b0fb3492eab74b13e5f8be12f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training model:   0%|          | 0/6250.0 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 279970.5\n",
            "Current max precision: 0\n",
            "Best prec achieved: 0.005388760585065435, parameters saved!\n",
            "Best prec achieved: 0.016166281755196306, parameters saved!\n",
            "Best prec achieved: 0.016936104695919937, parameters saved!\n",
            "Best prec achieved: 0.018475750577367205, parameters saved!\n",
            "Best prec achieved: 0.020785219399538105, parameters saved!\n",
            "Best prec achieved: 0.02386451116243264, parameters saved!\n",
            "Best prec achieved: 0.024634334103156273, parameters saved!\n",
            "Best prec achieved: 0.02848344880677444, parameters saved!\n",
            "Best prec achieved: 0.030792917628945343, parameters saved!\n",
            "Best prec achieved: 0.03387220939183987, parameters saved!\n",
            "Best prec achieved: 0.03464203233256351, parameters saved!\n",
            "Best prec achieved: 0.037721324095458045, parameters saved!\n",
            "Best prec achieved: 0.04157043879907621, parameters saved!\n",
            "Best prec achieved: 0.04311008468052348, parameters saved!\n",
            "Best prec achieved: 0.044649730561970746, parameters saved!\n",
            "Best prec achieved: 0.04541955350269438, parameters saved!\n",
            "Loss: 39600.0625\n",
            "Current max precision: 0.04541955350269438\n",
            "Best prec achieved: 0.046189376443418015, parameters saved!\n",
            "Best prec achieved: 0.04695919938414165, parameters saved!\n",
            "Best prec achieved: 0.04772902232486528, parameters saved!\n",
            "Loss: 15434.505859375\n",
            "Current max precision: 0.04772902232486528\n",
            "Best prec achieved: 0.049268668206312545, parameters saved!\n",
            "Best prec achieved: 0.05003849114703618, parameters saved!\n",
            "Best prec achieved: 0.05234795996920708, parameters saved!\n",
            "Loss: 9612.02734375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 6846.693359375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 4960.04248046875\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 4179.521484375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 3453.073486328125\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 3355.96044921875\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2765.519287109375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2607.03466796875\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2372.95068359375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2227.4462890625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2293.884765625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2003.9322509765625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1931.813232421875\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2001.976318359375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1818.77734375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1804.8837890625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1672.349365234375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1815.037353515625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 2025.092529296875\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1752.0433349609375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1574.644287109375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1634.8970947265625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1645.92578125\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1557.666748046875\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1624.3909912109375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1597.3011474609375\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1577.204345703125\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1527.4117431640625\n",
            "Current max precision: 0.05234795996920708\n",
            "Loss: 1521.576171875\n",
            "Current max precision: 0.05234795996920708\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \n\u001b[1;32m     44\u001b[0m   losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m---> 46\u001b[0m confidences, max_prec \u001b[39m=\u001b[39m perform_validation_step(pretrain_model, max_prec, BEST_PARAMS_PATH)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m30\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[27], line 2\u001b[0m, in \u001b[0;36mperform_validation_step\u001b[0;34m(model, max_prec, save_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_validation_step\u001b[39m(model: DAE_tied, max_prec: \u001b[39mfloat\u001b[39m, save_path:\u001b[39mstr\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     confidences, eval_loss, prec \u001b[39m=\u001b[39m validate(model)\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m prec \u001b[39m>\u001b[39m max_prec:\n\u001b[1;32m      5\u001b[0m         max_prec \u001b[39m=\u001b[39m prec\n",
            "Cell \u001b[0;32mIn[26], line 43\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     41\u001b[0m k \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(ground_truth \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m tot_k \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(k)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 43\u001b[0m confidences, prec_list \u001b[39m=\u001b[39m evaluate(song_dense, eval_preds, ground_truth)\n\u001b[1;32m     44\u001b[0m precs\u001b[39m.\u001b[39mextend(prec_list)\n\u001b[1;32m     45\u001b[0m confidence_list\u001b[39m.\u001b[39mextend(confidences)\n",
            "Cell \u001b[0;32mIn[23], line 35\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(input, eval_preds, ground_truth)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor, eval_preds: torch\u001b[39m.\u001b[39mTensor, ground_truth: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m k_prec(\u001b[39minput\u001b[39;49m, eval_preds, ground_truth)\n",
            "Cell \u001b[0;32mIn[23], line 17\u001b[0m, in \u001b[0;36mk_prec\u001b[0;34m(input, eval_preds, ground_truth)\u001b[0m\n\u001b[1;32m     14\u001b[0m k \u001b[39m=\u001b[39m num_input_songs \u001b[39m+\u001b[39m num_ground_truth_songs \n\u001b[1;32m     16\u001b[0m top_k_preds \u001b[39m=\u001b[39m eval_preds[i]\u001b[39m.\u001b[39mtopk(k, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m top_k_preds_idx \u001b[39m=\u001b[39m top_k_preds\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mflatten()\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m     18\u001b[0m confidences \u001b[39m=\u001b[39m top_k_preds\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     20\u001b[0m already_in_playlist \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mintersect1d(top_k_preds_idx\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), input_idx\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "NUM_EPOCHS = 2\n",
        "with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "      # Pick random input_keep_prob between 0.5 and 0.8\n",
        "    #   pretrain_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "      padded_song_tensor = row[\"tracks_indices\"]\n",
        "      padded_artist_tensor = row[\"artists_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      rand_int = np.random.randint(2)\n",
        "      if rand_int == 0:\n",
        "        #Zero-out the artists\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        # x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1).t()\n",
        "        x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        pretrain_model(x, y)\n",
        "        loss = pretrain_model.cost\n",
        "        pretrain_model.cost.backward()\n",
        "        pretrain_optimizer.step()\n",
        "      if rand_int == 1:\n",
        "        #Zero-out the tracks\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        # x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1).t()\n",
        "        x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        pretrain_model(x, y)\n",
        "        loss = pretrain_model.cost\n",
        "        pretrain_model.cost.backward()\n",
        "        pretrain_optimizer.step()\n",
        "\n",
        "      if batch_idx % 20 == 0:  \n",
        "        losses.append(loss)\n",
        "\n",
        "      confidences, max_prec = perform_validation_step(pretrain_model, max_prec, BEST_PARAMS_PATH)\n",
        "\n",
        "      if batch_idx % 30 == 0:\n",
        "        print(f\"Loss: {loss}\")\n",
        "        print(f\"Current max precision: {max_prec}\")\n",
        "        # print(f\"Current confidences: \", confidences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F85w_ZpmPD9t"
      },
      "outputs": [],
      "source": [
        "params = [param.cpu().detach().numpy() for param in pretrain_model.d_params]\n",
        "with open(BEST_PARAMS_PATH_2, 'wb') as f:\n",
        "  pickle.dump(params, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOoS7eGqlrH"
      },
      "source": [
        "Train with `DAE` loading the pretrained `DAE_tied` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IFAj48uuqlbd"
      },
      "outputs": [],
      "source": [
        "conf = {\n",
        "    'batch': 32,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH,\n",
        "    'hidden': 50,\n",
        "    'lr': 0.005,\n",
        "    'reg_lambda': 0.0001,\n",
        "    'initval': BEST_PARAMS_PATH,\n",
        "    \"keep_prob\": 0.8, \n",
        "    \"input_keep_prob\": 0.8, # This isn't used for now because of the .uniform()\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}\n",
        "dae_model = DAE(conf)\n",
        "dae_model.init_weight()\n",
        "optimizer = optim.Adam(dae_model.d_params, lr=conf['lr'])\n",
        "\n",
        "min_loss = 600\n",
        "losses = []\n",
        "best_params = []\n",
        "max_prec = 0\n",
        "FINE_TUNED_BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"final_best_params.pickle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gyw3SnVYyRP1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b584a3ee494246579479476cbe3d9493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training model:   0%|          | 0/6250.0 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best prec achieved: 0.05157813702848345, parameters saved!\n",
            "Loss: 14707.8505859375\n",
            "Current max precision: 0.05157813702848345\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "NUM_EPOCHS = 2\n",
        "with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "      # Pick random input_keep_prob between 0.5 and 0.8\n",
        "    #   dae_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "      padded_song_tensor = row[\"tracks_indices\"]\n",
        "      padded_artist_tensor = row[\"artists_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      rand_int = np.random.randint(2)\n",
        "      if rand_int == 0:\n",
        "        #Zero-out the artists\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "      if rand_int == 1:\n",
        "        #Zero-out the tracks\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if batch_idx % 20 == 0:  \n",
        "        losses.append(loss)\n",
        "\n",
        "      confidences, max_prec = perform_validation_step(dae_model, max_prec, FINE_TUNED_BEST_PARAMS_PATH)\n",
        "    \n",
        "      if batch_idx % 30 == 0:\n",
        "        print(f\"Loss: {loss}\")\n",
        "        print(f\"Current max precision: {max_prec}\")\n",
        "        # k = torch.sum(y == 1, dim=0)\n",
        "        # counts = evaluate(pretrain_model.y_pred, y.t(), k)\n",
        "        # print(\"Precisions: \", torch.tensor(counts)/k.cpu())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JQL3P9JByR7Q"
      },
      "source": [
        "Let's see how the loss decreases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF9gP0FMLdw2"
      },
      "outputs": [],
      "source": [
        "SAVE_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"model_new.pickle\")\n",
        "params = [param.cpu().detach().numpy() for param in dae_model.d_params]\n",
        "with open(SAVE_MODEL_PATH, 'wb') as f:\n",
        "  pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4JxgOCvx2TE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(len(losses))\n",
        "plt.plot(x, [loss.item() for loss in losses])\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Progression')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqyho9fPl0ow"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9RFO76gK8Qb"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lgIPJo2tbUL"
      },
      "outputs": [],
      "source": [
        "SONGS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "songs_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "songs_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "artists_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "artists_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "TEST_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "TEST_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "test_train_df = spark.read.schema(playlist_schema).json(TEST_TRAIN_DF_PATH)\n",
        "test_test_df = spark.read.schema(playlist_schema).json(TEST_TEST_DF_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKbyd0kRl3SH"
      },
      "outputs": [],
      "source": [
        "def construct_prediction_df(prediction: torch.Tensor, mapping: DataFrame, top_n: int = 50) -> DataFrame:\n",
        "  pred_np = prediction.detach().numpy()\n",
        "  indexes = np.arange(pred_np.shape[0]) # To compensate the index start at 1\n",
        "  schema = StructType([\n",
        "      StructField(\"pos\", IntegerType()),\n",
        "      StructField(\"confidence\", FloatType())\n",
        "  ])\n",
        "  prediction_df = spark.createDataFrame([(pos, conf) for pos, conf in zip(indexes.tolist(), pred_np.tolist())],schema)\n",
        "  prediction_info = prediction_df.join(mapping, \"pos\")\n",
        "  return prediction_info\n",
        "\n",
        "# prediction_df = construct_prediction_df(prediction, songs_df_test)\n",
        "# prediction_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ9LquCdSbJJ"
      },
      "outputs": [],
      "source": [
        "def remove_existing_tracks(playlist_tracks: DataFrame, recommendations_df: DataFrame) -> DataFrame:\n",
        "  playlist_tracks = playlist_tracks.select(\"track_uri\").cache()\n",
        "  playlist_tracks_compatible = playlist_tracks.join(F.broadcast(recommendations_df), on=\"track_uri\")\n",
        "  playlist_tracks.unpersist()\n",
        "  return recommendations_df.exceptAll(F.broadcast(playlist_tracks_compatible))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx-tghy0bAkU"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb793kgebBwJ"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(recommendations, ground_truth, num_of_recommendations) -> float:\n",
        "    \"\"\"\n",
        "    Calculates precision at k for the recommendations.\n",
        "    \"\"\"\n",
        "    recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").cache()\n",
        "    reccomended_relevant_tracks_count = recommended_relevant_tracks.count() #this can be top_n_results.join in order to be more performant\n",
        "    recommended_relevant_tracks.unpersist()\n",
        "    precision = reccomended_relevant_tracks_count / float(num_of_recommendations)\n",
        "\n",
        "    return precision\n",
        "\n",
        "\n",
        "import math\n",
        "def normalized_discounted_cumulative_gain(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "  recommendations_list = recommendations.collect()\n",
        "  cumulative_gain = 0\n",
        "\n",
        "  intersection = recommendations.join(ground_truth, \"track_uri\").count()\n",
        "  if intersection == 0: return 0\n",
        "\n",
        "  ideal_cumulative_gain = 1 + np.array([(1 / math.log(i, 2)) for i in range(2, 2+intersection)]).sum() #TODO: replace this with sum([])\n",
        "  for index, row in enumerate(recommendations_list):\n",
        "    i = index + 1\n",
        "    is_rel = ground_truth.filter(F.col(\"track_uri\").isin(row.track_uri)).count() > 0\n",
        "    rel = 1 if is_rel else 0\n",
        "    if i == 1:\n",
        "      cumulative_gain += rel\n",
        "    else:\n",
        "      cumulative_gain += (rel / math.log(i, 2))\n",
        "  return cumulative_gain / ideal_cumulative_gain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VUYEe5iHbTHs"
      },
      "source": [
        "Creating the dataloaders for the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYXLsGIrbH2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting floating-point columns to float32                                    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The median size 137021 B (< 50 MB) of the parquet files is too small. Total size: 233150 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///Users/dov/Desktop/big-data-project/data/cache/20230613203504-appid-local-1686674016052-044c5a3b-7e2e-4703-bde1-c5581a209825/part-00001-2209ea1f-fd7e-4e79-b87b-e7b847d72234-c000.parquet, ...\n",
            "Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The median size 19056 B (< 50 MB) of the parquet files is too small. Total size: 134516 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///Users/dov/Desktop/big-data-project/data/cache/20230613203506-appid-local-1686674016052-989e6c66-19d3-4e2c-9408-5bb7a5ea72a5/part-00000-c95c506b-0a0b-41e6-8a3b-3a5d31d76cac-c000.parquet, ...\n"
          ]
        }
      ],
      "source": [
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df_test = convert_sparse_to_indices(songs_embeddings_test_train.select(\"tracks\"))\n",
        "songs_converter_test = make_spark_converter(pytorch_songs_df_test)\n",
        "\n",
        "pytorch_artists_df_test = convert_sparse_to_indices(artists_embeddings_test_train.select(\"tracks\"))\n",
        "artist_converter_test = make_spark_converter(pytorch_artists_df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h7NRo3Ggt3h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DAE()"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 32,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH,\n",
        "    'hidden': 50,\n",
        "    'lr': 0.0001, #original 0.001\n",
        "    'reg_lambda': 0.0,\n",
        "    'initval': BEST_PARAMS_PATH, #TODO: change it in fine_tuned path\n",
        "    \"keep_prob\": 1,\n",
        "    \"input_keep_prob\": 1,\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}\n",
        "dae_model_test = DAE(conf)\n",
        "dae_model_test.init_weight()\n",
        "dae_model_test.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn2zrRCSdKQO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc248691ce9346268ecdfbdc206f3b47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluation...:   0%|          | 0/3125.0 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "with songs_converter_test.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs =1) as songs_dataloader:\n",
        "  with artist_converter_test.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs=1) as artists_dataloader:\n",
        "    zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "    for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Evaluation...\", total= (NUM_PLAYLISTS / conf[\"batch\"])):\n",
        "      padded_song_tensor = song[\"embedding_indices\"]\n",
        "      padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "      y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "      \n",
        "      dae_model_test(x,y)\n",
        "      \n",
        "      result = dae_model_test.y_pred\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRZ9w5nr5TaK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0261, 0.0018, 0.0084,  ..., 0.0003, 0.0004, 0.0004],\n",
              "        [0.0270, 0.0019, 0.0088,  ..., 0.0003, 0.0004, 0.0004],\n",
              "        [0.0272, 0.0019, 0.0089,  ..., 0.0003, 0.0004, 0.0004],\n",
              "        ...,\n",
              "        [0.0292, 0.0022, 0.0098,  ..., 0.0004, 0.0005, 0.0005],\n",
              "        [0.0262, 0.0018, 0.0084,  ..., 0.0003, 0.0004, 0.0004],\n",
              "        [0.0264, 0.0018, 0.0086,  ..., 0.0003, 0.0004, 0.0004]],\n",
              "       device='mps:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg8V8VSSu3XI"
      },
      "outputs": [],
      "source": [
        "# torch.concat((song_dense[:, 1:], artist_dense[:, 1:]), dim=1).t().shape\n",
        "# (song_dense[:, 0] == 0.).all(), (artist_dense[:, 0] == 0.).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLpygSgaMVr6"
      },
      "outputs": [],
      "source": [
        "# result = result.to(\"cpu\")\n",
        "# prediction_df = construct_prediction_df(result[10][:SONGS_VECTOR_LENGTH_TEST], songs_df_test, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uYwzqLFMjWE"
      },
      "outputs": [],
      "source": [
        "# prediction_df.orderBy(\"pos\").show(truncate=False)\n",
        "# songs_df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0wojIXkuFnN"
      },
      "outputs": [],
      "source": [
        "# def evaluate_batch(batch_result: torch.Tensor, batch_n: int) -> Tuple[float, float]:\n",
        "#   \"\"\"\n",
        "#   Returns the precision and NDCG for a given batch.\n",
        "#   \"\"\"\n",
        "  \n",
        "#   return 0,0\n",
        "\n",
        "# def perform_evaluation(songs_dataloader, artists_dataloader, test_set):\n",
        "#   \"\"\"\n",
        "#   Returns the precision and NDCG, averaged from all the samples in the test set\n",
        "#   \"\"\"\n",
        "#   with songs_converter_test.make_torch_dataloader(num_epochs =1) as songs_dataloader:\n",
        "#     with artist_converter_test.make_torch_dataloader(num_epochs=1) as artists_dataloader:\n",
        "#       zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "#       for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Evaluation...\", total= (NUM_PLAYLISTS / 32) * NUM_EPOCHS):\n",
        "#         padded_song_tensor = song[\"embedding_indices\"]\n",
        "#         padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "        \n",
        "#         song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "#         artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "#         x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "#         y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "#         dae_model(x,y)\n",
        "#         batch_result = dae_model.y_pred\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4LWsnwWeVAV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/13 20:36:00 WARN TaskSetManager: Stage 35 contains a task of very large size (1209 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71860 0.0 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[pos: int, confidence: float, track_uri: string]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing the first batch\n",
        "\n",
        "results = []\n",
        "# for i in tqdm(range(32)):\n",
        "PID = 71860\n",
        "ground_truth = test_test_df.filter(F.col(\"pid\") == PID).select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "playlist_train_songs = test_train_df.filter(F.col(\"pid\") == PID).select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "\n",
        "#Removing rare songs (that the model didn't consider)\n",
        "#This may be not the best approach since the train songs or ground truth may become 0\n",
        "clean_ground_truth = ground_truth.join(song_mapping, on=\"track_uri\").cache()\n",
        "clean_playlist_train_songs = playlist_train_songs.join(song_mapping, on=\"track_uri\").cache()\n",
        "\n",
        "# n_recommendations = ground_truth.count() or 1\n",
        "n_recommendations = 500\n",
        "result = result.cpu()\n",
        "\n",
        "#The result[i] has to be aligned with the PID. i != PID. \n",
        "prediction_df = construct_prediction_df(result[1][:SONGS_VECTOR_LENGTH], song_mapping, n_recommendations).cache()\n",
        "clean_prediction_df = remove_existing_tracks(clean_playlist_train_songs, prediction_df)\n",
        "\n",
        "clean_prediction_df = prediction_df.orderBy(F.col(\"confidence\").desc()).limit(n_recommendations).cache()\n",
        "\n",
        "prec = precision_at_k(clean_prediction_df, clean_ground_truth, n_recommendations)\n",
        "gain = normalized_discounted_cumulative_gain(clean_prediction_df, clean_ground_truth, n_recommendations)\n",
        "print(PID, prec, gain)\n",
        "results.append((prec, gain))\n",
        "\n",
        "ground_truth.unpersist()\n",
        "clean_ground_truth.unpersist()\n",
        "playlist_train_songs.unpersist()\n",
        "clean_playlist_train_songs.unpersist()\n",
        "prediction_df.unpersist()\n",
        "clean_prediction_df.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XUxFpPGVLf-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Playlist train songs\n",
            "+---+-------------+------------------------------------+-------------------------------------+---------------------------------------------------------------+------------------------------------+-----------+-----------------+\n",
            "|pos|artist_name  |track_uri                           |artist_uri                           |track_name                                                     |album_uri                           |duration_ms|album_name       |\n",
            "+---+-------------+------------------------------------+-------------------------------------+---------------------------------------------------------------+------------------------------------+-----------+-----------------+\n",
            "|17 |MIKA         |spotify:track:6AviHKu3ydzAePBmzEi62v|spotify:artist:5MmVJVhhYKQ86izuGHzJYA|Popular Song                                                   |spotify:album:6czdbbMtGbAkZ6ud2OMTcg|200213     |Yours Truly      |\n",
            "|8  |Ariana Grande|spotify:track:6EIsMa5lbvljYxqCkjZVDi|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Baby I                                                         |spotify:album:5xSvNPstcxHtR4ap2vvN8A|197600     |Yours Truly      |\n",
            "|16 |Ariana Grande|spotify:track:442j8VxaB60dWf9cBFuX5w|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Almost Is Never Enough                                         |spotify:album:6czdbbMtGbAkZ6ud2OMTcg|327773     |Yours Truly      |\n",
            "|12 |Ariana Grande|spotify:track:1xCqIXCApBgcjwRLostpKl|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Piano                                                          |spotify:album:5xSvNPstcxHtR4ap2vvN8A|234426     |Yours Truly      |\n",
            "|19 |Ariana Grande|spotify:track:7fYbFYt7X4FZvuJJC90EX0|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Problem                                                        |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|193893     |My Everything    |\n",
            "|9  |Ariana Grande|spotify:track:3yiopxxeHuwcpAg4e57Zjt|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Right There                                                    |spotify:album:5xSvNPstcxHtR4ap2vvN8A|247080     |Yours Truly      |\n",
            "|32 |Ariana Grande|spotify:track:3YQfqb3oFhDZa3iNcFu57G|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|You Don't Know Me                                              |spotify:album:5AMOKSM1ftb3opIbGT2d4q|233720     |My Everything    |\n",
            "|0  |Ariana Grande|spotify:track:1magKwGDsyU3RGjpo0BfPe|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Brand New You (feat. Brynn Williams & Caitlin Gann)            |spotify:album:48uMMsVHGfKipIsOuYcvjs|188093     |Brand New You    |\n",
            "|25 |Ariana Grande|spotify:track:26gXVZqHG3AThfmkXLlp3P|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Break Your Heart Right Back                                    |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|253386     |My Everything    |\n",
            "|5  |Ariana Grande|spotify:track:4r4V1wYecTxSAAXV11cFPD|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Focus                                                          |spotify:album:1TVt7uWfV3vLiU8bEmBEWL|211360     |Dangerous Woman  |\n",
            "|11 |Ariana Grande|spotify:track:7EpKfPAURnG9OCVer0S30N|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Lovin' It                                                      |spotify:album:5xSvNPstcxHtR4ap2vvN8A|180693     |Yours Truly      |\n",
            "|1  |Ariana Grande|spotify:track:5RNKIGhRllNHGjroVDPXat|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|A Little More Homework (feat. Graham Phillips) - Single Version|spotify:album:48uMMsVHGfKipIsOuYcvjs|302733     |Brand New You    |\n",
            "|37 |Ariana Grande|spotify:track:73pH60xHd2S87oNIlgztNY|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|True Love                                                      |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|166466     |Christmas & Chill|\n",
            "|21 |Ariana Grande|spotify:track:6LZkF2fayIxtA1SIMmAGoX|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Why Try                                                        |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|211880     |My Everything    |\n",
            "|2  |Ariana Grande|spotify:track:1ADjWm8QNhgNV8yCNNgQ1T|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Santa Tell Me                                                  |spotify:album:2Y42QS2bGi5NokHzjticau|204093     |Santa Tell Me    |\n",
            "|29 |Ariana Grande|spotify:track:0b0hbaQZnkFDOGjOUkIbUK|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|My Everything                                                  |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|168546     |My Everything    |\n",
            "|35 |Ariana Grande|spotify:track:1gCC4V2iW0juUv4jaDABsp|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|December                                                       |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|116800     |Christmas & Chill|\n",
            "|3  |Ariana Grande|spotify:track:2h1IPjP471JJRSShTHRUhi|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Dangerous Woman                                                |spotify:album:4lVR2fg3DAUQpGVJ6DciHW|235946     |Dangerous Woman  |\n",
            "|34 |Ariana Grande|spotify:track:2E5R3USq1SSOFDnl4SHe1Z|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Wit It This Christmas                                          |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|161680     |Christmas & Chill|\n",
            "|23 |Ariana Grande|spotify:track:1BP617VnYByf7FzCnLEjbg|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Best Mistake                                                   |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|233733     |My Everything    |\n",
            "+---+-------------+------------------------------------+-------------------------------------+---------------------------------------------------------------+------------------------------------+-----------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Clean playlist train songs\n",
            "+------------------------------------+---+-------------+-------------------------------------+---------------------------------------------------------------+------------------------------------+-----------+-----------------+------+\n",
            "|track_uri                           |pos|artist_name  |artist_uri                           |track_name                                                     |album_uri                           |duration_ms|album_name       |pos   |\n",
            "+------------------------------------+---+-------------+-------------------------------------+---------------------------------------------------------------+------------------------------------+-----------+-----------------+------+\n",
            "|spotify:track:6EIsMa5lbvljYxqCkjZVDi|8  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Baby I                                                         |spotify:album:5xSvNPstcxHtR4ap2vvN8A|197600     |Yours Truly      |440   |\n",
            "|spotify:track:1xCqIXCApBgcjwRLostpKl|12 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Piano                                                          |spotify:album:5xSvNPstcxHtR4ap2vvN8A|234426     |Yours Truly      |645   |\n",
            "|spotify:track:1gCC4V2iW0juUv4jaDABsp|35 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|December                                                       |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|116800     |Christmas & Chill|5231  |\n",
            "|spotify:track:1BP617VnYByf7FzCnLEjbg|23 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Best Mistake                                                   |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|233733     |My Everything    |6146  |\n",
            "|spotify:track:7fYbFYt7X4FZvuJJC90EX0|19 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Problem                                                        |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|193893     |My Everything    |15043 |\n",
            "|spotify:track:0b0hbaQZnkFDOGjOUkIbUK|29 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|My Everything                                                  |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|168546     |My Everything    |19506 |\n",
            "|spotify:track:2ofOe2OaXFpZF5ETbsc7Qu|7  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Honeymoon Avenue                                               |spotify:album:5xSvNPstcxHtR4ap2vvN8A|339733     |Yours Truly      |23797 |\n",
            "|spotify:track:1X2Zd5wKGbY1oKzb8dzJRy|22 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Break Free                                                     |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|214840     |My Everything    |28070 |\n",
            "|spotify:track:3yiopxxeHuwcpAg4e57Zjt|9  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Right There                                                    |spotify:album:5xSvNPstcxHtR4ap2vvN8A|247080     |Yours Truly      |31049 |\n",
            "|spotify:track:2E5R3USq1SSOFDnl4SHe1Z|34 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Wit It This Christmas                                          |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|161680     |Christmas & Chill|32035 |\n",
            "|spotify:track:73pH60xHd2S87oNIlgztNY|37 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|True Love                                                      |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|166466     |Christmas & Chill|32849 |\n",
            "|spotify:track:4r4V1wYecTxSAAXV11cFPD|5  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Focus                                                          |spotify:album:1TVt7uWfV3vLiU8bEmBEWL|211360     |Dangerous Woman  |36153 |\n",
            "|spotify:track:1ADjWm8QNhgNV8yCNNgQ1T|2  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Santa Tell Me                                                  |spotify:album:2Y42QS2bGi5NokHzjticau|204093     |Santa Tell Me    |38574 |\n",
            "|spotify:track:6AviHKu3ydzAePBmzEi62v|17 |MIKA         |spotify:artist:5MmVJVhhYKQ86izuGHzJYA|Popular Song                                                   |spotify:album:6czdbbMtGbAkZ6ud2OMTcg|200213     |Yours Truly      |39098 |\n",
            "|spotify:track:5Pnny78GESkBSLnxFmhRYZ|18 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Better Left Unsaid                                             |spotify:album:5xSvNPstcxHtR4ap2vvN8A|211226     |Yours Truly      |46939 |\n",
            "|spotify:track:6LZkF2fayIxtA1SIMmAGoX|21 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Why Try                                                        |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|211880     |My Everything    |49053 |\n",
            "|spotify:track:5RNKIGhRllNHGjroVDPXat|1  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|A Little More Homework (feat. Graham Phillips) - Single Version|spotify:album:48uMMsVHGfKipIsOuYcvjs|302733     |Brand New You    |54328 |\n",
            "|spotify:track:26gXVZqHG3AThfmkXLlp3P|25 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Break Your Heart Right Back                                    |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|253386     |My Everything    |344842|\n",
            "|spotify:track:7bJwvubZZaoGE1AGEfu8Fi|20 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|One Last Time                                                  |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|197253     |My Everything    |346905|\n",
            "|spotify:track:0zrQwV4XbcwZ2bS9r7F944|38 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Winter Things                                                  |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|158720     |Christmas & Chill|348561|\n",
            "+------------------------------------+---+-------------+-------------------------------------+---------------------------------------------------------------+------------------------------------+-----------+-----------------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Ground truth songs\n",
            "+---+-------------+------------------------------------+-------------------------------------+-------------------------------+------------------------------------+-----------+-----------------+\n",
            "|pos|artist_name  |track_uri                           |artist_uri                           |track_name                     |album_uri                           |duration_ms|album_name       |\n",
            "+---+-------------+------------------------------------+-------------------------------------+-------------------------------+------------------------------------+-----------+-----------------+\n",
            "|33 |Ariana Grande|spotify:track:4QgH3GXHnHuxMJu3RG69Hg|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Intro                          |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|65960      |Christmas & Chill|\n",
            "|36 |Ariana Grande|spotify:track:09VVVW2VxsmXE5Pyo3T0ah|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Not Just On Christmas          |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|123573     |Christmas & Chill|\n",
            "|4  |Ariana Grande|spotify:track:3tRFqxTYOjeTmCiVaGDCsq|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Be Alright                     |spotify:album:4lVR2fg3DAUQpGVJ6DciHW|179293     |Dangerous Woman  |\n",
            "|13 |Ariana Grande|spotify:track:7c86ULTZD9eNdAbJDQLRaC|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Daydreamin'                    |spotify:album:5xSvNPstcxHtR4ap2vvN8A|211293     |Yours Truly      |\n",
            "|30 |Jessie J     |spotify:track:3oEekS4xhmFQ88ieCVTZ7H|spotify:artist:2gsggkzM5R49q6jpPvazou|Bang Bang                      |spotify:album:5AMOKSM1ftb3opIbGT2d4q|199320     |My Everything    |\n",
            "|26 |Ariana Grande|spotify:track:45wBTYlOx3FsuFluuuRRQh|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Love Me Harder                 |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|236133     |My Everything    |\n",
            "|6  |Ariana Grande|spotify:track:5AKlnLpP3WLwZseNbjquND|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Let Me Love You                |spotify:album:3OZgEywV4krCZ814pTJWr7|223853     |Dangerous Woman  |\n",
            "|27 |Ariana Grande|spotify:track:5DsnMBbnSmEmqOQMFTwXRq|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Just A Little Bit Of Your Heart|spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|232586     |My Everything    |\n",
            "|24 |Ariana Grande|spotify:track:6djcQtHtVZHfAKlRNydlIi|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Be My Baby                     |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|217053     |My Everything    |\n",
            "|15 |Ariana Grande|spotify:track:4PqIj0WOfPAq4QAvisjgpd|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Youâll Never Know              |spotify:album:5xSvNPstcxHtR4ap2vvN8A|214280     |Yours Truly      |\n",
            "+---+-------------+------------------------------------+-------------------------------------+-------------------------------+------------------------------------+-----------+-----------------+\n",
            "\n",
            "Clean ground truth songs\n",
            "+------------------------------------+---+-------------+-------------------------------------+-------------------------------+------------------------------------+-----------+-----------------+------+\n",
            "|track_uri                           |pos|artist_name  |artist_uri                           |track_name                     |album_uri                           |duration_ms|album_name       |pos   |\n",
            "+------------------------------------+---+-------------+-------------------------------------+-------------------------------+------------------------------------+-----------+-----------------+------+\n",
            "|spotify:track:5AKlnLpP3WLwZseNbjquND|6  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Let Me Love You                |spotify:album:3OZgEywV4krCZ814pTJWr7|223853     |Dangerous Woman  |8333  |\n",
            "|spotify:track:6djcQtHtVZHfAKlRNydlIi|24 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Be My Baby                     |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|217053     |My Everything    |27633 |\n",
            "|spotify:track:3oEekS4xhmFQ88ieCVTZ7H|30 |Jessie J     |spotify:artist:2gsggkzM5R49q6jpPvazou|Bang Bang                      |spotify:album:5AMOKSM1ftb3opIbGT2d4q|199320     |My Everything    |28065 |\n",
            "|spotify:track:4PqIj0WOfPAq4QAvisjgpd|15 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Youâll Never Know              |spotify:album:5xSvNPstcxHtR4ap2vvN8A|214280     |Yours Truly      |30628 |\n",
            "|spotify:track:5DsnMBbnSmEmqOQMFTwXRq|27 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Just A Little Bit Of Your Heart|spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|232586     |My Everything    |36513 |\n",
            "|spotify:track:7c86ULTZD9eNdAbJDQLRaC|13 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Daydreamin'                    |spotify:album:5xSvNPstcxHtR4ap2vvN8A|211293     |Yours Truly      |55101 |\n",
            "|spotify:track:3tRFqxTYOjeTmCiVaGDCsq|4  |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Be Alright                     |spotify:album:4lVR2fg3DAUQpGVJ6DciHW|179293     |Dangerous Woman  |340926|\n",
            "|spotify:track:45wBTYlOx3FsuFluuuRRQh|26 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Love Me Harder                 |spotify:album:2ZnzBwKw4e2SHpGvOTWnj4|236133     |My Everything    |348129|\n",
            "|spotify:track:09VVVW2VxsmXE5Pyo3T0ah|36 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Not Just On Christmas          |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|123573     |Christmas & Chill|353659|\n",
            "|spotify:track:4QgH3GXHnHuxMJu3RG69Hg|33 |Ariana Grande|spotify:artist:66CXWjxzNUsdJxJ2JdwvnR|Intro                          |spotify:album:2A1KyqHu1DmLtjXpIMNoQq|65960      |Christmas & Chill|373013|\n",
            "+------------------------------------+---+-------------+-------------------------------------+-------------------------------+------------------------------------+-----------+-----------------+------+\n",
            "\n",
            "Prediction df songs (num recommendations: 500)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/13 20:36:18 WARN TaskSetManager: Stage 66 contains a task of very large size (1209 KiB). The maximum recommended task size is 1000 KiB.\n",
            "23/06/13 20:36:19 WARN TaskSetManager: Stage 71 contains a task of very large size (1209 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------+------------------------------------+\n",
            "|pos|confidence  |track_uri                           |\n",
            "+---+------------+------------------------------------+\n",
            "|7  |0.0051272963|spotify:track:48ZUwXrEOhaXLCxvmRYhZv|\n",
            "|19 |0.0037515892|spotify:track:4eQec76xYzMWafPfJEDELl|\n",
            "|22 |4.4842117E-4|spotify:track:0srm8RgwUfBvwdXKvkyDlT|\n",
            "|26 |0.015324642 |spotify:track:6G6u99wgTtoATpqtNmA02F|\n",
            "|29 |0.00205158  |spotify:track:3cwDSDzTiWr5H5xMQhQ6Mx|\n",
            "|34 |0.006128149 |spotify:track:2X9G82BPPSqhE78e5YjekE|\n",
            "|50 |4.309273E-4 |spotify:track:35k31HZI4z9PbBOioaI4dZ|\n",
            "|54 |8.9117774E-4|spotify:track:6iX1f3r7oUJnMbGgQ2gx1j|\n",
            "|57 |0.0011192594|spotify:track:0C1OcBOsuFmFFGkh3FWIGG|\n",
            "|65 |0.0026256384|spotify:track:4bEcoz1OcfMgUbp2ft8ieQ|\n",
            "|77 |2.4645153E-4|spotify:track:4Fvnz1ZJ86IdqDAepWYPAh|\n",
            "|94 |2.4668826E-4|spotify:track:6HEAUuBSW9lCL4clfEOTJ8|\n",
            "|110|2.5660856E-4|spotify:track:65lHwG8JFJs67PnOUhCYPq|\n",
            "|112|5.3823285E-4|spotify:track:1UlyHfT68gEVYwIMIVEX1u|\n",
            "|113|0.0024502387|spotify:track:7EgG37pjxw0Y4GHwjK3CBS|\n",
            "|126|4.112541E-4 |spotify:track:19e8D0Qvau0q8X7JCnYhtF|\n",
            "|130|2.4615446E-4|spotify:track:65wNNrAj5mMOLvLQ2JQhlv|\n",
            "|136|0.001412612 |spotify:track:6lAdV7Btii1FmVOjtvSUxt|\n",
            "|144|5.9100153E-4|spotify:track:54SFFQhDGvPCWhYLtu2bJn|\n",
            "|149|0.0012145697|spotify:track:6xHQ7ogo1QqWT89UGYv1Mf|\n",
            "+---+------------+------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Clean Prediction df songs (num recommendations: 500)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 72:>                                                         (0 + 8) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----------+------------------------------------+\n",
            "|pos   |confidence |track_uri                           |\n",
            "+------+-----------+------------------------------------+\n",
            "|681805|0.1904341  |spotify:track:7AbAfIqrkrWHwSF32eemMV|\n",
            "|23173 |0.1091218  |spotify:track:6DjEa4vziZjdYG0bq2Jxcl|\n",
            "|348871|0.099217884|spotify:track:539wfGOsGcRmT1IBVUfiJV|\n",
            "|338336|0.09756852 |spotify:track:0zMzyHAeMvwq5CRstru1Fp|\n",
            "|30442 |0.0918417  |spotify:track:4z0PnuB07fxtVZZRWsCfxb|\n",
            "|28061 |0.09160706 |spotify:track:2gdEfFFTRU8ylaqteESdk2|\n",
            "|36914 |0.082439065|spotify:track:5JDcQAztvZTIkrWoZihgvC|\n",
            "|367112|0.08186953 |spotify:track:5SECgk6Gf1TVMy2FApJmSO|\n",
            "|376243|0.08050343 |spotify:track:3cqZH3cqvfbV8wVbvHyPbG|\n",
            "|29257 |0.079650715|spotify:track:0SGkqnVQo9KPytSri1H6cF|\n",
            "|355820|0.07798547 |spotify:track:1SN1gifVAKecU85lZggS8k|\n",
            "|366732|0.075138435|spotify:track:66hayvUbTotekKU3H4ta1f|\n",
            "|348084|0.07473964 |spotify:track:6SwRhMLwNqEi6alNPVG00n|\n",
            "|38126 |0.07468223 |spotify:track:4TshxhRTfQqDhQbXmTvY50|\n",
            "|21166 |0.0745314  |spotify:track:2AOQVSqYB0KXryNscZCCOE|\n",
            "|15024 |0.0727116  |spotify:track:4immekPm8Har57BB9DcDSj|\n",
            "|21610 |0.07174019 |spotify:track:1FYf85iyKfzECNWS70QP8i|\n",
            "|29256 |0.07165393 |spotify:track:7dIDDbPxb3t9EmkyGLb1kb|\n",
            "|11706 |0.0711102  |spotify:track:6XLWV4y0TWPzZGY72m6mbA|\n",
            "|366742|0.07104324 |spotify:track:7A0FGrZsLgOUmeNtMTnt4z|\n",
            "+------+-----------+------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 0)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Playlist train songs\")\n",
        "playlist_train_songs.show(truncate=False)\n",
        "print(\"Clean playlist train songs\")\n",
        "clean_playlist_train_songs.show(truncate=False)\n",
        "print(\"Ground truth songs\")\n",
        "ground_truth.show(truncate=False)\n",
        "print(\"Clean ground truth songs\")\n",
        "clean_ground_truth.show(truncate=False)\n",
        "print(f\"Prediction df songs (num recommendations: {n_recommendations})\")\n",
        "prediction_df.show(truncate=False)\n",
        "print(f\"Clean Prediction df songs (num recommendations: {n_recommendations})\")\n",
        "clean_prediction_df.show(truncate=False)\n",
        "prec, gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN8Fk_7Jel4-"
      },
      "outputs": [],
      "source": [
        "def average_results(results):\n",
        "  prec_avg = sum(prec for prec, _ in results) / len(results)\n",
        "  gain_avg = sum(gain for _, gain in results) / len(results)\n",
        "  return prec_avg, gain_avg\n",
        "\n",
        "average_results(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMMG33ctvkyDDZwCI68FV48",
      "collapsed_sections": [
        "XvQ6e0PgCOZg"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "025feace7e5548cebe8d42356ed1963e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc43c4c34974993ad879ab6789719dc",
            "placeholder": "â",
            "style": "IPY_MODEL_3a7a8265d2f84b18b0556518474586cc",
            "value": "Training model:   0%"
          }
        },
        "1ecbd9fd2159484e9fd5f1ad4028717e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210989b9506346409dcb494dc08a7c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ecbd9fd2159484e9fd5f1ad4028717e",
            "placeholder": "â",
            "style": "IPY_MODEL_c54d6d9cf7614f20b5f000c29c5a5b18",
            "value": " 0/8000.0 [00:00&lt;?, ?it/s]"
          }
        },
        "2e5ed18f6ed644898269df6699939a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ff9818b0e24a4ca8fbe94afc45b797",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3af36fc968654a27826c33cbfa6a7b62",
            "value": 0
          }
        },
        "3a7a8265d2f84b18b0556518474586cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3af36fc968654a27826c33cbfa6a7b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cc43c4c34974993ad879ab6789719dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ff9818b0e24a4ca8fbe94afc45b797": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800d36c1bd2f4519ad7215f62fe694b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_025feace7e5548cebe8d42356ed1963e",
              "IPY_MODEL_2e5ed18f6ed644898269df6699939a96",
              "IPY_MODEL_210989b9506346409dcb494dc08a7c28"
            ],
            "layout": "IPY_MODEL_bada53838d434c47bfd5185687d2e133"
          }
        },
        "bada53838d434c47bfd5185687d2e133": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c54d6d9cf7614f20b5f000c29c5a5b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
