{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/dev/NN_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "42c169d8-dd98-4d5b-ce88-cd35f1b7ca4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install petastorm -qq\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "oozTtW3om3Ab"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "import torch\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from petastorm import make_batch_reader\n",
        "from petastorm.pytorch import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XG5sA53iP9z0"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MocM1cmqQpn5"
      },
      "outputs": [],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAu9mQsxTxHj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGgc9DHjT09S"
      },
      "outputs": [],
      "source": [
        "NUM_PLAYLISTS = 100_000\n",
        "SONGS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_INFO_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "ARTISTS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-train-{NUM_PLAYLISTS}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo8gbiN1U1XJ"
      },
      "outputs": [],
      "source": [
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH)\n",
        "artists_embeddings = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH)\n",
        "song_mapping = spark.read.json(SONGS_INFO_DF_PATH)\n",
        "\n",
        "with open(ARTIST_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  ARTIST_VECTOR_LENGTH = int(content) + 1\n",
        "with open(SONGS_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  SONGS_VECTOR_LENGTH = int(content) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY_szFyLTps4",
        "outputId": "764c5d24-4a9f-44d6-e349-6a2bca3bb948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               Mix|        false|24770| 1376352000|        67|        43|            1|(133478,[85,3092,...|       32|   16179675|         34|\n",
            "|         C H I L L|        false|84782| 1508889600|       105|        79|            2|(133478,[257,1952...|       74|   23647221|         59|\n",
            "|    COACHELLA 2013|        false| 3188| 1366675200|       207|        87|            1|(133478,[143,147,...|       16|   49566298|         41|\n",
            "|              Heat|        false| 3879| 1492992000|        27|        20|            2|(133478,[1172,373...|       11|    5580836|         18|\n",
            "|               Mix|        false|24303| 1446595200|        41|        32|            1|(133478,[566,1403...|       17|    9859435|         30|\n",
            "|       Wild Things|        false|24622| 1508803200|       108|        94|            1|(133478,[3,1754,2...|       55|   23296152|         88|\n",
            "|          espaÃ±ol |        false|35696| 1500422400|        30|        26|            1|(133478,[4660,609...|       18|    6338592|         19|\n",
            "|              hype|        false|24515| 1461369600|        34|        31|            2|(133478,[4139,524...|       18|    7390554|         28|\n",
            "|              jamz|        false| 3451| 1490832000|       143|       119|            1|(133478,[277,1440...|        9|   35734173|         87|\n",
            "| party party party|        false|24706| 1509062400|       111|        96|            4|(133478,[259,300,...|       14|   24131476|         70|\n",
            "|     #boostyourrun|        false|24105| 1411603200|        21|        20|            1|(133478,[7355,108...|        2|    4762022|         20|\n",
            "|     #boostyourrun|        false|84725| 1401235200|        21|        20|            1|(133478,[285,4424...|        2|    5053337|         19|\n",
            "|             #mood|        false|84298| 1508198400|        36|        33|            1|(133478,[628,1764...|       25|    8333889|         30|\n",
            "|               '16|        false| 3337| 1478217600|        25|        25|            1|(133478,[1678,220...|       14|    5357192|         24|\n",
            "|               '17|        false|24690| 1508716800|       206|       157|            1|(133478,[927,1228...|       27|   47330512|        120|\n",
            "|           'Merica|        false| 3639| 1435968000|        26|        21|            1|(133478,[2731,299...|        3|    6908434|         17|\n",
            "|               ---|        false|24893| 1484179200|        57|        41|            5|(133478,[4639,500...|       17|   13571222|         33|\n",
            "|          // march|        false|35465| 1490659200|        45|        44|            1|(133478,[2198,483...|       13|   10398911|         41|\n",
            "|        //MEXICO//|        false|35234| 1475798400|        93|        87|            1|(133478,[230,1946...|       17|   20085270|         65|\n",
            "|               0.0|        false|24981| 1369526400|       168|       142|            1|(133478,[174,273,...|        2|   40913340|        107|\n",
            "+------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|                name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+--------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|        Country Hits|        false|14258| 1504828800|       130|        77|            1|(110064,[2,850,10...|       61|   28245005|         44|\n",
            "|           Rock.....|        false| 1938| 1500163200|        57|        39|            1|(110064,[1263,137...|        5|   16766883|         19|\n",
            "|              #Dance|        false|14103| 1463961600|        33|        28|            1|(110064,[106,108,...|       12|    6901015|         24|\n",
            "|       #boostyourrun|        false|14325| 1425686400|        21|        21|            1|(110064,[2426,768...|        2|    5002212|         20|\n",
            "|       #boostyourrun|        false|20976| 1399075200|        21|        21|            1|(110064,[1041,197...|        2|    5400100|         19|\n",
            "|       #boostyourrun|        false|78200| 1402704000|        22|        22|            1|(110064,[3357,721...|        2|    5825915|         21|\n",
            "|            #college|        false|78334| 1490400000|        21|        19|            2|(110064,[193,1470...|        4|    5068086|         18|\n",
            "|              $andy$|        false|78110| 1457395200|       117|        63|            1|(110064,[101,971,...|       14|   28982414|         40|\n",
            "|                 '17|        false|20507| 1505433600|        38|        31|            2|(110064,[485,858,...|       31|    8051141|         30|\n",
            "|            'merica |        false|78916| 1499990400|        67|        45|            1|(110064,[46,850,8...|       21|   14868475|         35|\n",
            "|    **That New New**|        false|14379| 1432771200|        63|        52|            1|(110064,[15,101,1...|       21|   14353567|         47|\n",
            "|            *HANNAH*|        false|20194| 1506211200|        84|        22|            1|(110064,[34405,41...|        5|   11964693|          6|\n",
            "|              *sigh*|        false|14217| 1509321600|       154|       101|            4|(110064,[794,1071...|       25|   38055908|         64|\n",
            "|              *trap*|        false|14097| 1507334400|        17|        14|            1|(110064,[101,764,...|        4|    3360307|         12|\n",
            "|                 ---|        false|20146| 1440720000|       107|        94|            1|(110064,[1,4,101,...|       17|   25079715|         82|\n",
            "|     ...chill_out...|        false| 1952| 1473724800|        35|        29|            1|(110064,[199,215,...|       14|    9058086|         27|\n",
            "|             ...mind|        false| 1812| 1509062400|       104|        89|            3|(110064,[485,719,...|       56|   25186165|         75|\n",
            "|                 .15|        false| 1927| 1430524800|        20|        15|            1|(110064,[3529,426...|        2|    4731874|         14|\n",
            "|//Pretty Little L...|        false| 1633| 1458259200|        63|        55|            3|(110064,[600,1322...|       23|   14801506|         50|\n",
            "|               //aBc|        false|78236| 1463875200|        28|        25|            1|(110064,[101,1071...|       21|    6279845|         21|\n",
            "+--------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None, 110064, 681806)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "songs_embeddings.show(), artists_embeddings.show(), ARTIST_VECTOR_LENGTH, SONGS_VECTOR_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPOD60GQQGse"
      },
      "source": [
        "# Convert PySpark DataFrame into PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "outputs": [],
      "source": [
        "def convert_sparse_to_indices(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "\n",
        "  @F.udf(returnType=ArrayType(IntegerType()))\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    indices_list = item.indices.tolist()\n",
        "    padding_width = max_songs - len(indices_list)\n",
        "    return indices_list + [-1] * padding_width\n",
        "  \n",
        "  max_songs = songs_embeddings.select(F.max(\"num_tracks\")).first()[0]\n",
        "  print(f\"Max number of songs: {max_songs}\")\n",
        "  df = df.withColumn(\"embedding_indices\", transform_array(F.col(\"tracks\"))).drop(\"tracks\")\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIIqDVeN7cU0"
      },
      "outputs": [],
      "source": [
        "def padded_tensors_to_sparse_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    rows.append(sparse_tensor)\n",
        "  return torch.stack(rows)\n",
        "\n",
        "def padded_tensors_to_dense_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    dense = sparse_tensor.to_dense()\n",
        "    rows.append(dense)\n",
        "  unpadded = torch.stack(rows)\n",
        "  return unpadded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNak5bEo4cmV"
      },
      "source": [
        "In the paper they have two matrices,l et $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pKgJx26s73",
        "outputId": "fe16bfd0-5d64-4cf5-d160-1e7dbea99d2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 6803648 B (< 50 MB) of the parquet files is too small. Total size: 12283706 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///big_data/cache_5/20230610164757-appid-local-1686415648676-94cb1620-5784-436e-8f64-3b353685a74d/part-00001-b15e3974-6f10-444e-8223-ae2d11d0ee7b-c000.parquet, ...\n",
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 4487027 B (< 50 MB) of the parquet files is too small. Total size: 8021867 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///big_data/cache_5/20230610164829-appid-local-1686415648676-2661d272-93aa-4186-8a63-74511fbbaf3b/part-00000-1959fb26-1772-4ee7-a333-a54860a04130-c000.parquet, ...\n"
          ]
        }
      ],
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
        "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
        "from torchvision import transforms\n",
        "from petastorm import TransformSpec\n",
        "\n",
        "CACHE = os.path.join(GDRIVE_DIR, \"/big_data/cache_5\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df = convert_sparse_to_indices(songs_embeddings.select(\"tracks\"))\n",
        "songs_converter = make_spark_converter(pytorch_songs_df)\n",
        "\n",
        "pytorch_artists_df = convert_sparse_to_indices(artists_embeddings.select(\"tracks\"))\n",
        "artist_converter = make_spark_converter(pytorch_artists_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Av0W0P66lC"
      },
      "source": [
        "# PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "        nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "        self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "        nn.init.zeros_(self.biases['encoder_b'])\n",
        "        self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "        nn.init.zeros_(self.biases['decoder_b'])\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l2 = (torch.sum(self.weights['encoder_h']) ** 2)/2\n",
        "      decoder_b_l2 = (torch.sum(self.biases['decoder_b']) ** 2)/2\n",
        "      encoder_b_l2 = (torch.sum(self.biases['encoder_b']) ** 2)/2\n",
        "\n",
        "      l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2\n",
        "      return l2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        # TODO: Take sparse matrix representation instead of dense\n",
        "        # self.x = x_positxions\n",
        "        # self.x_ones = x_ones\n",
        "        # self.y_positions = y_positions\n",
        "        # self.y_ones = y_ones\n",
        "\n",
        "        self.x = x.t()\n",
        "        self.y = y.t()\n",
        "\n",
        "        # x_sparse = torch.sparse.FloatTensor(self.x_positions.t(), self.x_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        # self.x = x_sparse.to_dense()\n",
        "        # y_sparse = torch.sparse.FloatTensor(self.y_positions.t(), self.y_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        # self.y = y_sparse.to_dense()\n",
        "\n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p= 1 - self.input_keep_prob)\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       0.55 * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "            \n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l2 = (torch.sum(self.weights['encoder_h']) ** 2)/2\n",
        "      decoder_b_l2 = (torch.sum(self.biases['decoder_b']) ** 2)/2\n",
        "      encoder_b_l2 = (torch.sum(self.biases['encoder_b']) ** 2)/2\n",
        "      decoder_h_l2 = (torch.sum(self.weights['decoder_h']) ** 2)/2\n",
        "\n",
        "      l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2 + decoder_h_l2\n",
        "      return l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aykoeJfhOUBV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGpMdqs7dQQ",
        "outputId": "2001aebc-9e53-4b86-fb7d-2a2ba9463fed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DAE_tied()"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 250,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH - 2, #-2 to compense the first row of zeros since the pos in the df start at 1\n",
        "    'hidden': 256,\n",
        "    'lr': 0.0001,\n",
        "    'reg_lambda': 0.0,\n",
        "    'initval': \"NULL\",\n",
        "    \"keep_prob\": 0.8, \n",
        "    \"input_keep_prob\": 0.8, # This isn't used for now because of the .uniform()\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}\n",
        "pretrain_model = DAE_tied(conf)\n",
        "pretrain_model.init_weight()\n",
        "pretrain_model.train()\n",
        "# optimizer = optim.SGD(dae_model.d_params, lr=conf['lr'], momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNPmA2YijSO1"
      },
      "outputs": [],
      "source": [
        "pretrain_optimizer = optim.Adam(pretrain_model.d_params, lr=conf['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jAqv6QBEiK9",
        "outputId": "c0716d37-5a7f-4966-c860-65e8041a4f74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_model.weights['encoder_h'].device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuqQMu_E0yaN"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIg2DIDNjEKm"
      },
      "outputs": [],
      "source": [
        "min_loss = 600\n",
        "best_params = []\n",
        "losses = []\n",
        "BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"best_params.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvPuc5hqirK"
      },
      "source": [
        "Pretrain with `DAE_tied`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "800d36c1bd2f4519ad7215f62fe694b2",
            "025feace7e5548cebe8d42356ed1963e",
            "2e5ed18f6ed644898269df6699939a96",
            "210989b9506346409dcb494dc08a7c28",
            "bada53838d434c47bfd5185687d2e133",
            "4cc43c4c34974993ad879ab6789719dc",
            "3a7a8265d2f84b18b0556518474586cc",
            "56ff9818b0e24a4ca8fbe94afc45b797",
            "3af36fc968654a27826c33cbfa6a7b62",
            "1ecbd9fd2159484e9fd5f1ad4028717e",
            "c54d6d9cf7614f20b5f000c29c5a5b18"
          ]
        },
        "id": "Z_BjjpshDD2H",
        "outputId": "e1ab4815-5fa7-433f-b258-bb6317b2d657"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "800d36c1bd2f4519ad7215f62fe694b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training model:   0%|          | 0/8000.0 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "NUM_EPOCHS = 20\n",
        "with songs_converter.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as songs_dataloader:\n",
        "  with artist_converter.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as artists_dataloader:\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    # for epoch in tqdm(range(num_epochs), desc=\"Training...\"):\n",
        "    zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "    for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "      # Pick random input_keep_prob between 0.5 and 0.8\n",
        "      pretrain_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "      padded_song_tensor = song[\"embedding_indices\"]\n",
        "      padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)[:, 1:]\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)[:, 1:]\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      rand_int = np.random.randint(2)\n",
        "      if rand_int == 0:\n",
        "        #Zero-out the artists\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        pretrain_model(x, y)\n",
        "        loss = pretrain_model.cost\n",
        "        pretrain_model.cost.backward()\n",
        "        pretrain_optimizer.step()\n",
        "      if rand_int == 1:\n",
        "        #Zero-out the tracks\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        pretrain_model(x, y)\n",
        "        loss = pretrain_model.cost\n",
        "        pretrain_model.cost.backward()\n",
        "        pretrain_optimizer.step()\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "     \n",
        "      if loss < min_loss:\n",
        "        min_loss = loss\n",
        "        del best_params\n",
        "        best_params = [param.cpu().detach().numpy() for param in pretrain_model.d_params]\n",
        "        with open(BEST_PARAMS_PATH, \"wb\") as f:\n",
        "          pickle.dump(best_params, f)\n",
        "        print(f\"Best loss achieved: {min_loss}, parameters saved!\")\n",
        "\n",
        "      if batch_idx % 10 == 0:\n",
        "        print(f\"Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F85w_ZpmPD9t"
      },
      "outputs": [],
      "source": [
        "# params = [param.cpu().detach().numpy() for param in pretrain_model.d_params]\n",
        "# with open(BEST_PARAMS_PATH, 'wb') as f:\n",
        "#   pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOoS7eGqlrH"
      },
      "source": [
        "Train with `DAE` loading the pretrained `DAE_tied` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFAj48uuqlbd"
      },
      "outputs": [],
      "source": [
        "conf = {\n",
        "    'batch': 250,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH - 2, #-2 to compense the first row of zeros since the pos in the df start at 1\n",
        "    'hidden': 256,\n",
        "    'lr': 0.00005,\n",
        "    'reg_lambda': 0.0,\n",
        "    'initval': BEST_PARAMS_PATH,\n",
        "    \"keep_prob\": 0.8, \n",
        "    \"input_keep_prob\": 0.8, # This isn't used for now because of the .uniform()\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}\n",
        "dae_model = DAE(conf)\n",
        "dae_model.init_weight()\n",
        "optimizer = optim.Adam(dae_model.d_params, lr=conf['lr'])\n",
        "\n",
        "min_loss = 600\n",
        "losses = []\n",
        "best_params = []\n",
        "FINE_TUNED_BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"final_best_params.pickle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyw3SnVYyRP1"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "NUM_EPOCHS = 20\n",
        "with songs_converter.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as songs_dataloader:\n",
        "  with artist_converter.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as artists_dataloader:\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    # for epoch in tqdm(range(num_epochs), desc=\"Training...\"):\n",
        "    zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "    for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "      # Pick random input_keep_prob between 0.5 and 0.8\n",
        "      dae_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "      padded_song_tensor = song[\"embedding_indices\"]\n",
        "      padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)[:, 1:]\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)[:, 1:]\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      rand_int = np.random.randint(2)\n",
        "      if rand_int == 0:\n",
        "        #Zero-out the artists\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "      if rand_int == 1:\n",
        "        #Zero-out the tracks\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1).t()\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "     \n",
        "      if loss < min_loss:\n",
        "        min_loss = loss\n",
        "        del best_params\n",
        "        best_params = [param.cpu().detach().numpy() for param in dae_model.d_params]\n",
        "        with open(FINE_TUNED_BEST_PARAMS_PATH, \"wb\") as f:\n",
        "          pickle.dump(best_params, f)\n",
        "        print(f\"Best loss achieved: {min_loss}, parameters saved!\")\n",
        "\n",
        "      if batch_idx % 10 == 0:\n",
        "        print(f\"Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQL3P9JByR7Q"
      },
      "source": [
        "Let's see how the loss decreases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF9gP0FMLdw2"
      },
      "outputs": [],
      "source": [
        "SAVE_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"model_new.pickle\")\n",
        "params = [param.cpu().detach().numpy() for param in dae_model.d_params]\n",
        "with open(SAVE_MODEL_PATH, 'wb') as f:\n",
        "  pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4JxgOCvx2TE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(len(losses))\n",
        "plt.plot(x, [loss.item() for loss in losses])\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Progression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqyho9fPl0ow"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9RFO76gK8Qb"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lgIPJo2tbUL"
      },
      "outputs": [],
      "source": [
        "SONGS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "songs_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "songs_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "artists_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "artists_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "TEST_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "TEST_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "test_train_df = spark.read.schema(playlist_schema).json(TEST_TRAIN_DF_PATH)\n",
        "test_test_df = spark.read.schema(playlist_schema).json(TEST_TEST_DF_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKbyd0kRl3SH"
      },
      "outputs": [],
      "source": [
        "def construct_prediction_df(prediction: torch.Tensor, mapping: DataFrame, top_n: int = 50) -> DataFrame:\n",
        "  pred_np = prediction.detach().numpy()\n",
        "  indexes = np.arange(pred_np.shape[0]) + 1 # To compensate the index start at 1\n",
        "  schema = StructType([\n",
        "      StructField(\"pos\", IntegerType()),\n",
        "      StructField(\"confidence\", FloatType())\n",
        "  ])\n",
        "  prediction_df = spark.createDataFrame([(pos, conf) for pos, conf in zip(indexes.tolist(), pred_np.tolist())],schema)\n",
        "  prediction_info = prediction_df.join(mapping, \"pos\")\n",
        "  return prediction_info\n",
        "\n",
        "# prediction_df = construct_prediction_df(prediction, songs_df_test)\n",
        "# prediction_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ9LquCdSbJJ"
      },
      "outputs": [],
      "source": [
        "def remove_existing_tracks(playlist_tracks: DataFrame, recommendations_df: DataFrame) -> DataFrame:\n",
        "  playlist_tracks = playlist_tracks.select(\"track_uri\").cache()\n",
        "  playlist_tracks_compatible = playlist_tracks.join(F.broadcast(recommendations_df), on=\"track_uri\")\n",
        "  playlist_tracks.unpersist()\n",
        "  return recommendations_df.exceptAll(F.broadcast(playlist_tracks_compatible))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx-tghy0bAkU"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb793kgebBwJ"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(recommendations, ground_truth, num_of_recommendations) -> float:\n",
        "    \"\"\"\n",
        "    Calculates precision at k for the recommendations.\n",
        "    \"\"\"\n",
        "    recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").cache()\n",
        "    reccomended_relevant_tracks_count = recommended_relevant_tracks.count() #this can be top_n_results.join in order to be more performant\n",
        "    recommended_relevant_tracks.unpersist()\n",
        "    precision = reccomended_relevant_tracks_count / float(num_of_recommendations)\n",
        "\n",
        "    return precision\n",
        "\n",
        "\n",
        "import math\n",
        "def normalized_discounted_cumulative_gain(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "  recommendations_list = recommendations.collect()\n",
        "  cumulative_gain = 0\n",
        "\n",
        "  intersection = recommendations.join(ground_truth, \"track_uri\").count()\n",
        "  if intersection == 0: return 0\n",
        "\n",
        "  ideal_cumulative_gain = 1 + np.array([(1 / math.log(i, 2)) for i in range(2, 2+intersection)]).sum() #TODO: replace this with sum([])\n",
        "  for index, row in enumerate(recommendations_list):\n",
        "    i = index + 1\n",
        "    is_rel = ground_truth.filter(F.col(\"track_uri\").isin(row.track_uri)).count() > 0\n",
        "    rel = 1 if is_rel else 0\n",
        "    if i == 1:\n",
        "      cumulative_gain += rel\n",
        "    else:\n",
        "      cumulative_gain += (rel / math.log(i, 2))\n",
        "  return cumulative_gain / ideal_cumulative_gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUYEe5iHbTHs"
      },
      "source": [
        "Creating the dataloaders for the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYXLsGIrbH2e"
      },
      "outputs": [],
      "source": [
        "CACHE = os.path.join(GDRIVE_DIR, \"/big_data/cache_5\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df_test = convert_sparse_to_indices(songs_embeddings_test_train.select(\"tracks\"))\n",
        "songs_converter_test = make_spark_converter(pytorch_songs_df_test)\n",
        "\n",
        "pytorch_artists_df_test = convert_sparse_to_indices(artists_embeddings_test_train.select(\"tracks\"))\n",
        "artist_converter_test = make_spark_converter(pytorch_artists_df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h7NRo3Ggt3h"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 150,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH - 2, #-2 to compense the first row of zeros since the pos in the df starts at 1,\n",
        "    'hidden': 256,\n",
        "    'lr': 0.001, #original 0.001\n",
        "    'reg_lambda': 0.0,\n",
        "    'initval': BEST_PARAMS_PATH, #TODO: change it in fine_tuned path\n",
        "    \"keep_prob\": 1,\n",
        "    \"input_keep_prob\": 1,\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}\n",
        "dae_model_test = DAE(conf)\n",
        "dae_model_test.init_weight()\n",
        "dae_model_test.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn2zrRCSdKQO"
      },
      "outputs": [],
      "source": [
        "ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "with songs_converter_test.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs =1) as songs_dataloader:\n",
        "  with artist_converter_test.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs=1) as artists_dataloader:\n",
        "    zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "    for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Evaluation...\", total= (NUM_PLAYLISTS / conf[\"batch\"])):\n",
        "      padded_song_tensor = song[\"embedding_indices\"]\n",
        "      padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "      \n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)[:, 1:]\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)[:, 1:]\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "      y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "      \n",
        "      dae_model_test(x,y)\n",
        "      \n",
        "      result = dae_model_test.y_pred\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRZ9w5nr5TaK"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg8V8VSSu3XI"
      },
      "outputs": [],
      "source": [
        "# torch.concat((song_dense[:, 1:], artist_dense[:, 1:]), dim=1).t().shape\n",
        "# (song_dense[:, 0] == 0.).all(), (artist_dense[:, 0] == 0.).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLpygSgaMVr6"
      },
      "outputs": [],
      "source": [
        "# result = result.to(\"cpu\")\n",
        "# prediction_df = construct_prediction_df(result[10][:SONGS_VECTOR_LENGTH_TEST], songs_df_test, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uYwzqLFMjWE"
      },
      "outputs": [],
      "source": [
        "# prediction_df.orderBy(\"pos\").show(truncate=False)\n",
        "# songs_df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0wojIXkuFnN"
      },
      "outputs": [],
      "source": [
        "# def evaluate_batch(batch_result: torch.Tensor, batch_n: int) -> Tuple[float, float]:\n",
        "#   \"\"\"\n",
        "#   Returns the precision and NDCG for a given batch.\n",
        "#   \"\"\"\n",
        "  \n",
        "#   return 0,0\n",
        "\n",
        "# def perform_evaluation(songs_dataloader, artists_dataloader, test_set):\n",
        "#   \"\"\"\n",
        "#   Returns the precision and NDCG, averaged from all the samples in the test set\n",
        "#   \"\"\"\n",
        "#   with songs_converter_test.make_torch_dataloader(num_epochs =1) as songs_dataloader:\n",
        "#     with artist_converter_test.make_torch_dataloader(num_epochs=1) as artists_dataloader:\n",
        "#       zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "#       for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Evaluation...\", total= (NUM_PLAYLISTS / 32) * NUM_EPOCHS):\n",
        "#         padded_song_tensor = song[\"embedding_indices\"]\n",
        "#         padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "        \n",
        "#         song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "#         artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "#         x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "#         y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "#         dae_model(x,y)\n",
        "#         batch_result = dae_model.y_pred\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4LWsnwWeVAV"
      },
      "outputs": [],
      "source": [
        "#Testing the first playlist\n",
        "#TODO: maybe this is not the right way of making a prediction\n",
        "results = []\n",
        "for i in tqdm(range(32)):\n",
        "  ground_truth = test_test_df.filter(F.col(\"pid\") == 1000 + i).select(F.explode(\"tracks\")).select(\"col.*\").cache()\n",
        "  playlist_train_songs = test_train_df.filter(F.col(\"pid\") == 1000 + i).select(F.explode(\"tracks\")).select(\"col.*\").cache()\n",
        "  n_recommendations = ground_truth.count() or 1\n",
        "  result = result.cpu()\n",
        "\n",
        "  prediction_df = construct_prediction_df(result[i][:SONGS_VECTOR_LENGTH], song_mapping, n_recommendations).cache()\n",
        "  clean_prediction_df = remove_existing_tracks(playlist_train_songs, prediction_df)\n",
        "\n",
        "  clean_prediction_df = prediction_df.orderBy(F.col(\"confidence\").desc()).limit(n_recommendations).cache()\n",
        "\n",
        "  prec = precision_at_k(clean_prediction_df, ground_truth, n_recommendations)\n",
        "  gain = normalized_discounted_cumulative_gain(clean_prediction_df, ground_truth, n_recommendations)\n",
        "  print(i, prec, gain)\n",
        "  results.append((prec, gain))\n",
        "  \n",
        "  ground_truth.unpersist()\n",
        "  playlist_train_songs.unpersist()\n",
        "  prediction_df.unpersist()\n",
        "  clean_prediction_df.unpersist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XUxFpPGVLf-"
      },
      "outputs": [],
      "source": [
        "playlist_train_songs.show(truncate=False)\n",
        "ground_truth.show(truncate=False)\n",
        "prediction_df.show(truncate=False)\n",
        "clean_prediction_df.show(truncate=False)\n",
        "prec, gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQiXmG7R5plf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN8Fk_7Jel4-"
      },
      "outputs": [],
      "source": [
        "def average_results(results):\n",
        "  prec_avg = sum(prec for prec, _ in results) / len(results)\n",
        "  gain_avg = sum(gain for _, gain in results) / len(results)\n",
        "  return prec_avg, gain_avg\n",
        "\n",
        "average_results(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XvQ6e0PgCOZg"
      ],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMMG33ctvkyDDZwCI68FV48",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "025feace7e5548cebe8d42356ed1963e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc43c4c34974993ad879ab6789719dc",
            "placeholder": "â",
            "style": "IPY_MODEL_3a7a8265d2f84b18b0556518474586cc",
            "value": "Training model:   0%"
          }
        },
        "1ecbd9fd2159484e9fd5f1ad4028717e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210989b9506346409dcb494dc08a7c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ecbd9fd2159484e9fd5f1ad4028717e",
            "placeholder": "â",
            "style": "IPY_MODEL_c54d6d9cf7614f20b5f000c29c5a5b18",
            "value": " 0/8000.0 [00:00&lt;?, ?it/s]"
          }
        },
        "2e5ed18f6ed644898269df6699939a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ff9818b0e24a4ca8fbe94afc45b797",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3af36fc968654a27826c33cbfa6a7b62",
            "value": 0
          }
        },
        "3a7a8265d2f84b18b0556518474586cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3af36fc968654a27826c33cbfa6a7b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cc43c4c34974993ad879ab6789719dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ff9818b0e24a4ca8fbe94afc45b797": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800d36c1bd2f4519ad7215f62fe694b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_025feace7e5548cebe8d42356ed1963e",
              "IPY_MODEL_2e5ed18f6ed644898269df6699939a96",
              "IPY_MODEL_210989b9506346409dcb494dc08a7c28"
            ],
            "layout": "IPY_MODEL_bada53838d434c47bfd5185687d2e133"
          }
        },
        "bada53838d434c47bfd5185687d2e133": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c54d6d9cf7614f20b5f000c29c5a5b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}