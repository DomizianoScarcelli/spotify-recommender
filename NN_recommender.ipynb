{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XVvKeJoU4hq4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def is_running_on_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "LOCAL = not is_running_on_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "5f9d7ce6-a57f-4d0b-a76b-b49bdab152c1"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    !pip install petastorm -qq\n",
        "    !pip install pyspark -qq\n",
        "    !pip install -U -q PyDrive -qq\n",
        "    !apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "oozTtW3om3Ab"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "if not LOCAL:\n",
        "    from google.colab import drive\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "from functools import reduce\n",
        "import pickle\n",
        "import torch\n",
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XG5sA53iP9z0"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "if not LOCAL:\n",
        "    JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    GDRIVE_DIR = \"/content/drive\"\n",
        "    GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "    GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "    DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "    AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "    LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "    SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "    LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "    MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "    SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "else:\n",
        "    GDRIVE_DATA_DIR = os.path.abspath(\"./data\")\n",
        "    GDRIVE_HOME_DIR = GDRIVE_DATA_DIR\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "    JAVA_HOME = \"/opt/homebrew/opt/openjdk\"\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G20Yir8g4hq6",
        "outputId": "9222a563-ef80-4691-f776-d34a6928c2b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "config = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SyrTHVZR4hq8"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Load DataFrame"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the `DataFrame` schemas and load the primary `DataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JAu9mQsxTxHj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fGgc9DHjT09S"
      },
      "outputs": [],
      "source": [
        "NUM_PLAYLISTS = 100_000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I define the paths for all the different dataframes. This block is the same that can be found inside of the `data_preparation.ipynb` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BIalKyiH4hq9"
      },
      "outputs": [],
      "source": [
        "# The DF used for train (80% of the original) (playlist are different)\n",
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "# The DF used for testing (20% of the original) (playlist are different)\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "# The DF used for train in the NN model (can be filtered or not)\n",
        "NN_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_train_df-{NUM_PLAYLISTS}.json\")\n",
        "# The DF used for testing in the NN model (can be filtered or not)\n",
        "NN_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-{NUM_PLAYLISTS}.json\")\n",
        "# The partition in train test of the NN test set. (Same playlists, different songs)\n",
        "NN_TEST_DF_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_TEST_DF_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_EVAL_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-{NUM_PLAYLISTS}.json\")\n",
        "NN_EVAL_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_EVAL_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-test-{NUM_PLAYLISTS}.json\")\n",
        "# New one:\n",
        "ARTISTS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_EVAL = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_EVAL_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_EVAL_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "# The length of the artist vector length (Artist vectors are only used in the NN model)\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "# This may be filtered or not\n",
        "FILTERED_SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "SONGS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_TEST = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_SONGS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_SONGS_EMBEDDINGS_EVAL = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-{NUM_PLAYLISTS}.json\") #TODO: The logic to produce this still has to be coded.\n",
        "NN_SONGS_EMBEDDINGS_EVAL_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_EVAL_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "FILTERED_SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"nn_songs_info_df-{NUM_PLAYLISTS}.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataframes and vector lengths from file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lo8gbiN1U1XJ"
      },
      "outputs": [],
      "source": [
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_TRAIN)\n",
        "artists_embeddings = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_TRAIN)\n",
        "song_mapping = spark.read.json(FILTERED_SONGS_INFO_DF)\n",
        "\n",
        "songs_embeddings_eval_train = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_EVAL_TRAIN)\n",
        "songs_embeddings_eval_test = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_EVAL_TEST)\n",
        "\n",
        "artists_embeddings_eval_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_EVAL_TRAIN)\n",
        "artists_embeddings_eval_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_EVAL_TEST)\n",
        "\n",
        "with open(ARTIST_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  ARTIST_VECTOR_LENGTH = int(content)\n",
        "with open(SONGS_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  SONGS_VECTOR_LENGTH = int(content)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Showing the dataframes and printing the vector lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY_szFyLTps4",
        "outputId": "53bb6f6a-bea4-4f1f-d043-ba77346cfd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              Alone|        false|20080| 1496880000|        50|        48|            1|(681805,[4874,542...|       19|   11128916|         42|\n",
            "|       Country Hits|        false|14258| 1504828800|       130|        77|            1|(681805,[3,1666,1...|       61|   28245005|         44|\n",
            "|              Peter|        false|14812| 1446595200|        65|        56|            1|(681805,[213,540,...|       15|   14212644|         47|\n",
            "|          Rock.....|        false| 1938| 1500163200|        57|        39|            1|(681805,[2908,350...|        5|   16766883|         19|\n",
            "|               punk|        false| 1180| 1504483200|        67|        45|            1|(681805,[61,3298,...|       21|   15393647|         26|\n",
            "|#CODGhosts Playlist|        false|14362| 1382918400|        50|        50|            1|(681805,[1565,301...|        2|   13601217|         50|\n",
            "|             #Dance|        false|14103| 1463961600|        33|        28|            1|(681805,[389,7336...|       12|    6901015|         24|\n",
            "|      #boostyourrun|        false|14325| 1425686400|        21|        21|            1|(681805,[497,1341...|        2|    5002212|         20|\n",
            "|      #boostyourrun|        false|20976| 1399075200|        21|        21|            1|(681805,[4481,918...|        2|    5400100|         19|\n",
            "|      #boostyourrun|        false|78200| 1402704000|        22|        22|            1|(681805,[39754,10...|        2|    5825915|         21|\n",
            "|           #college|        false|78334| 1490400000|        21|        19|            2|(681805,[10948,16...|        4|    5068086|         18|\n",
            "|               #tbt|        false| 1168| 1436832000|        29|        22|            1|(681805,[3585,463...|        2|    6829177|         15|\n",
            "|             $andy$|        false|78110| 1457395200|       117|        63|            1|(681805,[1678,325...|       14|   28982414|         40|\n",
            "|                '17|        false|20507| 1505433600|        38|        31|            2|(681805,[1122,489...|       31|    8051141|         30|\n",
            "|                '17|        false|78571| 1509235200|        34|        29|            1|(681805,[11315,15...|       16|    7500239|         26|\n",
            "|            'Merica|        false|78912| 1499126400|       241|       237|            1|(681805,[13,608,7...|        6|   57441819|        217|\n",
            "|           'merica |        false|78916| 1499990400|        67|        45|            1|(681805,[1666,206...|       21|   14868475|         35|\n",
            "|   **That New New**|        false|14379| 1432771200|        63|        52|            1|(681805,[270,2071...|       21|   14353567|         47|\n",
            "|           *HANNAH*|        false|20194| 1506211200|        84|        22|            1|(681805,[24791,69...|        5|   11964693|          6|\n",
            "|             *sigh*|        false|14217| 1509321600|       154|       101|            4|(681805,[114,632,...|       25|   38055908|         64|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              Alone|        false|20080| 1496880000|        50|        48|            1|(110063,[392,588,...|       19|   11128916|         42|\n",
            "|       Country Hits|        false|14258| 1504828800|       130|        77|            1|(110063,[1,849,10...|       61|   28245005|         44|\n",
            "|              Peter|        false|14812| 1446595200|        65|        56|            1|(110063,[3,193,19...|       15|   14212644|         47|\n",
            "|          Rock.....|        false| 1938| 1500163200|        57|        39|            1|(110063,[1262,136...|        5|   16766883|         19|\n",
            "|               punk|        false| 1180| 1504483200|        67|        45|            1|(110063,[198,312,...|       21|   15393647|         26|\n",
            "|#CODGhosts Playlist|        false|14362| 1382918400|        50|        50|            1|(110063,[1070,140...|        2|   13601217|         50|\n",
            "|             #Dance|        false|14103| 1463961600|        33|        28|            1|(110063,[105,107,...|       12|    6901015|         24|\n",
            "|      #boostyourrun|        false|14325| 1425686400|        21|        21|            1|(110063,[2425,768...|        2|    5002212|         20|\n",
            "|      #boostyourrun|        false|20976| 1399075200|        21|        21|            1|(110063,[1040,197...|        2|    5400100|         19|\n",
            "|      #boostyourrun|        false|78200| 1402704000|        22|        22|            1|(110063,[3356,721...|        2|    5825915|         21|\n",
            "|           #college|        false|78334| 1490400000|        21|        19|            2|(110063,[192,1469...|        4|    5068086|         18|\n",
            "|               #tbt|        false| 1168| 1436832000|        29|        22|            1|(110063,[1967,213...|        2|    6829177|         15|\n",
            "|             $andy$|        false|78110| 1457395200|       117|        63|            1|(110063,[100,970,...|       14|   28982414|         40|\n",
            "|                '17|        false|20507| 1505433600|        38|        31|            2|(110063,[484,857,...|       31|    8051141|         30|\n",
            "|                '17|        false|78571| 1509235200|        34|        29|            1|(110063,[101,2233...|       16|    7500239|         26|\n",
            "|            'Merica|        false|78912| 1499126400|       241|       237|            1|(110063,[10,193,4...|        6|   57441819|        217|\n",
            "|           'merica |        false|78916| 1499990400|        67|        45|            1|(110063,[45,849,8...|       21|   14868475|         35|\n",
            "|   **That New New**|        false|14379| 1432771200|        63|        52|            1|(110063,[14,100,1...|       21|   14353567|         47|\n",
            "|           *HANNAH*|        false|20194| 1506211200|        84|        22|            1|(110063,[34404,41...|        5|   11964693|          6|\n",
            "|             *sigh*|        false|14217| 1509321600|       154|       101|            4|(110063,[793,1070...|       25|   38055908|         64|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None, 110063, 681805)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "songs_embeddings.show(), artists_embeddings.show(), ARTIST_VECTOR_LENGTH, SONGS_VECTOR_LENGTH"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lPOD60GQQGse"
      },
      "source": [
        "# Convert PySpark DataFrame into PyTorch DataLoader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to maintain the data distributed, I used Petastorm, that allows to create a Pytorch (or Tensorflow) DataLoader from a pyspark DataFrame.\n",
        "\n",
        "Before creating the DataLoader, I will define some utility function to convert the python a pyspark's `SparseVector` to a python list that contains only the indices. The indices are also padded with the value `-1` because in order to create a DataLoader i need that all the rows in the matrix that makes the mini-batch have the same shape. The padding will be removed once the single mini-batch is loaded into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "outputs": [],
      "source": [
        "def convert_sparse_to_indices(df: DataFrame, column_name: str) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "\n",
        "  @F.udf(returnType=ArrayType(IntegerType()))\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    indices_list = item.indices.tolist()\n",
        "    padding_width = max_songs - len(indices_list)\n",
        "    return indices_list + [-1] * padding_width\n",
        "\n",
        "  max_songs = songs_embeddings.select(F.max(\"num_tracks\")).first()[0]\n",
        "  print(f\"Max number of songs: {max_songs}\")\n",
        "  df = df.withColumn(f\"{column_name}_indices\", transform_array(F.col(column_name))).drop(column_name)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eIIqDVeN7cU0"
      },
      "outputs": [],
      "source": [
        "def padded_tensors_to_dense_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Given a tensor is obtained with the method `convert_sparse_to_indices`, that is so made up with indices, and then\n",
        "  a list of -1 values (padding), it removes the padding and returns the dense tensor that it represents.\n",
        "  \"\"\"\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    dense = sparse_tensor.to_dense()\n",
        "    rows.append(dense)\n",
        "  unpadded = torch.stack(rows)\n",
        "  return unpadded"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UNak5bEo4cmV"
      },
      "source": [
        "In the paper they have two matrices, let $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the songs and artists DataFrame, let's concatenate them into a single DataFrame, in order to associate the right playlist representation to the right artist representation, since the distributed behaviour of Petastorm doesn't preserve the order of the dataframe, so I cannot load them one by one. Done that, I will transform the DataFrame into a Pytorch DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pKgJx26s73",
        "outputId": "4e41cc69-c37d-4131-d0d0-ec0001f8d868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 12113751 B (< 50 MB) of the parquet files is too small. Total size: 50282358 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///content/drive/MyDrive/cache/20230630075131-appid-local-1688111453598-d9077280-d2b3-4f53-89cf-8d299397a9bd/part-00000-375a2551-c5ab-44f4-adbb-fd5d718028cf-c000.parquet, ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "98465 98465 98465\n"
          ]
        }
      ],
      "source": [
        "CACHE = os.path.join(GDRIVE_HOME_DIR, \"cache\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df = convert_sparse_to_indices(songs_embeddings.select(\"tracks\", \"pid\"), column_name=\"tracks\")\n",
        "pytorch_artists_df = convert_sparse_to_indices(artists_embeddings.withColumnRenamed(\"tracks\", \"artists\").select(\"pid\", \"artists\"), column_name=\"artists\")\n",
        "\n",
        "songs_artists_df = pytorch_songs_df.join(pytorch_artists_df, on=\"pid\")\n",
        "pytorch_merged_dataloader = make_spark_converter(songs_artists_df)\n",
        "\n",
        "\n",
        "print(pytorch_songs_df.count(), pytorch_artists_df.count(), songs_artists_df.count()) #Everything good here, this is nice!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Av0W0P66lC"
      },
      "source": [
        "# PyTorch Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now define the Pytorch Model. The model is a Denoising Autoencoder that tries to reconstruct a playlist vector, that is represented by a the concatenation of the songs that it contains and the artists that partecipate. \n",
        "\n",
        "We here define two models:\n",
        "- `DAE_tied`: the DAE with tied weights. The weights between the encoder and decoder are shared. This will be used during pretraining in order to improve the further training speed.\n",
        "- `DAE`: same structure as the DAE with tied weights, but the weights of the encoder and decoder are not shared anymore. The best weights of the tied DAE are used as initialization for the untied DAE, where is further trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q8oo_44ARWgb"
      },
      "outputs": [],
      "source": [
        "TRAIN = False # If true, train the model, otherwise skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "outputs": [],
      "source": [
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "        if LOCAL:\n",
        "            self.device = torch.device(\"mps\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "        self.z = None\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "        encoder_h_l1 = torch.sum(torch.abs(self.weights['encoder_h']))\n",
        "        decoder_b_l1 = torch.sum(torch.abs(self.biases['decoder_b']))\n",
        "        encoder_b_l1 = torch.sum(torch.abs(self.biases['encoder_b']))\n",
        "\n",
        "        l1 = encoder_h_l1 + decoder_b_l1 + encoder_b_l1\n",
        "        return l1\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p= 1 - self.input_keep_prob)\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "\n",
        "        self.z = encoder_op\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        ALPHA = 0.5 #Weighting factor for the loss weighting scheme\n",
        "\n",
        "        #Binary Cross Entropy Loss\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       ALPHA * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "\n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l1 = torch.sum(torch.abs(self.weights['encoder_h']))\n",
        "      decoder_b_l1 = torch.sum(torch.abs(self.biases['decoder_b']))\n",
        "      encoder_b_l1 = torch.sum(torch.abs(self.biases['encoder_b']))\n",
        "      decoder_h_l1 = torch.sum(torch.abs(self.weights['decoder_h']))\n",
        "\n",
        "\n",
        "      l1 = encoder_h_l1 + decoder_b_l1 + encoder_b_l1 + decoder_h_l1\n",
        "\n",
        "      return l1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create the two DataLoaders for the validation set.\n",
        "Note that the Validation set is split in train and test at track level, meaning the playlists are the same but the 75% of the songs in each playlist are in the training set, and the other 25% are in the test set. The songs in the test set will used as ground truth in the evaluation part. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNcWBM0U4hrB",
        "outputId": "b76cf860-0040-4988-b52f-5ce4d7e556bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n",
            "Max number of songs: 250\n",
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        }
      ],
      "source": [
        "pytorch_songs_eval_train_df = convert_sparse_to_indices(songs_embeddings_eval_train.withColumnRenamed(\"tracks\", \"train_tracks\").select(\"train_tracks\", \"pid\"), column_name=\"train_tracks\")\n",
        "pytorch_artists_eval_train_df = convert_sparse_to_indices(artists_embeddings_eval_train.withColumnRenamed(\"tracks\", \"train_artists\").select(\"pid\", \"train_artists\"), column_name=\"train_artists\")\n",
        "songs_artists_eval_train_df = pytorch_songs_eval_train_df.join(pytorch_artists_eval_train_df, on=\"pid\")\n",
        "\n",
        "pytorch_songs_eval_test_df = convert_sparse_to_indices(songs_embeddings_eval_test.withColumnRenamed(\"tracks\", \"test_tracks\").select(\"test_tracks\", \"pid\"), column_name=\"test_tracks\")\n",
        "pytorch_artists_eval_test_df = convert_sparse_to_indices(artists_embeddings_eval_test.withColumnRenamed(\"tracks\", \"test_artists\").select(\"pid\", \"test_artists\"), column_name=\"test_artists\")\n",
        "songs_artists_eval_test_df = pytorch_songs_eval_test_df.join(pytorch_artists_eval_test_df, on=\"pid\")\n",
        "\n",
        "eval_merged_df = songs_artists_eval_train_df.join(songs_artists_eval_test_df, on=\"pid\")\n",
        "\n",
        "# counter = F.udf(lambda x: len([item for item in x if item != -1]), returnType=IntegerType())\n",
        "# eval_merged_df = eval_merged_df\\\n",
        "#     .withColumn(\"train_tracks_count\", counter(F.col(\"train_tracks_indices\")))\\\n",
        "#     .withColumn(\"test_tracks_count\", counter(F.col(\"test_tracks_indices\")))\n",
        "# eval_merged_df = eval_merged_df.filter(\"train_tracks_count > 100\")\n",
        "eval_merged_dataloader = make_spark_converter(eval_merged_df)\n",
        "\n",
        "eval_merged_num = eval_merged_df.count()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the device to use for the training. The training will mostly occur on Colab with CUDA GPUs, in order to be fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Vgt0T-v0K-JX"
      },
      "outputs": [],
      "source": [
        "if LOCAL:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I define the functions that will be used to compute the evaluation metrics, that are *R-Precision* and *Normalized Discounted Cumulative Gain*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "kQPVslug4hrB"
      },
      "outputs": [],
      "source": [
        "def r_prec(input: torch.Tensor, eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes the R-Precision between the prediciton and the ground truth for a single batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    batch_size = 10\n",
        "    precs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        if ground_truth.shape[0] != batch_size:\n",
        "          continue\n",
        "        input_idx = torch.nonzero(input[i] == 1).squeeze().flatten()\n",
        "        num_input_songs = input_idx.shape[0]\n",
        "\n",
        "        ground_truth_idx = torch.nonzero(ground_truth[i] == 1).squeeze().flatten()\n",
        "        num_ground_truth_songs = ground_truth_idx.shape[0]\n",
        "\n",
        "        if num_ground_truth_songs == 0:\n",
        "          continue\n",
        "\n",
        "        k = num_input_songs + num_ground_truth_songs\n",
        "        top_k_preds = eval_preds[i].topk(k, dim=0)\n",
        "        top_k_preds_idx = top_k_preds.indices.flatten().cpu()\n",
        "\n",
        "        confidences = top_k_preds.values.flatten()\n",
        "\n",
        "        already_in_playlist = np.intersect1d(top_k_preds_idx.detach().numpy(), input_idx.cpu().detach().numpy())\n",
        "\n",
        "        top_k_preds_idx = np.array([item for item in top_k_preds_idx.detach().numpy() if item not in already_in_playlist])[:num_ground_truth_songs]\n",
        "\n",
        "        common_elements = np.intersect1d(top_k_preds_idx, ground_truth_idx.cpu().detach().numpy())\n",
        "        num_common_elements = len(common_elements)\n",
        "        precs.append(num_common_elements/num_ground_truth_songs)\n",
        "\n",
        "    return precs\n",
        "\n",
        "def ndcg(eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes the Normalized Discounted Cumulative Gain for a single batch, given the model output and the ground truth.\n",
        "    \"\"\"\n",
        "    batch_size = 10\n",
        "    scores = []\n",
        "    for i in range(batch_size):\n",
        "        score = 0\n",
        "        k = 500\n",
        "        try:\n",
        "          top_k_preds = eval_preds[i].topk(k, dim=0)\n",
        "        except: return scores\n",
        "        top_k_preds_idx = top_k_preds.indices.flatten().cpu().numpy()\n",
        "\n",
        "        ground_truth_idx = torch.nonzero(ground_truth[i] == 1).squeeze().flatten()\n",
        "\n",
        "        common_elements = np.intersect1d(top_k_preds_idx, ground_truth_idx.cpu().detach().numpy())\n",
        "        num_common_elements = len(common_elements)\n",
        "\n",
        "        for j, elem in enumerate(top_k_preds_idx):\n",
        "            if elem not in common_elements: continue\n",
        "            if j == 1:\n",
        "              score += 1\n",
        "            else:\n",
        "               score += (1 / np.log2(j))\n",
        "\n",
        "        ideal_score = 1 + sum([(1 / np.log2(i)) for i in range(2, num_common_elements + 1)])\n",
        "        normalized_score = score / ideal_score\n",
        "\n",
        "        scores.append(normalized_score)\n",
        "    return scores\n",
        "\n",
        "def evaluate(input: torch.Tensor, eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    return r_prec(input, eval_preds, ground_truth)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OYT35yY74hrB"
      },
      "source": [
        "Let's define the function that will perform a validation step. This function will be called during training at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yP8xk2L34hrB"
      },
      "outputs": [],
      "source": [
        "def validate(model: DAE_tied) -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"\n",
        "    Given the model, performs an evaluation on the validation set.\n",
        "    \"\"\"\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    precs = []\n",
        "    tot_k = 0\n",
        "    model.eval()\n",
        "    with eval_merged_dataloader.make_torch_dataloader(batch_size=10, num_epochs = 1) as eval_dataloader:\n",
        "        for batch_idx, row in enumerate(eval_dataloader):\n",
        "            with torch.no_grad():\n",
        "                padded_eval_song_tensor = row[\"train_tracks_indices\"]\n",
        "                padded_eval_artist_tensor = row[\"train_artists_indices\"]\n",
        "\n",
        "                song_dense = padded_tensors_to_dense_matrix(padded_eval_song_tensor, SONG_SHAPE)\n",
        "                artist_dense = padded_tensors_to_dense_matrix(padded_eval_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "                song_dense = song_dense.to(device)\n",
        "                artist_dense = artist_dense.to(device)\n",
        "\n",
        "                del padded_eval_song_tensor\n",
        "                del padded_eval_artist_tensor\n",
        "\n",
        "                x = torch.concat((song_dense, artist_dense), dim=1)\n",
        "                y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "\n",
        "                model(x, y)\n",
        "\n",
        "                eval_preds = model.y_pred[:, :SONGS_VECTOR_LENGTH]\n",
        "\n",
        "                padded_eval_song_tensor_test = row[\"test_tracks_indices\"]\n",
        "\n",
        "                ground_truth = padded_tensors_to_dense_matrix(padded_eval_song_tensor_test, SONG_SHAPE)\n",
        "\n",
        "                ground_truth = ground_truth.to(device)\n",
        "\n",
        "                prec_list = evaluate(song_dense, eval_preds, ground_truth)\n",
        "                precs.extend(prec_list)\n",
        "\n",
        "        mean_prec: float = sum(precs) / len(precs)\n",
        "        model.train()\n",
        "        return model.cost, mean_prec\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aHXRUYad4hrB"
      },
      "source": [
        "Define the validation function that is invoked during the training in order to save the model parameters that optimize the performance evaluation on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I75MLKjC4hrC"
      },
      "outputs": [],
      "source": [
        "def perform_validation_step(model: DAE_tied, max_prec: float, save_path:str, save: bool):\n",
        "    eval_loss, prec = validate(model)\n",
        "\n",
        "    if prec > max_prec:\n",
        "        max_prec = prec\n",
        "        best_params = [param.cpu().detach().numpy() for param in model.d_params]\n",
        "        if save:\n",
        "          with open(save_path, \"wb\") as f:\n",
        "              pickle.dump(best_params, f)\n",
        "          print(f\"Best prec achieved: {prec}, parameters saved!\")\n",
        "        else:\n",
        "          print(f\"Best prec achieved: {prec}!\")\n",
        "    return max_prec"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the hyperparameters and initialize the Pretraining Model (DAE with tied weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DVGpMdqs7dQQ"
      },
      "outputs": [],
      "source": [
        "BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"best_params.pickle\")\n",
        "BEST_PARAMS_PATH_2 = os.path.join(SAVED_MODELS, \"best_params_reg.pickle\")\n",
        "\n",
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 100, #Smaller batch size, because of RAM issues\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH,\n",
        "    'hidden': 256,\n",
        "    'lr': 0.0005, #Smaller learning rate, because of the less data\n",
        "    'reg_lambda': 0.0,\n",
        "    'initval': \"NULL\",\n",
        "    \"keep_prob\": 0.8,\n",
        "    \"input_keep_prob\": 0.8, # This isn't used for because of the .uniform() in the train loop\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MByLaK1Amzgv",
        "outputId": "74578034-ea4b-4561-85e5-ccbbb0e38fd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DAE_tied()"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_model = DAE_tied(conf)\n",
        "pretrain_model.init_weight()\n",
        "pretrain_model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mNPmA2YijSO1"
      },
      "outputs": [],
      "source": [
        "pretrain_optimizer = optim.Adam(pretrain_model.d_params, lr=conf[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jAqv6QBEiK9",
        "outputId": "37e2a271-f9d9-41fe-915b-0732900afaf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_model.weights['encoder_h'].device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MeuVole7-WWC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GuqQMu_E0yaN"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the model is ready for training. As said before, at the end of each epoch I test the R-precision on the validation set, and save the model parameters if the R-precision is the maximum achieved. \n",
        "\n",
        "The training will follow a regularization technique called Hide & Seek, in which each batch iteration, one of the song vector or artist vector is masked (all elements are put to zero). In this way the model has to learn the relationship between the artists and the songs to reconstruct the input vector.\n",
        "\n",
        "The pretraining will require about 4 hours on CUDA T4 GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jIg2DIDNjEKm"
      },
      "outputs": [],
      "source": [
        "min_loss = 2000\n",
        "max_prec = 0.01\n",
        "max_test_prec = 0\n",
        "best_params = []\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Z_BjjpshDD2H"
      },
      "outputs": [],
      "source": [
        "if TRAIN:\n",
        "  from tqdm.notebook import tqdm\n",
        "  import random\n",
        "  NUM_EPOCHS = 50\n",
        "  with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "      SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "      ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "      for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "        # Pick random input_keep_prob between 0.5 and 0.8\n",
        "        pretrain_model.input_keep_prob = random.uniform(0.5, 0.8) \n",
        "\n",
        "        padded_song_tensor = row[\"tracks_indices\"]\n",
        "        padded_artist_tensor = row[\"artists_indices\"]\n",
        "\n",
        "        song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "        artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "        song_dense = song_dense.to(device)\n",
        "        artist_dense = artist_dense.to(device)\n",
        "\n",
        "        rand_int = np.random.randint(2)\n",
        "        if rand_int == 0:\n",
        "          #Zero-out the artists\n",
        "          pretrain_optimizer.zero_grad()\n",
        "          x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1)\n",
        "          y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "          pretrain_model(x, y)\n",
        "          loss = pretrain_model.cost\n",
        "          pretrain_model.cost.backward()\n",
        "          pretrain_optimizer.step()\n",
        "        if rand_int == 1:\n",
        "          #Zero-out the tracks\n",
        "          pretrain_optimizer.zero_grad()\n",
        "          x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1)\n",
        "          y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "          pretrain_model(x, y)\n",
        "          loss = pretrain_model.cost\n",
        "          pretrain_model.cost.backward()\n",
        "          pretrain_optimizer.step()\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        if batch_idx % 1000 == 0:\n",
        "          max_prec = perform_validation_step(pretrain_model, max_prec, BEST_PARAMS_PATH, save=True)\n",
        "          print(f\"Loss: {loss}\")\n",
        "          print(f\"Current max precision: {max_prec}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOoS7eGqlrH"
      },
      "source": [
        "Now that the pretraining phase is finished, the DAE model without tied weights can be trained. The pretrained model parameters are used as initialization. This will require about 8 hours on CUDA T4 GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IFAj48uuqlbd"
      },
      "outputs": [],
      "source": [
        "FINE_TUNED_BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"final_best_params.pickle\")\n",
        "\n",
        "conf_train = conf.copy()\n",
        "conf_train[\"initval\"] = BEST_PARAMS_PATH\n",
        "conf_train[\"lr\"] = conf[\"lr\"] / 2\n",
        "\n",
        "dae_model = DAE(conf_train)\n",
        "dae_model.init_weight()\n",
        "optimizer = optim.Adam(dae_model.d_params, lr=conf_train['lr'])\n",
        "\n",
        "min_loss = 600\n",
        "losses = []\n",
        "best_params = []\n",
        "max_prec = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gyw3SnVYyRP1"
      },
      "outputs": [],
      "source": [
        "if TRAIN:\n",
        "  NUM_EPOCHS = 100\n",
        "  with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "      ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "      SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "      for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "        # Pick random input_keep_prob between 0.5 and 0.8\n",
        "        dae_model.input_keep_prob = random.uniform(0.5, 0.8)\n",
        "\n",
        "        padded_song_tensor = row[\"tracks_indices\"]\n",
        "        padded_artist_tensor = row[\"artists_indices\"]\n",
        "\n",
        "        song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "        artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "        song_dense = song_dense.to(device)\n",
        "        artist_dense = artist_dense.to(device)\n",
        "\n",
        "        rand_int = np.random.randint(2)\n",
        "        if rand_int == 0:\n",
        "          #Zero-out the artists\n",
        "          optimizer.zero_grad()\n",
        "          x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1)\n",
        "          y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "          dae_model(x, y)\n",
        "          loss = dae_model.cost\n",
        "          dae_model.cost.backward()\n",
        "          optimizer.step()\n",
        "        if rand_int == 1:\n",
        "          #Zero-out the tracks\n",
        "          optimizer.zero_grad()\n",
        "          x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1)\n",
        "          y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "          dae_model(x, y)\n",
        "          loss = dae_model.cost\n",
        "          dae_model.cost.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "\n",
        "        if batch_idx % 1000 == 0:\n",
        "          max_prec = perform_validation_step(dae_model, max_prec, FINE_TUNED_BEST_PARAMS_PATH, save=True)\n",
        "          print(f\"Loss: {loss}\")\n",
        "          print(f\"Current max precision: {max_prec}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqyho9fPl0ow"
      },
      "source": [
        "# Performance Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's laod the Test set DataFrames and let's build the model in order to evaluate its performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "S9RFO76gK8Qb"
      },
      "outputs": [],
      "source": [
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5lgIPJo2tbUL"
      },
      "outputs": [],
      "source": [
        "SONGS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "songs_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "songs_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "artists_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "artists_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "TEST_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "TEST_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "test_train_df = spark.read.schema(playlist_schema).json(TEST_TRAIN_DF_PATH)\n",
        "test_test_df = spark.read.schema(playlist_schema).json(TEST_TEST_DF_PATH)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFaj_pRpK-Jd"
      },
      "source": [
        "As before, I create the Pytorch DataLoaders of the Test set. The test set, as the validation set, is further split in train and test (75%, 25%) in order to have songs to generate recommendations (Test-Train set), and a ground truth to evaluate the recommendations (Test-Test set). I apologize for the confusing names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgx4mZqKK-Jd",
        "outputId": "db80de5d-5c77-4f87-f79d-2a6860bbd234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n",
            "Max number of songs: 250\n",
            "Max number of songs: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max number of songs: 250\n"
          ]
        }
      ],
      "source": [
        "pytorch_songs_test_train_df = convert_sparse_to_indices(songs_embeddings_test_train.withColumnRenamed(\"tracks\", \"train_tracks\").select(\"train_tracks\", \"pid\"), column_name=\"train_tracks\")\n",
        "pytorch_artists_test_train_df = convert_sparse_to_indices(artists_embeddings_test_train.withColumnRenamed(\"tracks\", \"train_artists\").select(\"pid\", \"train_artists\"), column_name=\"train_artists\")\n",
        "songs_artists_test_train_df = pytorch_songs_test_train_df.join(pytorch_artists_test_train_df, on=\"pid\")\n",
        "\n",
        "pytorch_songs_test_test_df = convert_sparse_to_indices(songs_embeddings_test_test.withColumnRenamed(\"tracks\", \"test_tracks\").select(\"test_tracks\", \"pid\"), column_name=\"test_tracks\")\n",
        "pytorch_artists_test_test_df = convert_sparse_to_indices(artists_embeddings_test_test.withColumnRenamed(\"tracks\", \"test_artists\").select(\"pid\", \"test_artists\"), column_name=\"test_artists\")\n",
        "songs_artists_test_test_df = pytorch_songs_test_test_df.join(pytorch_artists_test_test_df, on=\"pid\")\n",
        "\n",
        "test_merged_df = songs_artists_test_train_df.join(songs_artists_test_test_df, on=\"pid\")\n",
        "\n",
        "# MIN_TRACK_COUNT = 50\n",
        "# MAX_TRACK_COUNT = 100\n",
        "# counter = F.udf(lambda x: len([item for item in x if item != -1]), returnType=IntegerType())\n",
        "# test_merged_df = test_merged_df\\\n",
        "#     .withColumn(\"train_tracks_count\", counter(F.col(\"train_tracks_indices\")))\\\n",
        "#     .withColumn(\"test_tracks_count\", counter(F.col(\"test_tracks_indices\")))\n",
        "# test_merged_df = test_merged_df.filter(f\"train_tracks_count >= {MIN_TRACK_COUNT}\").filter((f\"train_tracks_count <= {MAX_TRACK_COUNT}\"))\n",
        "test_merged_dataloader = make_spark_converter(test_merged_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6C9o6mymK-Jd"
      },
      "source": [
        "Define the function to perform the evaluation on the entire test set. Here of course Hide & Seek and dropout will not be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mmBmMnhVK-Jd"
      },
      "outputs": [],
      "source": [
        "def perform_evaluation(model: DAE_tied) -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"\n",
        "    Given the model, performs an evaluation on the validation set.\n",
        "    \"\"\"\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    precs = []\n",
        "    ndcgs = []\n",
        "    tot_k = 0\n",
        "    model.eval()\n",
        "    with test_merged_dataloader.make_torch_dataloader(batch_size=10, num_epochs = 1) as test_dataloader:\n",
        "        for batch_idx, row in enumerate(tqdm(test_dataloader, \"Testing...\", total=100)):\n",
        "            with torch.no_grad():\n",
        "                padded_eval_song_tensor = row[\"train_tracks_indices\"]\n",
        "                padded_eval_artist_tensor = row[\"train_artists_indices\"]\n",
        "\n",
        "                song_dense = padded_tensors_to_dense_matrix(padded_eval_song_tensor, SONG_SHAPE)\n",
        "                artist_dense = padded_tensors_to_dense_matrix(padded_eval_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "                song_dense = song_dense.to(device)\n",
        "                artist_dense = artist_dense.to(device)\n",
        "\n",
        "                del padded_eval_song_tensor\n",
        "                del padded_eval_artist_tensor\n",
        "\n",
        "                x = torch.concat((song_dense, artist_dense), dim=1)\n",
        "                y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "\n",
        "                model(x, y)\n",
        "\n",
        "                eval_preds = model.y_pred[:, :SONGS_VECTOR_LENGTH]\n",
        "\n",
        "                padded_eval_song_tensor_test = row[\"test_tracks_indices\"]\n",
        "\n",
        "                ground_truth = padded_tensors_to_dense_matrix(padded_eval_song_tensor_test, SONG_SHAPE)\n",
        "\n",
        "                ground_truth = ground_truth.to(device)\n",
        "\n",
        "                prec_list = r_prec(song_dense, eval_preds, ground_truth)\n",
        "                ndcg_list = ndcg(eval_preds, ground_truth)\n",
        "\n",
        "                print(prec_list, ndcg_list)\n",
        "\n",
        "                ndcgs.extend(ndcg_list)\n",
        "                precs.extend(prec_list)\n",
        "\n",
        "        mean_prec: float = sum(precs) / len(precs)\n",
        "        mean_ndcgs: float = sum(ndcgs) / len(ndcgs)\n",
        "\n",
        "\n",
        "        return mean_prec, mean_ndcgs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the configuration for the DAE model used during test. The configuration is almost the same, apart for the smaller batch size, and the initialization parameters, which are of course the parameters that achieved the best R-precision during the DAE without tied weights training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h7NRo3Ggt3h",
        "outputId": "1851f8d2-be5d-463c-e157-f6927c25999e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DAE()"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conf_test = conf_train.copy()\n",
        "conf_test[\"batch\"] = 10\n",
        "conf_test[\"initval\"] = FINE_TUNED_BEST_PARAMS_PATH\n",
        "\n",
        "dae_model_test = DAE(conf_test)\n",
        "dae_model_test.init_weight()\n",
        "dae_model_test.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "24181ccbc508497c8d91bc62977c7a57",
            "6bd1013ec55d4fed9d444fc863743609",
            "b98501d8e518406ebdb731e7d362f82d",
            "ada3f3131dda4b2181bbfabccea3d351",
            "d7ce340cf9154816a1503105908334ec",
            "cadcde31d164405a87e92ab2bb611b42",
            "81729cc12a754bd582c4b06cdd9d4356",
            "de3aa2009fcb452f93c5ff5dc4eca20a",
            "d0a730242456422bb38ffc4e9a0c40f6",
            "998ffffa3742403d9adef377f9c354fb",
            "e5507d89c0cf482c8fc7881fd1ca9672"
          ]
        },
        "id": "hRZ9w5nr5TaK",
        "outputId": "64fc642c-dffe-4f4b-e113-7fa17e48cf51"
      },
      "outputs": [],
      "source": [
        "mean_prec, mean_ndcg = perform_evaluation(dae_model_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After testing the model, we finally have the average of all the R-precisions and Normalized Discounted Cumulative Gains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPfJQI-pfONo",
        "outputId": "b6ce2d78-d100-49a3-ff84-3289097b679c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.13272003915706643, 0.33434479988039556)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_prec, mean_ndcg"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XvQ6e0PgCOZg"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24181ccbc508497c8d91bc62977c7a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bd1013ec55d4fed9d444fc863743609",
              "IPY_MODEL_b98501d8e518406ebdb731e7d362f82d",
              "IPY_MODEL_ada3f3131dda4b2181bbfabccea3d351"
            ],
            "layout": "IPY_MODEL_d7ce340cf9154816a1503105908334ec"
          }
        },
        "6bd1013ec55d4fed9d444fc863743609": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cadcde31d164405a87e92ab2bb611b42",
            "placeholder": "​",
            "style": "IPY_MODEL_81729cc12a754bd582c4b06cdd9d4356",
            "value": "Testing...: "
          }
        },
        "81729cc12a754bd582c4b06cdd9d4356": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "998ffffa3742403d9adef377f9c354fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada3f3131dda4b2181bbfabccea3d351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_998ffffa3742403d9adef377f9c354fb",
            "placeholder": "​",
            "style": "IPY_MODEL_e5507d89c0cf482c8fc7881fd1ca9672",
            "value": " 101/? [02:03&lt;00:00,  1.25s/it]"
          }
        },
        "b98501d8e518406ebdb731e7d362f82d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de3aa2009fcb452f93c5ff5dc4eca20a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0a730242456422bb38ffc4e9a0c40f6",
            "value": 100
          }
        },
        "cadcde31d164405a87e92ab2bb611b42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a730242456422bb38ffc4e9a0c40f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7ce340cf9154816a1503105908334ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3aa2009fcb452f93c5ff5dc4eca20a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5507d89c0cf482c8fc7881fd1ca9672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
