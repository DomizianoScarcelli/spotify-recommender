{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X-s_eNiVaxhn",
        "VJeY9PpvaHUJ",
        "m0Av0W0P66lC"
      ],
      "authorship_tag": "ABX9TyM1R9Z+glBu0uFtglgk3Gmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/main/NN_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install petastorm -qq\n",
        "!pip install lightning -qq\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "4facbf7d-1309-4ca8-b9fe-126ac28fc2b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hThe following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122532 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oozTtW3om3Ab",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from petastorm import make_batch_reader\n",
        "from petastorm.pytorch import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ],
      "metadata": {
        "id": "XG5sA53iP9z0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MocM1cmqQpn5",
        "outputId": "49ceabdd-ccdf-40a7-ee48-366d88fd0abe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cbb245-20a3-4d6f-b834-6b3cb1eb7f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=6c02fec8fdb7fc956e8633b27e8126d403ff956c7390e327316a1008308ed519\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f080ecd3-261b-4726-ec22-0b7a8f104d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8492c913-c23d-4e2e-ea26-e6f93fdc27a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-24T13:51:59+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b6e250-e4e7-427b-db62-4dac2f14f7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://c8d9-34-86-102-236.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78936ce1-0aaf-4876-e612-953b5f15515d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7fd9bf417bb0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.app.startTime', '1684936303502'),\n",
              "  ('spark.app.id', 'local-1684936307154'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.app.submitTime', '1684936303096'),\n",
              "  ('spark.driver.port', '40919'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.driver.host', '27ecc1793f2d'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataframe inside DataLoader"
      ],
      "metadata": {
        "id": "lPOD60GQQGse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hX-eiKFAcppy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "uuXmHBsbzPPH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"playlist_embeddings_new.parquet\")\n",
        "JSON_PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"playlist_embeddings_NN.json\")"
      ],
      "metadata": {
        "id": "6yhVVp_UQVrX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(JSON_PLAYLIST_EMBEDDINGS):\n",
        "  mapped_slice_df = spark.read.schema(playlist_schema_mapped).json(JSON_PLAYLIST_EMBEDDINGS)\n",
        "else:\n",
        "  mapped_slice_df = spark.read.schema(playlist_schema_mapped).parquet(PLAYLIST_EMBEDDINGS)\n",
        "  mapped_slice_df.write.json(JSON_PLAYLIST_EMBEDDINGS)"
      ],
      "metadata": {
        "id": "bI_e5oa0zlH5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import udf, length\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "import torch\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "def convert_sparse_to_indices(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "  # SparseTupleType = ArrayType(\n",
        "  #   ArrayType(IntegerType())\n",
        "  # )\n",
        "\n",
        "  @udf(returnType=ArrayType(IntegerType()))\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    indices_list = item.indices.tolist()\n",
        "    padding_width = max_songs - len(indices_list)\n",
        "    return indices_list + [-1] * padding_width\n",
        "  \n",
        "  max_songs = mapped_slice_df.select(max(\"num_tracks\")).first()[0]\n",
        "  print(f\"Max number of songs: {max_songs}\")\n",
        "  df = df.withColumn(\"embedding_indices\", transform_array(col(\"tracks\"))).drop(\"tracks\")\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SONGS = 681_806\n",
        "NUM_PLAYLISTS = 100_000\n",
        "def padded_tensors_to_sparse_matrix(padded_tensor: torch.Tensor) -> torch.Tensor:\n",
        "  print(f\"Shape of padded tensor: {padded_tensor.shape}\")\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  shape = (NUM_SONGS, NUM_PLAYLISTS)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), (NUM_SONGS, ))\n",
        "    rows.append(sparse_tensor)\n",
        "  unpadded = torch.stack(rows)\n",
        "  print(f\"Shape of unpadded tensor: {unpadded.shape}\")\n",
        "  return unpadded"
      ],
      "metadata": {
        "id": "eIIqDVeN7cU0"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper they have two matrices,l et $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ],
      "metadata": {
        "id": "UNak5bEo4cmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
        "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
        "from torchvision import transforms\n",
        "from petastorm import TransformSpec\n",
        "\n",
        "CACHE = os.path.join(GDRIVE_DIR, \"/big_data/cache_5\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_df = convert_sparse_to_indices(mapped_slice_df.select(\"tracks\"))\n",
        "converter = make_spark_converter(pytorch_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pKgJx26s73",
        "outputId": "02e00a6e-d461-4258-bce4-9c6cf37c4898"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 35870968 B (< 50 MB) of the parquet files is too small. Total size: 67065143 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///big_data/cache_5/20230524135613-appid-local-1684936307154-387b5a14-a13f-4038-9d02-ed733ab50ca7/part-00000-ff779d65-cc6c-41ef-bc5e-925f9140178b-c000.parquet, ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with converter.make_torch_dataloader() as dataloader:\n",
        "  for item in dataloader:\n",
        "    padded_tensor = item[\"embedding_indices\"]\n",
        "    sparse_tensor = padded_tensors_to_sparse_matrix(padded_tensor)\n",
        "    break"
      ],
      "metadata": {
        "id": "Z_BjjpshDD2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c526c7cb-cf59-4bdd-f79a-5f0efc4112ee"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded tensor: torch.Size([32, 250])\n",
            "Shape of unpadded tensor: torch.Size([32, 681806])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Model"
      ],
      "metadata": {
        "id": "m0Av0W0P66lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.learning_rate = conf[\"lr\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.x_positions = torch.LongTensor()\n",
        "        self.x_ones = torch.FloatTensor()\n",
        "\n",
        "        self.y_positions = torch.LongTensor()\n",
        "        self.y_ones = torch.FloatTensor()\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "        nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "        self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden))\n",
        "        nn.init.zeros_(self.biases['encoder_b'])\n",
        "        self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input))\n",
        "        nn.init.zeros_(self.biases['decoder_b'])\n",
        "        self.d_params = [self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l2 = torch.norm(self.weights['encoder_h']) ** 2\n",
        "      decoder_b_l2 = torch.norm(self.biases['decoder_b']) ** 2\n",
        "      encoder_b_l2 = torch.norm(self.biases['encoder_b']) ** 2\n",
        "\n",
        "      l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2\n",
        "      return l2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        # TODO: Take sparse matrix representation instead of dense\n",
        "        # self.x = x_positxions\n",
        "        # self.x_ones = x_ones\n",
        "        # self.y_positions = y_positions\n",
        "        # self.y_ones = y_ones\n",
        "\n",
        "        self.x = x.t()\n",
        "        self.y = y.t()\n",
        "\n",
        "        x_sparse = torch.sparse.FloatTensor(self.x_positions.t(), self.x_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        self.x = x_sparse.to_dense()\n",
        "        y_sparse = torch.sparse.FloatTensor(self.y_positions.t(), self.y_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        self.y = y_sparse.to_dense()\n",
        "\n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p= 1 - self.input_keep_prob) # Maybe error\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       0.55 * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "            \n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "      encoder_h_l2 = torch.norm(self.weights['encoder_h']) ** 2\n",
        "      decoder_b_l2 = torch.norm(self.biases['decoder_b']) ** 2\n",
        "      encoder_b_l2 = torch.norm(self.biases['encoder_b']) ** 2\n",
        "      decoder_h_l2 = torch.norm(self.weights['decoder_h']) ** 2\n",
        "\n",
        "      l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2 + decoder_h_l2\n",
        "      return l2"
      ],
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm trying to figure out what x and y are. From the source code, we can see this in the `data_readery.py:48` file:\n",
        "```python\n",
        "trk_positions = np.concatenate(trk_positions)\n",
        "art_positions = np.concatenate(art_positions)\n",
        "y_positions = np.concatenate((trk_positions, art_positions), 0)\n",
        "```\n",
        "So I can assume that y is just the concatenation of p_i and a_i.\n",
        "\n",
        "On the other hand, from `data_reader.py:250`, we can see:\n",
        "```python\n",
        "trk_positions = np.concatenate(trk_positions)\n",
        "art_positions = np.concatenate(art_positions)\n",
        "x_positions = np.concatenate((trk_positions, art_positions), 0)\n",
        "```\n",
        "\n",
        "So From this I assume `y` and `x` are the same thing, meanin the concatenation of A and P."
      ],
      "metadata": {
        "id": "zSdJCqx44lgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 32,\n",
        "    'n_input': 10000,\n",
        "    'hidden': 64,\n",
        "    'lr': 0.001,\n",
        "    'reg_lambda': 0.001,\n",
        "    'initval': 'NULL',\n",
        "    \"keep_prob\": 0.8,\n",
        "    \"input_keep_prob\": 0.8,\n",
        "    'save': './'\n",
        "}\n",
        "dae_model = DAE(conf)\n",
        "dae_model.init_weight()\n",
        "optimizer = optim.Adam(dae_model.d_params, lr=conf['lr'])"
      ],
      "metadata": {
        "id": "DVGpMdqs7dQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "np.random.seed(42)  # Set the random seed for reproducibility\n",
        "\n",
        "n = conf[\"n_input\"]  # Number of unique songs\n",
        "m = 1000   # Number of playlists\n",
        "k = 10000 # Number of unique artists\n",
        "\n",
        "# Generate random binary matrix P\n",
        "P = np.random.randint(0, 2, size=(m, n))\n",
        "\n",
        "# Generate random binary matrix A\n",
        "A = np.random.randint(0, 2, size=(m, k))\n",
        "\n",
        "A = torch.tensor(A, dtype=torch.float)\n",
        "P = torch.tensor(P, dtype=torch.float)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, A, P):\n",
        "        self.A = A\n",
        "        self.P = P\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.A)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.A[index], self.P[index])\n",
        "\n",
        "dataset = MyDataset(A, P)\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "\n",
        "batch_size = conf[\"batch\"]\n",
        "\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "bsAx78hM9JZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "num_epochs = 50\n",
        "losses = []\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Training...\"):\n",
        "    for p, a in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((p, a), dim=0).t()\n",
        "        y = torch.concat((p,a), dim=0).t()\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "    losses.append(loss)\n",
        "    print(f\"Loss: {loss}\")"
      ],
      "metadata": {
        "id": "DFPUteox8tHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}