{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XVvKeJoU4hq4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def is_running_on_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "LOCAL = not is_running_on_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "9f680ccc-7e00-4a7d-ee1c-b718bdd9ac06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    !pip install petastorm -qq\n",
        "    !pip install pyspark -qq\n",
        "    !pip install -U -q PyDrive -qq\n",
        "    !apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "oozTtW3om3Ab"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "if not LOCAL:\n",
        "    from google.colab import drive\n",
        "\n",
        "from typing import Tuple\n",
        "from functools import reduce\n",
        "import pickle\n",
        "import torch\n",
        "from petastorm import make_batch_reader\n",
        "from petastorm.pytorch import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XG5sA53iP9z0"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "if not LOCAL:\n",
        "    JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    GDRIVE_DIR = \"/content/drive\"\n",
        "    GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "    GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "    DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "    AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "    LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "    SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "    LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "    MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "    SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "else:\n",
        "    GDRIVE_DATA_DIR = os.path.abspath(\"./data\")\n",
        "    GDRIVE_HOME_DIR = GDRIVE_DATA_DIR\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "    JAVA_HOME = \"/opt/homebrew/opt/openjdk\"\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G20Yir8g4hq6",
        "outputId": "d822288c-cfa0-44c8-df65-816ee926b199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "config = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SyrTHVZR4hq8"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JAu9mQsxTxHj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fGgc9DHjT09S"
      },
      "outputs": [],
      "source": [
        "NUM_PLAYLISTS = 100_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BIalKyiH4hq9"
      },
      "outputs": [],
      "source": [
        "# The DF used for train (80% of the original) (playlist are different)\n",
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "# The DF used for testing (20% of the original) (playlist are different)\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "# The DF used for train in the NN model (can be filtered or not)\n",
        "NN_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_train_df-{NUM_PLAYLISTS}.json\")\n",
        "# The DF used for testing in the NN model (can be filtered or not)\n",
        "NN_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-{NUM_PLAYLISTS}.json\")\n",
        "# The partition in train test of the NN test set. (Same playlists, different songs)\n",
        "NN_TEST_DF_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_TEST_DF_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_EVAL_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-{NUM_PLAYLISTS}.json\")\n",
        "NN_EVAL_TRAIN_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_EVAL_TEST_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_eval_df-test-{NUM_PLAYLISTS}.json\")\n",
        "# New one:\n",
        "ARTISTS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_EVAL = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_EVAL_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_EVAL_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-eval-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "# The length of the artist vector length (Artist vectors are only used in the NN model)\n",
        "ARTIST_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_artist_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "# This may be filtered or not\n",
        "FILTERED_SONGS_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "\n",
        "SONGS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_TEST = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_SONGS_EMBEDDINGS_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "NN_SONGS_EMBEDDINGS_EVAL = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-{NUM_PLAYLISTS}.json\") #TODO: The logic to produce this still has to be coded.\n",
        "NN_SONGS_EMBEDDINGS_EVAL_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-train-{NUM_PLAYLISTS}.json\")\n",
        "NN_SONGS_EMBEDDINGS_EVAL_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-eval-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{NUM_PLAYLISTS}.json\")\n",
        "FILTERED_SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"nn_songs_info_df-{NUM_PLAYLISTS}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lo8gbiN1U1XJ"
      },
      "outputs": [],
      "source": [
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_TRAIN)\n",
        "artists_embeddings = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_TRAIN)\n",
        "song_mapping = spark.read.json(FILTERED_SONGS_INFO_DF)\n",
        "\n",
        "songs_embeddings_eval_train = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_EVAL_TRAIN)\n",
        "songs_embeddings_eval_test = spark.read.schema(playlist_schema_mapped).json(NN_SONGS_EMBEDDINGS_EVAL_TEST)\n",
        "\n",
        "artists_embeddings_eval_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_EVAL_TRAIN)\n",
        "artists_embeddings_eval_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_EVAL_TEST)\n",
        "\n",
        "with open(ARTIST_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  ARTIST_VECTOR_LENGTH = int(content)\n",
        "with open(SONGS_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  content = f.read()\n",
        "  SONGS_VECTOR_LENGTH = int(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY_szFyLTps4",
        "outputId": "d4c93451-c089-473e-a566-7f7ff4053886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              Alone|        false|20080| 1496880000|        50|        48|            1|(681805,[4874,542...|       19|   11128916|         42|\n",
            "|       Country Hits|        false|14258| 1504828800|       130|        77|            1|(681805,[3,1666,1...|       61|   28245005|         44|\n",
            "|              Peter|        false|14812| 1446595200|        65|        56|            1|(681805,[213,540,...|       15|   14212644|         47|\n",
            "|          Rock.....|        false| 1938| 1500163200|        57|        39|            1|(681805,[2908,350...|        5|   16766883|         19|\n",
            "|               punk|        false| 1180| 1504483200|        67|        45|            1|(681805,[61,3298,...|       21|   15393647|         26|\n",
            "|#CODGhosts Playlist|        false|14362| 1382918400|        50|        50|            1|(681805,[1565,301...|        2|   13601217|         50|\n",
            "|             #Dance|        false|14103| 1463961600|        33|        28|            1|(681805,[389,7336...|       12|    6901015|         24|\n",
            "|      #boostyourrun|        false|14325| 1425686400|        21|        21|            1|(681805,[497,1341...|        2|    5002212|         20|\n",
            "|      #boostyourrun|        false|20976| 1399075200|        21|        21|            1|(681805,[4481,918...|        2|    5400100|         19|\n",
            "|      #boostyourrun|        false|78200| 1402704000|        22|        22|            1|(681805,[39754,10...|        2|    5825915|         21|\n",
            "|           #college|        false|78334| 1490400000|        21|        19|            2|(681805,[10948,16...|        4|    5068086|         18|\n",
            "|               #tbt|        false| 1168| 1436832000|        29|        22|            1|(681805,[3585,463...|        2|    6829177|         15|\n",
            "|             $andy$|        false|78110| 1457395200|       117|        63|            1|(681805,[1678,325...|       14|   28982414|         40|\n",
            "|                '17|        false|20507| 1505433600|        38|        31|            2|(681805,[1122,489...|       31|    8051141|         30|\n",
            "|                '17|        false|78571| 1509235200|        34|        29|            1|(681805,[11315,15...|       16|    7500239|         26|\n",
            "|            'Merica|        false|78912| 1499126400|       241|       237|            1|(681805,[13,608,7...|        6|   57441819|        217|\n",
            "|           'merica |        false|78916| 1499990400|        67|        45|            1|(681805,[1666,206...|       21|   14868475|         35|\n",
            "|   **That New New**|        false|14379| 1432771200|        63|        52|            1|(681805,[270,2071...|       21|   14353567|         47|\n",
            "|           *HANNAH*|        false|20194| 1506211200|        84|        22|            1|(681805,[24791,69...|        5|   11964693|          6|\n",
            "|             *sigh*|        false|14217| 1509321600|       154|       101|            4|(681805,[114,632,...|       25|   38055908|         64|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|               name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|              Alone|        false|20080| 1496880000|        50|        48|            1|(110063,[392,588,...|       19|   11128916|         42|\n",
            "|       Country Hits|        false|14258| 1504828800|       130|        77|            1|(110063,[1,849,10...|       61|   28245005|         44|\n",
            "|              Peter|        false|14812| 1446595200|        65|        56|            1|(110063,[3,193,19...|       15|   14212644|         47|\n",
            "|          Rock.....|        false| 1938| 1500163200|        57|        39|            1|(110063,[1262,136...|        5|   16766883|         19|\n",
            "|               punk|        false| 1180| 1504483200|        67|        45|            1|(110063,[198,312,...|       21|   15393647|         26|\n",
            "|#CODGhosts Playlist|        false|14362| 1382918400|        50|        50|            1|(110063,[1070,140...|        2|   13601217|         50|\n",
            "|             #Dance|        false|14103| 1463961600|        33|        28|            1|(110063,[105,107,...|       12|    6901015|         24|\n",
            "|      #boostyourrun|        false|14325| 1425686400|        21|        21|            1|(110063,[2425,768...|        2|    5002212|         20|\n",
            "|      #boostyourrun|        false|20976| 1399075200|        21|        21|            1|(110063,[1040,197...|        2|    5400100|         19|\n",
            "|      #boostyourrun|        false|78200| 1402704000|        22|        22|            1|(110063,[3356,721...|        2|    5825915|         21|\n",
            "|           #college|        false|78334| 1490400000|        21|        19|            2|(110063,[192,1469...|        4|    5068086|         18|\n",
            "|               #tbt|        false| 1168| 1436832000|        29|        22|            1|(110063,[1967,213...|        2|    6829177|         15|\n",
            "|             $andy$|        false|78110| 1457395200|       117|        63|            1|(110063,[100,970,...|       14|   28982414|         40|\n",
            "|                '17|        false|20507| 1505433600|        38|        31|            2|(110063,[484,857,...|       31|    8051141|         30|\n",
            "|                '17|        false|78571| 1509235200|        34|        29|            1|(110063,[101,2233...|       16|    7500239|         26|\n",
            "|            'Merica|        false|78912| 1499126400|       241|       237|            1|(110063,[10,193,4...|        6|   57441819|        217|\n",
            "|           'merica |        false|78916| 1499990400|        67|        45|            1|(110063,[45,849,8...|       21|   14868475|         35|\n",
            "|   **That New New**|        false|14379| 1432771200|        63|        52|            1|(110063,[14,100,1...|       21|   14353567|         47|\n",
            "|           *HANNAH*|        false|20194| 1506211200|        84|        22|            1|(110063,[34404,41...|        5|   11964693|          6|\n",
            "|             *sigh*|        false|14217| 1509321600|       154|       101|            4|(110063,[793,1070...|       25|   38055908|         64|\n",
            "+-------------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None, 110063, 681805)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "songs_embeddings.show(), artists_embeddings.show(), ARTIST_VECTOR_LENGTH, SONGS_VECTOR_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPOD60GQQGse"
      },
      "source": [
        "# Convert PySpark DataFrame into PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "outputs": [],
      "source": [
        "def convert_sparse_to_indices(df: DataFrame, column_name: str) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "\n",
        "  @F.udf(returnType=ArrayType(IntegerType()))\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    indices_list = item.indices.tolist()\n",
        "    padding_width = max_songs - len(indices_list)\n",
        "    return indices_list + [-1] * padding_width\n",
        "\n",
        "  max_songs = songs_embeddings.select(F.max(\"num_tracks\")).first()[0]\n",
        "  print(f\"Max number of songs: {max_songs}\")\n",
        "  df = df.withColumn(f\"{column_name}_indices\", transform_array(F.col(column_name))).drop(column_name)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eIIqDVeN7cU0"
      },
      "outputs": [],
      "source": [
        "def padded_tensors_to_sparse_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    rows.append(sparse_tensor)\n",
        "  return torch.stack(rows)\n",
        "\n",
        "def padded_tensors_to_dense_matrix(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "  batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "  rows = []\n",
        "  for row_idx in range(batch_size):\n",
        "    row = padded_tensor[row_idx]\n",
        "    indices = row[row != -1]\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices.unsqueeze(0), torch.ones(indices.shape), shape)\n",
        "    dense = sparse_tensor.to_dense()\n",
        "    rows.append(dense)\n",
        "  unpadded = torch.stack(rows)\n",
        "  return unpadded\n",
        "\n",
        "def padded_tensors_to_dense_matrix_fast(padded_tensor: torch.Tensor, shape: tuple) -> torch.Tensor:\n",
        "    batch_size, max_songs = padded_tensor.size(0), padded_tensor.size(1)\n",
        "\n",
        "    row_indices = torch.arange(batch_size).unsqueeze(1).repeat(1, max_songs)\n",
        "    valid_mask = padded_tensor != -1\n",
        "\n",
        "    indices = torch.stack([row_indices[valid_mask], padded_tensor[valid_mask]], dim=0)\n",
        "    values = torch.ones(indices.shape[1])\n",
        "\n",
        "    size = (batch_size, shape[0])\n",
        "\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n",
        "    dense = sparse_tensor.to_dense()\n",
        "\n",
        "    return dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNak5bEo4cmV"
      },
      "source": [
        "In the paper they have two matrices,l et $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pKgJx26s73",
        "outputId": "e04d5fde-bc52-45b1-d62b-1c09d08230e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of songs: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 12113751 B (< 50 MB) of the parquet files is too small. Total size: 50282358 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///content/drive/MyDrive/cache/20230626164841-appid-local-1687798090577-8f063a0f-6761-436f-9d0a-2941f64e7cff/part-00003-d90c2d8c-daca-41b4-bbe0-5d6a0e28017b-c000.parquet, ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98465 98465 98465\n"
          ]
        }
      ],
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "\n",
        "CACHE = os.path.join(GDRIVE_HOME_DIR, \"cache\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "pytorch_songs_df = convert_sparse_to_indices(songs_embeddings.select(\"tracks\", \"pid\"), column_name=\"tracks\")\n",
        "# songs_converter = make_spark_converter(pytorch_songs_df)\n",
        "pytorch_artists_df = convert_sparse_to_indices(artists_embeddings.withColumnRenamed(\"tracks\", \"artists\").select(\"pid\", \"artists\"), column_name=\"artists\")\n",
        "# artist_converter = make_spark_converter(pytorch_artists_df)\n",
        "songs_artists_df = pytorch_songs_df.join(pytorch_artists_df, on=\"pid\")\n",
        "pytorch_merged_dataloader = make_spark_converter(songs_artists_df)\n",
        "\n",
        "\n",
        "print(pytorch_songs_df.count(), pytorch_artists_df.count(), songs_artists_df.count()) #Everything good here, this is nice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RueabAy34hq_"
      },
      "source": [
        "Creating the dataloader for the evaluation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Av0W0P66lC"
      },
      "source": [
        "# PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "        if LOCAL:\n",
        "            self.device = torch.device(\"mps\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "        self.z = None\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "        # encoder_h_l2 = (torch.sum(self.weights['encoder_h']) ** 2)/2\n",
        "        # decoder_b_l2 = (torch.sum(self.biases['decoder_b']) ** 2)/2\n",
        "        # encoder_b_l2 = (torch.sum(self.biases['encoder_b']) ** 2)/2\n",
        "\n",
        "        encoder_h_l1 = torch.sum(torch.abs(self.weights['encoder_h']))\n",
        "        decoder_b_l1 = torch.sum(torch.abs(self.biases['decoder_b']))\n",
        "        encoder_b_l1 = torch.sum(torch.abs(self.biases['encoder_b']))\n",
        "\n",
        "        # l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2\n",
        "        l1 = encoder_h_l1 + decoder_b_l1 + encoder_b_l1\n",
        "        return l1\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p= 1 - self.input_keep_prob)\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "\n",
        "        self.z = encoder_op\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        ALPHA = 0.5\n",
        "\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       ALPHA * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "\n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden).to(self.device))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden).to(self.device))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input).to(self.device))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]).to(self.device))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]).to(self.device))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]).to(self.device))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]).to(self.device))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "    #   encoder_h_l2 = (torch.sum(self.weights['encoder_h']) ** 2)/2\n",
        "    #   decoder_b_l2 = (torch.sum(self.biases['decoder_b']) ** 2)/2\n",
        "    #   encoder_b_l2 = (torch.sum(self.biases['encoder_b']) ** 2)/2\n",
        "    #   decoder_h_l2 = (torch.sum(self.weights['decoder_h']) ** 2)/2\n",
        "\n",
        "      encoder_h_l1 = torch.sum(torch.abs(self.weights['encoder_h']))\n",
        "      decoder_b_l1 = torch.sum(torch.abs(self.biases['decoder_b']))\n",
        "      encoder_b_l1 = torch.sum(torch.abs(self.biases['encoder_b']))\n",
        "      decoder_h_l1 = torch.sum(torch.abs(self.weights['decoder_h']))\n",
        "\n",
        "\n",
        "      l1 = encoder_h_l1 + decoder_b_l1 + encoder_b_l1 + decoder_h_l1\n",
        "\n",
        "    #   l2 = encoder_h_l2 + decoder_b_l2 + encoder_b_l2 + decoder_h_l2\n",
        "\n",
        "      return l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNcWBM0U4hrB",
        "outputId": "965f1929-b1a9-4dd5-d38a-e64f847532d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of songs: 250\n",
            "Max number of songs: 250\n",
            "Max number of songs: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max number of songs: 250\n"
          ]
        }
      ],
      "source": [
        "pytorch_songs_eval_train_df = convert_sparse_to_indices(songs_embeddings_eval_train.withColumnRenamed(\"tracks\", \"train_tracks\").select(\"train_tracks\", \"pid\"), column_name=\"train_tracks\")\n",
        "pytorch_artists_eval_train_df = convert_sparse_to_indices(artists_embeddings_eval_train.withColumnRenamed(\"tracks\", \"train_artists\").select(\"pid\", \"train_artists\"), column_name=\"train_artists\")\n",
        "songs_artists_eval_train_df = pytorch_songs_eval_train_df.join(pytorch_artists_eval_train_df, on=\"pid\")\n",
        "\n",
        "pytorch_songs_eval_test_df = convert_sparse_to_indices(songs_embeddings_eval_test.withColumnRenamed(\"tracks\", \"test_tracks\").select(\"test_tracks\", \"pid\"), column_name=\"test_tracks\")\n",
        "pytorch_artists_eval_test_df = convert_sparse_to_indices(artists_embeddings_eval_test.withColumnRenamed(\"tracks\", \"test_artists\").select(\"pid\", \"test_artists\"), column_name=\"test_artists\")\n",
        "songs_artists_eval_test_df = pytorch_songs_eval_test_df.join(pytorch_artists_eval_test_df, on=\"pid\")\n",
        "\n",
        "eval_merged_df = songs_artists_eval_train_df.join(songs_artists_eval_test_df, on=\"pid\")\n",
        "\n",
        "# counter = F.udf(lambda x: len([item for item in x if item != -1]), returnType=IntegerType())\n",
        "# eval_merged_df = eval_merged_df\\\n",
        "#     .withColumn(\"train_tracks_count\", counter(F.col(\"train_tracks_indices\")))\\\n",
        "#     .withColumn(\"test_tracks_count\", counter(F.col(\"test_tracks_indices\")))\n",
        "# eval_merged_df = eval_merged_df.filter(\"train_tracks_count > 100\")\n",
        "eval_merged_dataloader = make_spark_converter(eval_merged_df)\n",
        "\n",
        "eval_merged_num = eval_merged_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Vgt0T-v0K-JX"
      },
      "outputs": [],
      "source": [
        "if LOCAL:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kQPVslug4hrB"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "def k_prec(input: torch.Tensor, eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    batch_size = 10\n",
        "    precs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        if ground_truth.shape[0] != batch_size:\n",
        "          continue\n",
        "        input_idx = torch.nonzero(input[i] == 1).squeeze().flatten()\n",
        "        num_input_songs = input_idx.shape[0]\n",
        "\n",
        "        ground_truth_idx = torch.nonzero(ground_truth[i] == 1).squeeze().flatten()\n",
        "        num_ground_truth_songs = ground_truth_idx.shape[0]\n",
        "\n",
        "        if num_ground_truth_songs == 0:\n",
        "          continue\n",
        "\n",
        "        k = num_input_songs + num_ground_truth_songs\n",
        "        top_k_preds = eval_preds[i].topk(k, dim=0)\n",
        "        top_k_preds_idx = top_k_preds.indices.flatten().cpu()\n",
        "\n",
        "        confidences = top_k_preds.values.flatten()\n",
        "\n",
        "        already_in_playlist = np.intersect1d(top_k_preds_idx.detach().numpy(), input_idx.cpu().detach().numpy())\n",
        "\n",
        "        top_k_preds_idx = np.array([item for item in top_k_preds_idx.detach().numpy() if item not in already_in_playlist])[:num_ground_truth_songs]\n",
        "\n",
        "        common_elements = np.intersect1d(top_k_preds_idx, ground_truth_idx.cpu().detach().numpy())\n",
        "        num_common_elements = len(common_elements)\n",
        "        precs.append(num_common_elements/num_ground_truth_songs)\n",
        "\n",
        "    return precs\n",
        "\n",
        "def ndcg(eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> float:\n",
        "    return 0\n",
        "\n",
        "def evaluate(input: torch.Tensor, eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    return k_prec(input, eval_preds, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SK52nYJ-Edb9"
      },
      "outputs": [],
      "source": [
        "def test_k_prec(eval_preds: torch.Tensor, ground_truth: torch.Tensor) -> List[float]:\n",
        "    batch_size = 10\n",
        "    precs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        ground_truth_idx = torch.nonzero(ground_truth[i] == 1).squeeze().flatten()\n",
        "        num_ground_truth_songs = ground_truth_idx.shape[0]\n",
        "\n",
        "        k = num_ground_truth_songs\n",
        "        top_k_preds = eval_preds[i].topk(k, dim=0)\n",
        "        top_k_preds_idx = top_k_preds.indices.flatten().cpu()\n",
        "\n",
        "        confidences = top_k_preds.values.flatten()\n",
        "\n",
        "        common_elements = np.intersect1d(top_k_preds_idx, ground_truth_idx.cpu().detach().numpy())\n",
        "        num_common_elements = len(common_elements)\n",
        "        precs.append(num_common_elements/num_ground_truth_songs)\n",
        "\n",
        "    return precs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYT35yY74hrB"
      },
      "source": [
        "Creating the petastorm converters for validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yP8xk2L34hrB"
      },
      "outputs": [],
      "source": [
        "def validate(model: DAE_tied) -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"\n",
        "    Given the model, performs an evaluation on the validation set.\n",
        "    \"\"\"\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    precs = []\n",
        "    tot_k = 0\n",
        "    model.eval()\n",
        "    with eval_merged_dataloader.make_torch_dataloader(batch_size=10, num_epochs = 1) as eval_dataloader:\n",
        "        for batch_idx, row in enumerate(eval_dataloader):\n",
        "            with torch.no_grad():\n",
        "                # if batch_idx == 1:\n",
        "                    # break #TODO: faster but less generalized, remove for the final training\n",
        "                padded_eval_song_tensor = row[\"train_tracks_indices\"]\n",
        "                padded_eval_artist_tensor = row[\"train_artists_indices\"]\n",
        "\n",
        "                song_dense = padded_tensors_to_dense_matrix(padded_eval_song_tensor, SONG_SHAPE)\n",
        "                artist_dense = padded_tensors_to_dense_matrix(padded_eval_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "                song_dense = song_dense.to(device)\n",
        "                artist_dense = artist_dense.to(device)\n",
        "\n",
        "                del padded_eval_song_tensor\n",
        "                del padded_eval_artist_tensor\n",
        "\n",
        "                x = torch.concat((song_dense, artist_dense), dim=1)\n",
        "                y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "\n",
        "                model(x, y)\n",
        "\n",
        "                eval_preds = model.y_pred[:, :SONGS_VECTOR_LENGTH]\n",
        "\n",
        "                padded_eval_song_tensor_test = row[\"test_tracks_indices\"]\n",
        "\n",
        "                ground_truth = padded_tensors_to_dense_matrix(padded_eval_song_tensor_test, SONG_SHAPE)\n",
        "\n",
        "                ground_truth = ground_truth.to(device)\n",
        "\n",
        "                prec_list = evaluate(song_dense, eval_preds, ground_truth)\n",
        "                precs.extend(prec_list)\n",
        "\n",
        "        mean_prec: float = sum(precs) / len(precs)\n",
        "        model.train()\n",
        "        return model.cost, mean_prec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHXRUYad4hrB"
      },
      "source": [
        "Define the validation function that is invoked during the training in order to save the model parameters that optimize the performance evaluation on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "I75MLKjC4hrC"
      },
      "outputs": [],
      "source": [
        "def perform_validation_step(model: DAE_tied, max_prec: float, save_path:str, save: bool):\n",
        "    eval_loss, prec = validate(model)\n",
        "\n",
        "    if prec > max_prec:\n",
        "        max_prec = prec\n",
        "        best_params = [param.cpu().detach().numpy() for param in model.d_params]\n",
        "        if save:\n",
        "          with open(save_path, \"wb\") as f:\n",
        "              pickle.dump(best_params, f)\n",
        "          print(f\"Best prec achieved: {prec}, parameters saved!\")\n",
        "        else:\n",
        "          print(f\"Best prec achieved: {prec}!\")\n",
        "    return max_prec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DVGpMdqs7dQQ"
      },
      "outputs": [],
      "source": [
        "BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"best_params.pickle\")\n",
        "BEST_PARAMS_PATH_2 = os.path.join(SAVED_MODELS, \"best_params_reg.pickle\")\n",
        "\n",
        "#Hyperparameters used in the paper\n",
        "conf = {\n",
        "    'batch': 100,\n",
        "    'n_input': SONGS_VECTOR_LENGTH + ARTIST_VECTOR_LENGTH,\n",
        "    'hidden': 256,\n",
        "    'lr': 0.0005,\n",
        "    'reg_lambda': 0.0,\n",
        "    'initval': \"NULL\", #TODO: just for now\n",
        "    \"keep_prob\": 0.8,\n",
        "    \"input_keep_prob\": 0.8, # This isn't used for now because of the .uniform()\n",
        "    'save': os.path.join(SAVED_MODELS, \"dae_model.pickle\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_model = DAE_tied(conf)\n",
        "pretrain_model.init_weight()\n",
        "pretrain_model.train()"
      ],
      "metadata": {
        "id": "MByLaK1Amzgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "mNPmA2YijSO1"
      },
      "outputs": [],
      "source": [
        "pretrain_optimizer = optim.Adam(pretrain_model.d_params, lr=conf[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jAqv6QBEiK9",
        "outputId": "8cb1c36b-fdb4-4429-f6de-79d0e2f4198b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "pretrain_model.weights['encoder_h'].device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "MeuVole7-WWC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuqQMu_E0yaN"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jIg2DIDNjEKm"
      },
      "outputs": [],
      "source": [
        "min_loss = 2000\n",
        "max_prec = 0.01\n",
        "max_test_prec = 0\n",
        "best_params = []\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvPuc5hqirK"
      },
      "source": [
        "Pretrain with `DAE_tied`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2babae799a764aa280ec958a5b0c21bb",
            "3875320ed0984dbb9f2b80091a212016",
            "4d626adbf48c4d2c851169239092b1e4",
            "45734152ab714109854d220ed95ec827",
            "605a4c54b499454684336ffa89626c1e",
            "4e55d64275a749f790ee367ccd496a3e",
            "c6341a642114480d928ed3cbbf7ce969",
            "3386ceaafa8e44839f276db6bf7eccd9",
            "a4cc87be4a27467b9ed027aad878c070",
            "0322e38756a843329e4bd8f374d40600",
            "65b6752c2a1046c5ae8b12227738c227"
          ]
        },
        "id": "Z_BjjpshDD2H",
        "outputId": "086e64b5-46d4-468b-ffc4-fbc438c4d3a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2babae799a764aa280ec958a5b0c21bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training model:   0%|          | 0/50000.0 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best prec achieved: 0.0652841740696658, parameters saved!\n",
            "Loss: 555.4766235351562\n",
            "Current max precision: 0.0652841740696658\n",
            "Loss: 506.506591796875\n",
            "Current max precision: 0.0652841740696658\n",
            "Best prec achieved: 0.06933045716177416, parameters saved!\n",
            "Loss: 494.8131103515625\n",
            "Current max precision: 0.06933045716177416\n",
            "Best prec achieved: 0.07013826569713057, parameters saved!\n",
            "Loss: 523.2730712890625\n",
            "Current max precision: 0.07013826569713057\n",
            "Best prec achieved: 0.07259800662562141, parameters saved!\n",
            "Loss: 608.2797241210938\n",
            "Current max precision: 0.07259800662562141\n",
            "Loss: 473.7826843261719\n",
            "Current max precision: 0.07259800662562141\n",
            "Best prec achieved: 0.07503012409782188, parameters saved!\n",
            "Loss: 510.06732177734375\n",
            "Current max precision: 0.07503012409782188\n",
            "Best prec achieved: 0.07915146271568177, parameters saved!\n",
            "Loss: 384.23773193359375\n",
            "Current max precision: 0.07915146271568177\n",
            "Best prec achieved: 0.07917328833648327, parameters saved!\n",
            "Loss: 490.74267578125\n",
            "Current max precision: 0.07917328833648327\n",
            "Loss: 421.5077209472656\n",
            "Current max precision: 0.07917328833648327\n",
            "Loss: 472.80859375\n",
            "Current max precision: 0.07917328833648327\n",
            "Loss: 518.7977905273438\n",
            "Current max precision: 0.07917328833648327\n",
            "Best prec achieved: 0.0815041887030879, parameters saved!\n",
            "Loss: 404.8358459472656\n",
            "Current max precision: 0.0815041887030879\n",
            "Loss: 450.681640625\n",
            "Current max precision: 0.0815041887030879\n",
            "Loss: 500.4065246582031\n",
            "Current max precision: 0.0815041887030879\n",
            "Best prec achieved: 0.08404697487480872, parameters saved!\n",
            "Loss: 428.1878356933594\n",
            "Current max precision: 0.08404697487480872\n",
            "Loss: 499.6080017089844\n",
            "Current max precision: 0.08404697487480872\n",
            "Best prec achieved: 0.0844509831692617, parameters saved!\n",
            "Loss: 367.69091796875\n",
            "Current max precision: 0.0844509831692617\n",
            "Best prec achieved: 0.08617660797895743, parameters saved!\n",
            "Loss: 482.34796142578125\n",
            "Current max precision: 0.08617660797895743\n",
            "Best prec achieved: 0.08642720961201303, parameters saved!\n",
            "Loss: 410.0203857421875\n",
            "Current max precision: 0.08642720961201303\n",
            "Best prec achieved: 0.08954001290837695, parameters saved!\n",
            "Loss: 505.1968688964844\n",
            "Current max precision: 0.08954001290837695\n",
            "Best prec achieved: 0.09181037164570013, parameters saved!\n",
            "Loss: 453.57647705078125\n",
            "Current max precision: 0.09181037164570013\n",
            "Loss: 451.2860107421875\n",
            "Current max precision: 0.09181037164570013\n",
            "Loss: 349.99072265625\n",
            "Current max precision: 0.09181037164570013\n",
            "Loss: 375.874755859375\n",
            "Current max precision: 0.09181037164570013\n",
            "Loss: 422.7665100097656\n",
            "Current max precision: 0.09181037164570013\n",
            "Best prec achieved: 0.09463658163578417, parameters saved!\n",
            "Loss: 443.2144775390625\n",
            "Current max precision: 0.09463658163578417\n",
            "Loss: 379.8240661621094\n",
            "Current max precision: 0.09463658163578417\n",
            "Loss: 389.3409423828125\n",
            "Current max precision: 0.09463658163578417\n",
            "Loss: 422.4903564453125\n",
            "Current max precision: 0.09463658163578417\n",
            "Loss: 421.4541320800781\n",
            "Current max precision: 0.09463658163578417\n",
            "Loss: 395.1285705566406\n",
            "Current max precision: 0.09463658163578417\n",
            "Best prec achieved: 0.09721131553892198, parameters saved!\n",
            "Loss: 457.421875\n",
            "Current max precision: 0.09721131553892198\n",
            "Best prec achieved: 0.09772541752765912, parameters saved!\n",
            "Loss: 502.2916259765625\n",
            "Current max precision: 0.09772541752765912\n",
            "Best prec achieved: 0.09940425311387732, parameters saved!\n",
            "Loss: 505.1724853515625\n",
            "Current max precision: 0.09940425311387732\n",
            "Loss: 457.5738220214844\n",
            "Current max precision: 0.09940425311387732\n",
            "Loss: 351.3299560546875\n",
            "Current max precision: 0.09940425311387732\n",
            "Loss: 421.4560852050781\n",
            "Current max precision: 0.09940425311387732\n",
            "Best prec achieved: 0.10098609275153261, parameters saved!\n",
            "Loss: 374.83392333984375\n",
            "Current max precision: 0.10098609275153261\n",
            "Loss: 466.20263671875\n",
            "Current max precision: 0.10098609275153261\n",
            "Loss: 353.91717529296875\n",
            "Current max precision: 0.10098609275153261\n",
            "Loss: 378.5515441894531\n",
            "Current max precision: 0.10098609275153261\n",
            "Loss: 395.8990478515625\n",
            "Current max precision: 0.10098609275153261\n",
            "Loss: 438.2190856933594\n",
            "Current max precision: 0.10098609275153261\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "NUM_EPOCHS = 50\n",
        "with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "      # Pick random input_keep_prob between 0.5 and 0.8\n",
        "      pretrain_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "      padded_song_tensor = row[\"tracks_indices\"]\n",
        "      padded_artist_tensor = row[\"artists_indices\"]\n",
        "\n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      rand_int = np.random.randint(2)\n",
        "      if rand_int == 0:\n",
        "        #Zero-out the artists\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1)\n",
        "        # x = torch.concat((song_dense, artist_dense), dim=1)\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "        pretrain_model(x, y)\n",
        "        loss = pretrain_model.cost\n",
        "        pretrain_model.cost.backward()\n",
        "        pretrain_optimizer.step()\n",
        "      if rand_int == 1:\n",
        "        #Zero-out the tracks\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1)\n",
        "        # x = torch.concat((song_dense, artist_dense), dim=1)\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "        pretrain_model(x, y)\n",
        "        loss = pretrain_model.cost\n",
        "        pretrain_model.cost.backward()\n",
        "        pretrain_optimizer.step()\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "      if batch_idx % 1000 == 0:\n",
        "        max_prec = perform_validation_step(pretrain_model, max_prec, BEST_PARAMS_PATH, save=True)\n",
        "        print(f\"Loss: {loss}\")\n",
        "        print(f\"Current max precision: {max_prec}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_model.y_pred[0].cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "id": "o-FMr7tYmQ4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F85w_ZpmPD9t"
      },
      "outputs": [],
      "source": [
        "params = [param.cpu().detach().numpy() for param in pretrain_model.d_params]\n",
        "with open(BEST_PARAMS_PATH_2, 'wb') as f:\n",
        "  pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOoS7eGqlrH"
      },
      "source": [
        "Train with `DAE` loading the pretrained `DAE_tied` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IFAj48uuqlbd"
      },
      "outputs": [],
      "source": [
        "conf_train = conf.copy()\n",
        "conf_train[\"initval\"] = BEST_PARAMS_PATH\n",
        "conf_train[\"lr\"] = conf[\"lr\"] / 2\n",
        "\n",
        "dae_model = DAE(conf_train)\n",
        "dae_model.init_weight()\n",
        "optimizer = optim.Adam(dae_model.d_params, lr=conf_train['lr'])\n",
        "\n",
        "min_loss = 600\n",
        "losses = []\n",
        "best_params = []\n",
        "max_prec = 0.01\n",
        "FINE_TUNED_BEST_PARAMS_PATH = os.path.join(SAVED_MODELS, \"final_best_params.pickle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "osIKMm2WCdLM"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyw3SnVYyRP1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "ef0a953b27f34050a3a89129426545be",
            "4a9d77063cc4437495f3b5000889d90e",
            "d465a6d7db8848c297281d4711f270d0",
            "043ea6a71bab4e20a08546e5c7281ab9",
            "c56a8609c1f848deb15094a8b18a7c98",
            "a77a45b266664f868ebc1ccf06033776",
            "1cf3a0d83fa548518ddc1e820bf55951",
            "cd37b756e64c4683896df2b16e96bdde",
            "1f9f61511cb445d09e6c05e37333d65e",
            "affd7d3eee704f21b5026fc904ce28d2",
            "49cbb40a848443df981474ca7b17d52d"
          ]
        },
        "outputId": "3e8438cb-bdc5-43dc-bc9a-fb1fadf28532"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training model:   0%|          | 0/50000.0 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef0a953b27f34050a3a89129426545be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best prec achieved: 0.09693931031575653, parameters saved!\n",
            "Loss: 455.0484313964844\n",
            "Current max precision: 0.09693931031575653\n",
            "Best prec achieved: 0.09921560449568, parameters saved!\n",
            "Loss: 351.79998779296875\n",
            "Current max precision: 0.09921560449568\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "NUM_EPOCHS = 50\n",
        "with pytorch_merged_dataloader.make_torch_dataloader(batch_size=conf[\"batch\"], num_epochs = NUM_EPOCHS) as train_dataloader:\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    for batch_idx, row in tqdm(enumerate(train_dataloader), desc=f\"Training model\", total= (NUM_PLAYLISTS / conf[\"batch\"]) * NUM_EPOCHS):\n",
        "      # Pick random input_keep_prob between 0.5 and 0.8\n",
        "      dae_model.input_keep_prob = random.uniform(0.5, 0.8) #TODO: make this dynamic inside the conf\n",
        "\n",
        "      padded_song_tensor = row[\"tracks_indices\"]\n",
        "      padded_artist_tensor = row[\"artists_indices\"]\n",
        "\n",
        "      song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "      artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "      song_dense = song_dense.to(device)\n",
        "      artist_dense = artist_dense.to(device)\n",
        "\n",
        "      rand_int = np.random.randint(2)\n",
        "      if rand_int == 0:\n",
        "        #Zero-out the artists\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((song_dense, torch.zeros_like(artist_dense)), dim=1)\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "      if rand_int == 1:\n",
        "        #Zero-out the tracks\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.concat((torch.zeros_like(song_dense), artist_dense), dim=1)\n",
        "        y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "        dae_model(x, y)\n",
        "        loss = dae_model.cost\n",
        "        dae_model.cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "\n",
        "      if batch_idx % 1000 == 0:\n",
        "        max_prec = perform_validation_step(dae_model, max_prec, FINE_TUNED_BEST_PARAMS_PATH, save=True)\n",
        "        print(f\"Loss: {loss}\")\n",
        "        print(f\"Current max precision: {max_prec}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dae_model.y_pred[0].cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "id": "qswK6RoeuW8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQL3P9JByR7Q"
      },
      "source": [
        "Let's see how the loss decreases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF9gP0FMLdw2"
      },
      "outputs": [],
      "source": [
        "SAVE_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"model_new.pickle\")\n",
        "params = [param.cpu().detach().numpy() for param in dae_model.d_params]\n",
        "with open(SAVE_MODEL_PATH, 'wb') as f:\n",
        "  pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4JxgOCvx2TE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(len(losses))\n",
        "plt.plot(x, [loss.item() for loss in losses])\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Progression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqyho9fPl0ow"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9RFO76gK8Qb"
      },
      "outputs": [],
      "source": [
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lgIPJo2tbUL"
      },
      "outputs": [],
      "source": [
        "SONGS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_songs_embeddings-test-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-train-{NUM_PLAYLISTS}.json\")\n",
        "ARTISTS_EMBEDDINGS_PATH_TEST_TEST = os.path.join(SAVED_DFS_PATH, f\"nn_artists_embeddings-test-test{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "songs_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "songs_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "artists_embeddings_test_train = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TRAIN)\n",
        "artists_embeddings_test_test = spark.read.schema(playlist_schema_mapped).json(ARTISTS_EMBEDDINGS_PATH_TEST_TEST)\n",
        "\n",
        "TEST_TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-train-{NUM_PLAYLISTS}.json\")\n",
        "TEST_TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"nn_test_df-test-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "test_train_df = spark.read.schema(playlist_schema).json(TEST_TRAIN_DF_PATH)\n",
        "test_test_df = spark.read.schema(playlist_schema).json(TEST_TEST_DF_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKbyd0kRl3SH"
      },
      "outputs": [],
      "source": [
        "# def construct_prediction_df(prediction: torch.Tensor, mapping: DataFrame, top_n: int = 50) -> DataFrame:\n",
        "#   pred_np = prediction.detach().numpy()\n",
        "#   indexes = np.arange(pred_np.shape[0]) # To compensate the index start at 1\n",
        "#   schema = StructType([\n",
        "#       StructField(\"pos\", IntegerType()),\n",
        "#       StructField(\"confidence\", FloatType())\n",
        "#   ])\n",
        "#   prediction_df = spark.createDataFrame([(pos, conf) for pos, conf in zip(indexes.tolist(), pred_np.tolist())],schema)\n",
        "#   prediction_info = prediction_df.join(mapping, \"pos\")\n",
        "#   return prediction_info\n",
        "\n",
        "# # prediction_df = construct_prediction_df(prediction, songs_df_test)\n",
        "# # prediction_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ9LquCdSbJJ"
      },
      "outputs": [],
      "source": [
        "# def remove_existing_tracks(playlist_tracks: DataFrame, recommendations_df: DataFrame) -> DataFrame:\n",
        "#   playlist_tracks = playlist_tracks.select(\"track_uri\").cache()\n",
        "#   playlist_tracks_compatible = playlist_tracks.join(F.broadcast(recommendations_df), on=\"track_uri\")\n",
        "#   playlist_tracks.unpersist()\n",
        "#   return recommendations_df.exceptAll(F.broadcast(playlist_tracks_compatible))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx-tghy0bAkU"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb793kgebBwJ"
      },
      "outputs": [],
      "source": [
        "# def precision_at_k(recommendations, ground_truth, num_of_recommendations) -> float:\n",
        "#     \"\"\"\n",
        "#     Calculates precision at k for the recommendations.\n",
        "#     \"\"\"\n",
        "#     recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").cache()\n",
        "#     reccomended_relevant_tracks_count = recommended_relevant_tracks.count() #this can be top_n_results.join in order to be more performant\n",
        "#     recommended_relevant_tracks.unpersist()\n",
        "#     precision = reccomended_relevant_tracks_count / float(num_of_recommendations)\n",
        "\n",
        "#     return precision\n",
        "\n",
        "\n",
        "# import math\n",
        "# def normalized_discounted_cumulative_gain(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "#   recommendations_list = recommendations.collect()\n",
        "#   cumulative_gain = 0\n",
        "\n",
        "#   intersection = recommendations.join(ground_truth, \"track_uri\").count()\n",
        "#   if intersection == 0: return 0\n",
        "\n",
        "#   ideal_cumulative_gain = 1 + np.array([(1 / math.log(i, 2)) for i in range(2, 2+intersection)]).sum() #TODO: replace this with sum([])\n",
        "#   for index, row in enumerate(recommendations_list):\n",
        "#     i = index + 1\n",
        "#     is_rel = ground_truth.filter(F.col(\"track_uri\").isin(row.track_uri)).count() > 0\n",
        "#     rel = 1 if is_rel else 0\n",
        "#     if i == 1:\n",
        "#       cumulative_gain += rel\n",
        "#     else:\n",
        "#       cumulative_gain += (rel / math.log(i, 2))\n",
        "#   return cumulative_gain / ideal_cumulative_gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFaj_pRpK-Jd"
      },
      "source": [
        "Create Test Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgx4mZqKK-Jd"
      },
      "outputs": [],
      "source": [
        "pytorch_songs_test_train_df = convert_sparse_to_indices(songs_embeddings_test_train.withColumnRenamed(\"tracks\", \"train_tracks\").select(\"train_tracks\", \"pid\"), column_name=\"train_tracks\")\n",
        "pytorch_artists_test_train_df = convert_sparse_to_indices(artists_embeddings_test_train.withColumnRenamed(\"tracks\", \"train_artists\").select(\"pid\", \"train_artists\"), column_name=\"train_artists\")\n",
        "songs_artists_test_train_df = pytorch_songs_test_train_df.join(pytorch_artists_test_train_df, on=\"pid\")\n",
        "\n",
        "pytorch_songs_test_test_df = convert_sparse_to_indices(songs_embeddings_test_test.withColumnRenamed(\"tracks\", \"test_tracks\").select(\"test_tracks\", \"pid\"), column_name=\"test_tracks\")\n",
        "pytorch_artists_test_test_df = convert_sparse_to_indices(artists_embeddings_test_test.withColumnRenamed(\"tracks\", \"test_artists\").select(\"pid\", \"test_artists\"), column_name=\"test_artists\")\n",
        "songs_artists_test_test_df = pytorch_songs_test_test_df.join(pytorch_artists_test_test_df, on=\"pid\")\n",
        "\n",
        "test_merged_df = songs_artists_test_train_df.join(songs_artists_test_test_df, on=\"pid\")\n",
        "\n",
        "MIN_TRACK_COUNT = 50\n",
        "MAX_TRACK_COUNT = 100\n",
        "counter = F.udf(lambda x: len([item for item in x if item != -1]), returnType=IntegerType())\n",
        "test_merged_df = test_merged_df\\\n",
        "    .withColumn(\"train_tracks_count\", counter(F.col(\"train_tracks_indices\")))\\\n",
        "    .withColumn(\"test_tracks_count\", counter(F.col(\"test_tracks_indices\")))\n",
        "test_merged_df = test_merged_df.filter(f\"train_tracks_count >= {MIN_TRACK_COUNT}\").filter((f\"train_tracks_count <= {MAX_TRACK_COUNT}\"))\n",
        "test_merged_dataloader = make_spark_converter(test_merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_merged_df.count()"
      ],
      "metadata": {
        "id": "hCCXY08Sfmad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C9o6mymK-Jd"
      },
      "source": [
        "Define function to perform the full evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmBmMnhVK-Jd"
      },
      "outputs": [],
      "source": [
        "def perform_evaluation(model: DAE_tied) -> Tuple[torch.Tensor, float, float]:\n",
        "    \"\"\"\n",
        "    Given the model, performs an evaluation on the validation set.\n",
        "    \"\"\"\n",
        "    ARTIST_SHAPE = (ARTIST_VECTOR_LENGTH, )\n",
        "    SONG_SHAPE = (SONGS_VECTOR_LENGTH, )\n",
        "    precs = []\n",
        "    tot_k = 0\n",
        "    model.eval()\n",
        "    with test_merged_dataloader.make_torch_dataloader(batch_size=10, num_epochs = 1) as test_dataloader:\n",
        "        for batch_idx, row in enumerate(tqdm(test_dataloader, \"Testing...\")):\n",
        "            with torch.no_grad():\n",
        "                padded_eval_song_tensor = row[\"train_tracks_indices\"]\n",
        "                padded_eval_artist_tensor = row[\"train_artists_indices\"]\n",
        "\n",
        "                song_dense = padded_tensors_to_dense_matrix(padded_eval_song_tensor, SONG_SHAPE)\n",
        "                artist_dense = padded_tensors_to_dense_matrix(padded_eval_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "                song_dense = song_dense.to(device)\n",
        "                artist_dense = artist_dense.to(device)\n",
        "\n",
        "                del padded_eval_song_tensor\n",
        "                del padded_eval_artist_tensor\n",
        "\n",
        "                x = torch.concat((song_dense, artist_dense), dim=1)\n",
        "                y = torch.concat((song_dense, artist_dense), dim=1)\n",
        "\n",
        "                model(x, y)\n",
        "\n",
        "                eval_preds = model.y_pred[:, :SONGS_VECTOR_LENGTH]\n",
        "\n",
        "                padded_eval_song_tensor_test = row[\"test_tracks_indices\"]\n",
        "\n",
        "                ground_truth = padded_tensors_to_dense_matrix(padded_eval_song_tensor_test, SONG_SHAPE)\n",
        "\n",
        "                ground_truth = ground_truth.to(device)\n",
        "\n",
        "                prec_list = evaluate(song_dense, eval_preds, ground_truth)\n",
        "                precs.extend(prec_list)\n",
        "\n",
        "        mean_prec: float = sum(precs) / len(precs)\n",
        "        return model.cost, mean_prec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUYEe5iHbTHs"
      },
      "source": [
        "Creating the dataloaders for the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h7NRo3Ggt3h"
      },
      "outputs": [],
      "source": [
        "conf_test = conf_train.copy()\n",
        "conf_test[\"batch\"] = 10\n",
        "conf_test[\"initval\"] = FINE_TUNED_BEST_PARAMS_PATH\n",
        "\n",
        "dae_model_test = DAE(conf_test)\n",
        "dae_model_test.init_weight()\n",
        "dae_model_test.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "O6ZXFlX1fDP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRZ9w5nr5TaK"
      },
      "outputs": [],
      "source": [
        "_, mean_prec = perform_evaluation(dae_model_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_prec"
      ],
      "metadata": {
        "id": "qPfJQI-pfONo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CHWL9oIYfRQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg8V8VSSu3XI"
      },
      "outputs": [],
      "source": [
        "# torch.concat((song_dense[:, 1:], artist_dense[:, 1:]), dim=1).t().shape\n",
        "# (song_dense[:, 0] == 0.).all(), (artist_dense[:, 0] == 0.).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLpygSgaMVr6"
      },
      "outputs": [],
      "source": [
        "# result = result.to(\"cpu\")\n",
        "# prediction_df = construct_prediction_df(result[10][:SONGS_VECTOR_LENGTH_TEST], songs_df_test, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uYwzqLFMjWE"
      },
      "outputs": [],
      "source": [
        "# prediction_df.orderBy(\"pos\").show(truncate=False)\n",
        "# songs_df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0wojIXkuFnN"
      },
      "outputs": [],
      "source": [
        "# def evaluate_batch(batch_result: torch.Tensor, batch_n: int) -> Tuple[float, float]:\n",
        "#   \"\"\"\n",
        "#   Returns the precision and NDCG for a given batch.\n",
        "#   \"\"\"\n",
        "\n",
        "#   return 0,0\n",
        "\n",
        "# def perform_evaluation(songs_dataloader, artists_dataloader, test_set):\n",
        "#   \"\"\"\n",
        "#   Returns the precision and NDCG, averaged from all the samples in the test set\n",
        "#   \"\"\"\n",
        "#   with songs_converter_test.make_torch_dataloader(num_epochs =1) as songs_dataloader:\n",
        "#     with artist_converter_test.make_torch_dataloader(num_epochs=1) as artists_dataloader:\n",
        "#       zipped_dataloaders = zip(songs_dataloader, artists_dataloader)\n",
        "#       for batch_idx, (song, artist) in tqdm(enumerate(zipped_dataloaders), desc=f\"Evaluation...\", total= (NUM_PLAYLISTS / 32) * NUM_EPOCHS):\n",
        "#         padded_song_tensor = song[\"embedding_indices\"]\n",
        "#         padded_artist_tensor = artist[\"embedding_indices\"]\n",
        "\n",
        "#         song_dense = padded_tensors_to_dense_matrix(padded_song_tensor, SONG_SHAPE)\n",
        "#         artist_dense = padded_tensors_to_dense_matrix(padded_artist_tensor, ARTIST_SHAPE)\n",
        "\n",
        "#         x = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "#         y = torch.concat((song_dense, artist_dense), dim=1).t()\n",
        "#         dae_model(x,y)\n",
        "#         batch_result = dae_model.y_pred\n",
        "#         break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4LWsnwWeVAV"
      },
      "outputs": [],
      "source": [
        "# #Testing the first batch\n",
        "\n",
        "# results = []\n",
        "# # for i in tqdm(range(32)):\n",
        "# PID = 71860\n",
        "# ground_truth = test_test_df.filter(F.col(\"pid\") == PID).select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "# playlist_train_songs = test_train_df.filter(F.col(\"pid\") == PID).select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "\n",
        "# #Removing rare songs (that the model didn't consider)\n",
        "# #This may be not the best approach since the train songs or ground truth may become 0\n",
        "# clean_ground_truth = ground_truth.join(song_mapping, on=\"track_uri\").cache()\n",
        "# clean_playlist_train_songs = playlist_train_songs.join(song_mapping, on=\"track_uri\").cache()\n",
        "\n",
        "# # n_recommendations = ground_truth.count() or 1\n",
        "# n_recommendations = 500\n",
        "# result = result.cpu()\n",
        "\n",
        "# #The result[i] has to be aligned with the PID. i != PID.\n",
        "# prediction_df = construct_prediction_df(result[1][:SONGS_VECTOR_LENGTH], song_mapping, n_recommendations).cache()\n",
        "# clean_prediction_df = remove_existing_tracks(clean_playlist_train_songs, prediction_df)\n",
        "\n",
        "# clean_prediction_df = prediction_df.orderBy(F.col(\"confidence\").desc()).limit(n_recommendations).cache()\n",
        "\n",
        "# prec = precision_at_k(clean_prediction_df, clean_ground_truth, n_recommendations)\n",
        "# gain = normalized_discounted_cumulative_gain(clean_prediction_df, clean_ground_truth, n_recommendations)\n",
        "# print(PID, prec, gain)\n",
        "# results.append((prec, gain))\n",
        "\n",
        "# ground_truth.unpersist()\n",
        "# clean_ground_truth.unpersist()\n",
        "# playlist_train_songs.unpersist()\n",
        "# clean_playlist_train_songs.unpersist()\n",
        "# prediction_df.unpersist()\n",
        "# clean_prediction_df.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XUxFpPGVLf-"
      },
      "outputs": [],
      "source": [
        "# print(\"Playlist train songs\")\n",
        "# playlist_train_songs.show(truncate=False)\n",
        "# print(\"Clean playlist train songs\")\n",
        "# clean_playlist_train_songs.show(truncate=False)\n",
        "# print(\"Ground truth songs\")\n",
        "# ground_truth.show(truncate=False)\n",
        "# print(\"Clean ground truth songs\")\n",
        "# clean_ground_truth.show(truncate=False)\n",
        "# print(f\"Prediction df songs (num recommendations: {n_recommendations})\")\n",
        "# prediction_df.show(truncate=False)\n",
        "# print(f\"Clean Prediction df songs (num recommendations: {n_recommendations})\")\n",
        "# clean_prediction_df.show(truncate=False)\n",
        "# prec, gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN8Fk_7Jel4-"
      },
      "outputs": [],
      "source": [
        "# def average_results(results):\n",
        "#   prec_avg = sum(prec for prec, _ in results) / len(results)\n",
        "#   gain_avg = sum(gain for _, gain in results) / len(results)\n",
        "#   return prec_avg, gain_avg\n",
        "\n",
        "# average_results(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XvQ6e0PgCOZg"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2babae799a764aa280ec958a5b0c21bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3875320ed0984dbb9f2b80091a212016",
              "IPY_MODEL_4d626adbf48c4d2c851169239092b1e4",
              "IPY_MODEL_45734152ab714109854d220ed95ec827"
            ],
            "layout": "IPY_MODEL_605a4c54b499454684336ffa89626c1e"
          }
        },
        "3875320ed0984dbb9f2b80091a212016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e55d64275a749f790ee367ccd496a3e",
            "placeholder": "​",
            "style": "IPY_MODEL_c6341a642114480d928ed3cbbf7ce969",
            "value": "Training model:  88%"
          }
        },
        "4d626adbf48c4d2c851169239092b1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3386ceaafa8e44839f276db6bf7eccd9",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4cc87be4a27467b9ed027aad878c070",
            "value": 43805
          }
        },
        "45734152ab714109854d220ed95ec827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0322e38756a843329e4bd8f374d40600",
            "placeholder": "​",
            "style": "IPY_MODEL_65b6752c2a1046c5ae8b12227738c227",
            "value": " 43805/50000.0 [4:36:02&lt;38:00,  2.72it/s]"
          }
        },
        "605a4c54b499454684336ffa89626c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e55d64275a749f790ee367ccd496a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6341a642114480d928ed3cbbf7ce969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3386ceaafa8e44839f276db6bf7eccd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4cc87be4a27467b9ed027aad878c070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0322e38756a843329e4bd8f374d40600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b6752c2a1046c5ae8b12227738c227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef0a953b27f34050a3a89129426545be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a9d77063cc4437495f3b5000889d90e",
              "IPY_MODEL_d465a6d7db8848c297281d4711f270d0",
              "IPY_MODEL_043ea6a71bab4e20a08546e5c7281ab9"
            ],
            "layout": "IPY_MODEL_c56a8609c1f848deb15094a8b18a7c98"
          }
        },
        "4a9d77063cc4437495f3b5000889d90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a77a45b266664f868ebc1ccf06033776",
            "placeholder": "​",
            "style": "IPY_MODEL_1cf3a0d83fa548518ddc1e820bf55951",
            "value": "Training model:   2%"
          }
        },
        "d465a6d7db8848c297281d4711f270d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd37b756e64c4683896df2b16e96bdde",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f9f61511cb445d09e6c05e37333d65e",
            "value": 1018
          }
        },
        "043ea6a71bab4e20a08546e5c7281ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_affd7d3eee704f21b5026fc904ce28d2",
            "placeholder": "​",
            "style": "IPY_MODEL_49cbb40a848443df981474ca7b17d52d",
            "value": " 1018/50000.0 [07:09&lt;5:27:29,  2.49it/s]"
          }
        },
        "c56a8609c1f848deb15094a8b18a7c98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77a45b266664f868ebc1ccf06033776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cf3a0d83fa548518ddc1e820bf55951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd37b756e64c4683896df2b16e96bdde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9f61511cb445d09e6c05e37333d65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "affd7d3eee704f21b5026fc904ce28d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49cbb40a848443df981474ca7b17d52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}