{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XvQ6e0PgCOZg",
        "X-s_eNiVaxhn",
        "VJeY9PpvaHUJ"
      ],
      "authorship_tag": "ABX9TyNEx7lKhHwMoa/50N6EsozC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/main/NN_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "XvQ6e0PgCOZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install petastorm -qq\n",
        "!pip install lightning -qq\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjPz4xH4WFYB",
        "outputId": "ea0330da-75ce-44f9-b422-6ab6b6cc59a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hopenjdk-8-jdk-headless is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oozTtW3om3Ab",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from petastorm import make_batch_reader\n",
        "from petastorm.pytorch import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ],
      "metadata": {
        "id": "XG5sA53iP9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MocM1cmqQpn5",
        "outputId": "1f2b09a5-00f6-4362-8938-f7f88e13ccf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07709900-2f07-4825-b4d0-d4634f8c4089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=70953b6345089854df609c684d4a8a85b0b721e03d70b7cfd96aae21b568c420\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9ed70f-2b14-48ac-fa2b-0cc89ec06007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef69a0b-946c-4311-8a24-638f6c7edc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-22T14:14:38+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed38bdf-ddc6-4916-d577-7ab6a65417d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://abc7-107-167-177-9.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d736648f-2761-4a83-f9cd-8884d94f3ff4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f9841592da0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.driver.host', '74431fbd7482'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.app.submitTime', '1684764853540'),\n",
              "  ('spark.app.startTime', '1684764854004'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.driver.port', '39189'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.app.id', 'local-1684764858262'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataframe inside DataLoader"
      ],
      "metadata": {
        "id": "lPOD60GQQGse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hX-eiKFAcppy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"playlist_embeddings.json\")"
      ],
      "metadata": {
        "id": "6yhVVp_UQVrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_slice_df = spark.read.json(PLAYLIST_EMBEDDINGS)"
      ],
      "metadata": {
        "id": "bI_e5oa0zlH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "import torch\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "def convert_sparse_to_indices(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a dataframe fo columns \"pos\":int and \"tracks\":SparseVector, it returns a new dataframe where\n",
        "  the SparseVector are replaced with a list of the indices where the values are.\n",
        "  (The value information is lost, but we don't care since they are binary values so they will be all ones)\n",
        "  \"\"\"\n",
        "\n",
        "  SparseTupleType = ArrayType(\n",
        "    ArrayType(IntegerType())\n",
        "  )\n",
        "\n",
        "  @udf(returnType=SparseTupleType)\n",
        "  def transform_array(item: SparseVector):\n",
        "    \"\"\"\n",
        "    Given a SparseVector (binary) it returns the tuple that represent it, of the type (size, indices)\n",
        "    \"\"\"\n",
        "    return [item.indices]\n",
        "\n",
        "  embeddings_df = df.withColumn(\"embedding_indices\", transform_array(col(\"tracks\"))).drop(\"tracks\")\n",
        "  return embeddings_df\n"
      ],
      "metadata": {
        "id": "QCKbUCcaEpQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_df = convert_sparse_to_indices(mapped_slice_df.select(\"pid\", \"tracks\"))"
      ],
      "metadata": {
        "id": "9M9jjVhC465w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32T5gbVS65ce",
        "outputId": "02de8414-ed08-4049-ec89-ba074a06e22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "| pid|          embeddings|\n",
            "+----+--------------------+\n",
            "|1000|[[3345, 3386, 452...|\n",
            "|1001|[[21674, 30886, 4...|\n",
            "|1002|[[7381, 9005, 215...|\n",
            "|1003|[[1667, 1681, 336...|\n",
            "|1004|[[210, 810, 1282,...|\n",
            "|1005|[[1666, 10041, 10...|\n",
            "|1006|[[1235, 2470, 763...|\n",
            "|1007|[[2455, 5450, 577...|\n",
            "|1008|[[1479, 3330, 805...|\n",
            "|1009|[[2546, 3285, 646...|\n",
            "|1010|[[850, 1254, 2481...|\n",
            "|1011|[[73, 1266, 2973,...|\n",
            "|1012|[[1764, 3714, 730...|\n",
            "|1013|[[429, 1258, 1681...|\n",
            "|1014|[[136, 3715, 4561...|\n",
            "|1015|[[50287, 57102, 7...|\n",
            "|1016|[[835, 1515, 2142...|\n",
            "|1017|[[20910, 69451, 7...|\n",
            "|1018|[[812, 1252, 2778...|\n",
            "|1019|[[2088, 3665, 409...|\n",
            "+----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_indices_to_sparse_tensor(indices: List[int]) -> torch.sparse.FloatTensor:\n",
        "    \"\"\"\n",
        "    Given a list of strictly increasing indices, it returns the sparse binary torch tensor that they represent.\n",
        "    \"\"\"\n",
        "    indices = torch.LongTensor(indices)\n",
        "    SIZE = 681806\n",
        "    arr = np.zeros(SIZE)\n",
        "    arr[indices] = np.ones(len(indices))\n",
        "    return torch.tensor(arr).to_sparse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIIqDVeN7cU0",
        "outputId": "74b71ce9-d248-461f-9235-a755ed24268b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[1, 2, 3, 4]]),\n",
              "       values=tensor([1., 1., 1., 1.]),\n",
              "       size=(681806,), nnz=4, dtype=torch.float64, layout=torch.sparse_coo)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper they have two matrices,l et $n$ be the number of unique songs, $m$ the number of playlists and $k$ the number of unique artists:\n",
        "\n",
        "- $P \\in \\mathbb{R}^{m \\times n}$ where $p_i = 1$ if song $i$ is in the playlist, $p_i=0$ otherwise\n",
        "- $A \\in \\mathbb{R}^{m \\times k}$ where $a_i=1$ if the artist is present in the playlist, $a_i = 0$ otherwise"
      ],
      "metadata": {
        "id": "UNak5bEo4cmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
        "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
        "from torchvision import transforms\n",
        "from petastorm import TransformSpec\n",
        "\n",
        "\n",
        "CACHE= os.path.join(GDRIVE_DIR, \"/big_data/cache\")\n",
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, f'file://{CACHE}')\n",
        "\n",
        "\n",
        "# DfSchema = Unischema('DfSchema', [\n",
        "#     UnischemaField('pos', np.uint8, (), ScalarCodec(IntegerType()), False),\n",
        "#     UnischemaField('embedding_indices', np.float32, (2,), NdarrayCodec(), False),\n",
        "# ])\n",
        "\n",
        "pytorch_df = convert_sparse_to_indices(mapped_slice_df.select(\"pid\", \"tracks\"))\n",
        "converter = make_spark_converter(pytorch_df)\n",
        "\n",
        "with converter.make_torch_dataloader() as dataloader:\n",
        "  for item in dataloader:\n",
        "    print(item)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pKgJx26s73",
        "outputId": "a981edbc-e833-4f25-d793-ce35b003f175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n",
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 13464913 B (< 50 MB) of the parquet files is too small. Total size: 25541995 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///big_data/cache/20230522151558-appid-local-1684764858262-af6d0bee-434e-4882-b19e-bb24779b19ad/part-00001-225bfa33-8bb0-4af7-a8f9-5207025d204d-c000.parquet, ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pid': tensor([51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009,\n",
            "        51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019,\n",
            "        51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029,\n",
            "        51030, 51031]), 'modified_at': tensor([1483228800, 1480636800, 1491609600, 1492300800, 1507766400, 1508716800,\n",
            "        1494460800, 1508889600, 1509062400, 1500940800, 1477785600, 1509148800,\n",
            "        1475193600, 1507593600, 1472688000, 1466294400, 1466812800, 1490140800,\n",
            "        1445472000, 1509062400, 1449532800, 1480204800, 1466640000, 1448409600,\n",
            "        1427241600, 1509235200, 1440979200, 1382400000, 1478304000, 1445472000,\n",
            "        1504569600, 1495065600])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = pytorch_df.first().embedding_indices"
      ],
      "metadata": {
        "id": "Z_BjjpshDD2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Model"
      ],
      "metadata": {
        "id": "m0Av0W0P66lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "class DAE_tied(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE_tied, self).__init__()\n",
        "        self.save_dir = conf[\"save\"]\n",
        "\n",
        "        self.n_batch = conf[\"batch\"]\n",
        "        self.n_input = conf[\"n_input\"]\n",
        "        self.n_hidden = conf[\"hidden\"]\n",
        "        self.learning_rate = conf[\"lr\"]\n",
        "        self.reg_lambda = conf[\"reg_lambda\"]\n",
        "\n",
        "        self.x_positions = torch.LongTensor()\n",
        "        self.x_ones = torch.FloatTensor()\n",
        "\n",
        "        self.y_positions = torch.LongTensor()\n",
        "        self.y_ones = torch.FloatTensor()\n",
        "\n",
        "        self.keep_prob = torch.tensor(conf[\"keep_prob\"], dtype=torch.float32)\n",
        "        self.input_keep_prob = torch.tensor(conf[\"input_keep_prob\"], dtype=torch.float32)\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.d_params = []\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "        nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "        self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden))\n",
        "        nn.init.zeros_(self.biases['encoder_b'])\n",
        "        self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input))\n",
        "        nn.init.zeros_(self.biases['decoder_b'])\n",
        "        self.d_params = [self.weights['encoder_h'], self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    # Building the encoder\n",
        "    def encoder(self, x):\n",
        "        # Encoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.add(torch.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
        "        layer = torch.sigmoid(layer)\n",
        "        layer = torch.nn.functional.dropout(layer, p=1 - self.keep_prob)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    # Building the decoder\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['encoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "        l2 = torch.nn.functional.l2_loss(self.weights['encoder_h']) + torch.nn.functional.l2_loss(\n",
        "            self.biases['decoder_b']) + torch.nn.functional.l2_loss(self.biases['encoder_b'])\n",
        "        return l2\n",
        "\n",
        "    def forward(self, x_positions, x_ones, y_positions, y_ones):\n",
        "        self.x_positions = x_positions\n",
        "        self.x_ones = x_ones\n",
        "        self.y_positions = y_positions\n",
        "        self.y_ones = y_ones\n",
        "\n",
        "        x_sparse = torch.sparse.FloatTensor(self.x_positions.t(), self.x_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        self.x = x_sparse.to_dense()\n",
        "        y_sparse = torch.sparse.FloatTensor(self.y_positions.t(), self.y_ones, torch.Size([self.n_batch, self.n_input]))\n",
        "        self.y = y_sparse.to_dense()\n",
        "\n",
        "        x_dropout = torch.nn.functional.dropout(self.x, p=1 - self.input_keep_prob)\n",
        "        reduce_sum = torch.sum(x_dropout, dim=1, keepdim=True)\n",
        "        self.x_dropout = torch.div(x_dropout, reduce_sum + 1e-10)\n",
        "\n",
        "        encoder_op = self.encoder(self.x_dropout)\n",
        "        self.y_pred = self.decoder(encoder_op)\n",
        "\n",
        "        l2 = self.l2_loss()\n",
        "\n",
        "        L = -torch.sum(self.y * torch.log(self.y_pred + 1e-10) +\n",
        "                       0.55 * (1 - self.y) * torch.log(1 - self.y_pred + 1e-10), dim=1)\n",
        "        self.cost = torch.mean(L) + self.reg_lambda * l2\n",
        "\n",
        "    def save_model(self):\n",
        "        params = [param.detach().numpy() for param in self.d_params]\n",
        "        with open(self.save_dir, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "            \n",
        "class DAE(DAE_tied):\n",
        "    def __init__(self, conf):\n",
        "        super(DAE, self).__init__(conf)\n",
        "        self.initval_dir = conf[\"initval\"]\n",
        "\n",
        "    def init_weight(self):\n",
        "        if self.initval_dir == 'NULL':\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "            nn.init.xavier_uniform_(self.weights['encoder_h'])\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(self.n_input, self.n_hidden))\n",
        "            nn.init.xavier_uniform_(self.weights['decoder_h'])\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(self.n_hidden))\n",
        "            nn.init.zeros_(self.biases['encoder_b'])\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(self.n_input))\n",
        "            nn.init.zeros_(self.biases['decoder_b'])\n",
        "        else:\n",
        "            with open(self.initval_dir, 'rb') as f:\n",
        "                emb = pickle.load(f)\n",
        "            self.weights['encoder_h'] = nn.Parameter(torch.FloatTensor(emb[0]))\n",
        "            self.weights['decoder_h'] = nn.Parameter(torch.FloatTensor(emb[1]))\n",
        "            self.biases['encoder_b'] = nn.Parameter(torch.FloatTensor(emb[2]))\n",
        "            self.biases['decoder_b'] = nn.Parameter(torch.FloatTensor(emb[3]))\n",
        "\n",
        "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
        "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder Hidden layer with sigmoid activation #1\n",
        "        layer = torch.sigmoid(torch.add(torch.matmul(x, self.weights['decoder_h'].t()), self.biases['decoder_b']))\n",
        "        return layer\n",
        "\n",
        "    def l2_loss(self):\n",
        "        l2 = torch.nn.functional.l2_loss(self.weights['encoder_h']) + torch.nn.functional.l2_loss(\n",
        "            self.biases['decoder_b']) + torch.nn.functional.l2_loss(self.biases['encoder_b']) + torch.nn.functional.l2_loss(\n",
        "            self.weights['decoder_h'])\n",
        "        return l2"
      ],
      "metadata": {
        "id": "6-YbESXD66Oh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = {\n",
        "    'batch': 32,\n",
        "    'n_input': 100,\n",
        "    'hidden': 64,\n",
        "    'lr': 0.001,\n",
        "    'reg_lambda': 0.001,\n",
        "    'initval': 'NULL',\n",
        "    \"keep_prob\": 0.8,\n",
        "    \"input_keep_prob\": (0.5,0.8),\n",
        "    'save': './'\n",
        "}\n",
        "dae_model = DAE(conf)\n",
        "dae_model.init_weight()"
      ],
      "metadata": {
        "id": "DVGpMdqs7dQQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(dae_model.parameters(), lr=conf['lr'])"
      ],
      "metadata": {
        "id": "Z_SNRwMm8mr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "np.random.seed(42)  # Set the random seed for reproducibility\n",
        "\n",
        "n = 500  # Number of unique songs\n",
        "m = 50   # Number of playlists\n",
        "k = 100  # Number of unique artists\n",
        "\n",
        "# Generate random binary matrix P\n",
        "P = np.random.randint(0, 2, size=(m, n))\n",
        "\n",
        "# Generate random binary matrix A\n",
        "A = np.random.randint(0, 2, size=(m, k))\n",
        "\n",
        "A = torch.tensor(A)\n",
        "P = torch.tensor(P)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, A):\n",
        "        self.A = A\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.A)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.A[index]\n",
        "\n",
        "dataset = MyDataset(A)\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "batch_size = 32\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "bsAx78hM9JZI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_data in train_loader:\n",
        "        inputs, targets = batch_data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = dae_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "DFPUteox8tHq",
        "outputId": "2d6a8f9c-e44e-4c5c-f810-6ea7f31f68d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e12a5f765e58>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdae_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}