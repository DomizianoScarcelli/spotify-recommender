{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VJeY9PpvaHUJ",
        "x9gbMKT_lw9S",
        "aoVjgtwlI9qe"
      ],
      "authorship_tag": "ABX9TyPlil86YP6j8xDhQXonq8Ud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/item-based-cf/item_based_CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N3wVgRfHYoOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2f486a-44c8-4e64-db5e-5587b618f18a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122542 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGHPv9OqY9MI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from typing import Tuple, Callable, Dict\n",
        "from functools import reduce\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db91832-0614-4a77-9f75-a816b415d243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e51b959-352d-4184-ad16-e74170781b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=0e5ba23692511bc83de748351a5c9259dd1176c3600ae422d20225d38d6985ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa2b085-10ca-4d53-ab4b-ddcc97f3db7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ngrok ...\rDownloading ngrok: 0%\rDownloading ngrok: 1%\rDownloading ngrok: 2%\rDownloading ngrok: 3%\rDownloading ngrok: 4%\rDownloading ngrok: 5%\rDownloading ngrok: 6%\rDownloading ngrok: 7%\rDownloading ngrok: 8%\rDownloading ngrok: 9%\rDownloading ngrok: 10%\rDownloading ngrok: 11%\rDownloading ngrok: 12%\rDownloading ngrok: 13%\rDownloading ngrok: 14%\rDownloading ngrok: 15%\rDownloading ngrok: 16%\rDownloading ngrok: 17%\rDownloading ngrok: 18%\rDownloading ngrok: 19%\rDownloading ngrok: 20%\rDownloading ngrok: 21%\rDownloading ngrok: 22%\rDownloading ngrok: 23%\rDownloading ngrok: 24%\rDownloading ngrok: 25%\rDownloading ngrok: 26%\rDownloading ngrok: 27%\rDownloading ngrok: 28%\rDownloading ngrok: 29%\rDownloading ngrok: 30%\rDownloading ngrok: 31%\rDownloading ngrok: 32%\rDownloading ngrok: 33%\rDownloading ngrok: 34%\rDownloading ngrok: 35%\rDownloading ngrok: 36%\rDownloading ngrok: 37%\rDownloading ngrok: 38%\rDownloading ngrok: 39%\rDownloading ngrok: 40%\rDownloading ngrok: 41%\rDownloading ngrok: 42%\rDownloading ngrok: 43%\rDownloading ngrok: 44%\rDownloading ngrok: 45%\rDownloading ngrok: 46%\rDownloading ngrok: 47%\rDownloading ngrok: 48%\rDownloading ngrok: 49%\rDownloading ngrok: 50%\rDownloading ngrok: 51%\rDownloading ngrok: 52%\rDownloading ngrok: 53%\rDownloading ngrok: 54%\rDownloading ngrok: 55%\rDownloading ngrok: 56%\rDownloading ngrok: 57%\rDownloading ngrok: 58%\rDownloading ngrok: 59%\rDownloading ngrok: 60%\rDownloading ngrok: 61%\rDownloading ngrok: 62%\rDownloading ngrok: 63%\rDownloading ngrok: 64%\rDownloading ngrok: 65%\rDownloading ngrok: 66%\rDownloading ngrok: 67%\rDownloading ngrok: 68%\rDownloading ngrok: 69%\rDownloading ngrok: 70%\rDownloading ngrok: 71%\rDownloading ngrok: 72%\rDownloading ngrok: 73%\rDownloading ngrok: 74%\rDownloading ngrok: 75%\rDownloading ngrok: 76%\rDownloading ngrok: 77%\rDownloading ngrok: 78%\rDownloading ngrok: 79%\rDownloading ngrok: 80%\rDownloading ngrok: 81%\rDownloading ngrok: 82%\rDownloading ngrok: 83%\rDownloading ngrok: 84%\rDownloading ngrok: 85%\rDownloading ngrok: 86%\rDownloading ngrok: 87%\rDownloading ngrok: 88%\rDownloading ngrok: 89%\rDownloading ngrok: 90%\rDownloading ngrok: 91%\rDownloading ngrok: 92%\rDownloading ngrok: 93%\rDownloading ngrok: 94%\rDownloading ngrok: 95%\rDownloading ngrok: 96%\rDownloading ngrok: 97%\rDownloading ngrok: 98%\rDownloading ngrok: 99%\rDownloading ngrok: 100%\r                                                                                                    \rInstalling ngrok ... \r                                                                                                    \rAuthtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576328d2-143a-4b7c-aa51-53b4832ba570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-03T21:51:31+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec4f2d4-15fa-4b69-ce93-85aaf0d42d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://664a-35-221-57-162.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c997653-5f90-47bb-ae1b-3e4f46ca02f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f3c5ba1dae0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.driver.port', '34167'),\n",
              "  ('spark.app.id', 'local-1685827483849'),\n",
              "  ('spark.app.submitTime', '1685827482558'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.driver.host', 'f4ad0dc2ae14'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.app.startTime', '1685827482725'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "# slice_df = spark.read.schema(playlist_schema).json(LITTLE_SLICE_FILE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PEI_1vcSPxOG"
      },
      "outputs": [],
      "source": [
        "# slice_df.select(\"tracks\").first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gnNiOotq3p3f"
      },
      "outputs": [],
      "source": [
        "# slice_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item-Based Collaborative Filtering"
      ],
      "metadata": {
        "id": "p211b9samoI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Item-Based Collaboartive Filtering is the \"transpose\" approach to user-based CF. This time we won't consider the users' feature vectors, but the items'.\n",
        "An item's rating vector $\\mathbf{r}_i$ is the vector of ratings given to the item $i$ for all the users.\n",
        "\n",
        "Let $m$ be the number of users, $n$ the number of playlists, then $\\mathbf{r}_i \\in \\mathbb{R}^m$ and $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$.\n",
        "\n",
        "In order to make a prediction, we take the set of items $I_u$ rated by the user $u$, and we compute the set $I^k_u$ of the top-$k$ most similar items to $i$ rated by $u$, for each item $i \\in I_u$. Once done that, we average the $k$ rating vectors weighting them by their respective similarity."
      ],
      "metadata": {
        "id": "1pR8c5Ro5_DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slice_df = slice_df.cache()"
      ],
      "metadata": {
        "id": "8n4nUj_wSJeJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = True"
      ],
      "metadata": {
        "id": "wSg4hiaeewgY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PLAYLISTS = 100_000\n",
        "SONGS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-train-{NUM_PLAYLISTS}.json\") #TODO: Little bug this is songs_df, meaning it hasn't got any info, but we don't actually care.\n",
        "RATING_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-train-{NUM_PLAYLISTS}.txt\")\n",
        "with open(RATING_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  RATING_VECTOR_LENGTH = int(f.read()) + 1\n",
        "\n",
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH)\n",
        "songs_df = spark.read.json(SONGS_INFO_DF)\n",
        "\n",
        "playlist_map_schema = StructType([\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"embedding\", VectorUDT(), True)\n",
        "])\n",
        "PLAYLIST_MAP_PATH = os.path.join(SAVED_DFS_PATH, f\"playlist_map-{NUM_PLAYLISTS}.json\")\n",
        "playlist_map = spark.read.schema(playlist_map_schema).json(PLAYLIST_MAP_PATH)"
      ],
      "metadata": {
        "id": "dXUCLE9XSKsi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "train_df = spark.read.schema(playlist_schema).json(TRAIN_DF_PATH)\n",
        "test_df = spark.read.schema(playlist_schema).json(TEST_DF_PATH)"
      ],
      "metadata": {
        "id": "HnchIh93YdaU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(vector_1: SparseVector, vector_2: SparseVector) -> float:\n",
        "  \"\"\"\n",
        "  Computes the Jaccard Similarity between two sparse binary vectors\n",
        "  \"\"\"\n",
        "  # Convert SparseVectors to sets\n",
        "  set1 = set(vector_1.indices)\n",
        "  set2 = set(vector_2.indices)\n",
        "\n",
        "  # Calculate the intersection and union of the sets\n",
        "  intersection = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "\n",
        "  # Calculate the similarity\n",
        "  similarity = intersection / union\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "eaPyxrxMZClT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity between each couple is intractable, we can cluster the similar tracks in buckets using Locally Sensitive Hashing."
      ],
      "metadata": {
        "id": "Z2QVHgY4WRJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinHashLSH, MinHashLSHModel\n",
        "\n",
        "LSH_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"lsh_model-{NUM_PLAYLISTS}.pickle\")\n",
        "if os.path.exists(LSH_MODEL_PATH):\n",
        "  model = MinHashLSHModel.load(LSH_MODEL_PATH)\n",
        "else:\n",
        "  mh = MinHashLSH(inputCol=\"embedding\", outputCol=\"hashes\", numHashTables=5)\n",
        "  model = mh.fit(playlist_map)\n",
        "  model.save(LSH_MODEL_PATH)\n",
        "\n",
        "model.transform(playlist_map).show()"
      ],
      "metadata": {
        "id": "roOL0ckQgNKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the MinHasLSH model is not so fast, let's try with another type of LSH model"
      ],
      "metadata": {
        "id": "Co6K7luPdWoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.feature import BucketedRandomProjectionLSH, BucketedRandomProjectionLSHModel\n",
        "\n",
        "# BRP_LSH_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"brp_lsh_model-{NUM_PLAYLISTS}.pickle\")\n",
        "# if os.path.exists(BRP_LSH_MODEL_PATH):\n",
        "#   model = BucketedRandomProjectionLSHModel.load(BRP_LSH_MODEL_PATH)\n",
        "# else:\n",
        "#   brp = BucketedRandomProjectionLSH(inputCol=\"embedding\", outputCol=\"hashes\", numHashTables=5)\n",
        "#   model = brp.fit(playlist_map)\n",
        "#   model.save(BRP_LSH_MODEL_PATH)\n",
        "\n",
        "# model.transform(playlist_map).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv3f7k0vdbaO",
        "outputId": "12378bf9-db05-4b30-d5dc-71018d1e44ba"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|           track_uri|           embedding|              hashes|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|spotify:track:000...|(100001,[28824,60...|[[-1.0], [-1.0], ...|\n",
            "|spotify:track:000...|(100001,[60352],[...|[[-1.0], [0.0], [...|\n",
            "|spotify:track:000...|(100001,[76926],[...|[[0.0], [-1.0], [...|\n",
            "|spotify:track:000...|(100001,[79686],[...|[[-1.0], [-1.0], ...|\n",
            "|spotify:track:000...|(100001,[14092,21...|[[0.0], [0.0], [-...|\n",
            "|spotify:track:000...|(100001,[74372],[...|[[0.0], [0.0], [-...|\n",
            "|spotify:track:000...|(100001,[8758],[1...|[[0.0], [-1.0], [...|\n",
            "|spotify:track:000...|(100001,[70399],[...|[[0.0], [-1.0], [...|\n",
            "|spotify:track:001...|(100001,[9796,169...|[[0.0], [0.0], [-...|\n",
            "|spotify:track:001...|(100001,[3698,702...|[[0.0], [0.0], [-...|\n",
            "|spotify:track:001...|(100001,[92210],[...|[[0.0], [-1.0], [...|\n",
            "|spotify:track:001...|(100001,[7110,250...|[[0.0], [0.0], [-...|\n",
            "|spotify:track:001...|(100001,[80893],[...|[[0.0], [-1.0], [...|\n",
            "|spotify:track:001...|(100001,[50318,63...|[[0.0], [-1.0], [...|\n",
            "|spotify:track:001...|(100001,[2270,191...|[[-1.0], [-1.0], ...|\n",
            "|spotify:track:001...|(100001,[10656,31...|[[-1.0], [0.0], [...|\n",
            "|spotify:track:002...|(100001,[93642],[...|[[-1.0], [0.0], [...|\n",
            "|spotify:track:002...|(100001,[12674],[...|[[0.0], [0.0], [-...|\n",
            "|spotify:track:002...|(100001,[43238,87...|[[-1.0], [-1.0], ...|\n",
            "|spotify:track:002...|(100001,[1,5324,1...|[[0.0], [-1.0], [...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_aggregate(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Transforms the dataframe in order to be compatible for the union with an aggregate df.\n",
        "  \"\"\"\n",
        "  return df.withColumn(\"sum\", F.col(\"distCol\")).withColumnRenamed(\"distCol\", \"squared_sum\")\n",
        "\n",
        "def merge_aggregate_df(aggregate_df: DataFrame, other_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Sums an aggregate df (track_uri, embedding, hashes, squared_sum, sum)\n",
        "  with a simple df (track_uri, embedding, hashes, distCol).\n",
        "  \"\"\"\n",
        "  other_df = transform_to_aggregate(other_df)\n",
        "  merged_df = aggregate_df.unionAll(other_df)\n",
        "  return merged_df.groupBy(\"track_uri\", \"embedding\", \"hashes\").agg(\n",
        "    F.sum(F.pow(\"squared_sum\", 2)).alias(\"squared_sum\"),\n",
        "    F.sum(\"sum\").alias(\"sum\"))\n",
        "\n",
        "if DEBUG:\n",
        "  key = playlist_map.first()[1]\n",
        "  neigh = model.approxNearestNeighbors(playlist_map, key, 10)\n",
        "  merged_df_2 = merge_aggregate_df(transform_to_aggregate(neigh), neigh)\n",
        "  merged_df_2.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPFF6Xwj8wed",
        "outputId": "dbb6ae23-c1a2-4be3-bc08-dfc32a943bc4"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+-----------+---+\n",
            "|           track_uri|           embedding|              hashes|squared_sum|sum|\n",
            "+--------------------+--------------------+--------------------+-----------+---+\n",
            "|spotify:track:5YG...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:0aT...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:657...|(100001,[60767],[...|[[-1.0], [-1.0], ...|        2.0|2.0|\n",
            "|spotify:track:7LU...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:6Jd...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:3eJ...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:7rU...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:2uU...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:0lm...|(100001,[28824],[...|[[0.0], [0.0], [-...|        2.0|2.0|\n",
            "|spotify:track:000...|(100001,[28824,60...|[[-1.0], [-1.0], ...|        0.0|0.0|\n",
            "+--------------------+--------------------+--------------------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def df_to_dict(df: DataFrame) -> DataFrame:\n",
        "  return df.select(\"track_uri\", \"distCol\").rdd.collectAsMap()\n",
        "\n",
        "def merge_dicts(d1: Dict[str, float], d2: Dict[str, float]) -> Dict[str, float]:\n",
        "  for key, value in d2.items():\n",
        "    if key in d1:\n",
        "      if type(d1[key]) is float:\n",
        "        d1[key] = [d1[key]]\n",
        "      d1[key] += [value]\n",
        "    else:\n",
        "      d1[key] = [value]\n",
        "  return d1\n",
        "\n",
        "if DEBUG:\n",
        "  key = playlist_map.first()[1]\n",
        "  neigh = F.broadcast(model.approxNearestNeighbors(playlist_map, key, 10)).cache()\n",
        "  merged_df_2 = merge_dicts(df_to_dict(neigh), df_to_dict(neigh))\n",
        "  merged_df_2"
      ],
      "metadata": {
        "id": "XmUp55bUg8f9"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from functools import reduce\n",
        "\n",
        "#TODO: see if the dictionary approach is right\n",
        "def extract_similar_songs(playlist_tracks: DataFrame, playlist_map, model, k=10) -> DataFrame:\n",
        "  aggregate_df = None\n",
        "  tracks_embedding = F.broadcast(playlist_map.join(F.broadcast(playlist_tracks), \"track_uri\").select(\"track_uri\", \"embedding\"))\n",
        "  transformed_tracks_embeddings = model.transform(tracks_embedding).cache()\n",
        "  k_neighs = []\n",
        "\n",
        "  for row in tqdm(tracks_embedding.collect(), desc='Extracting k-neighbors'):\n",
        "    k_neigh = F.broadcast(model.approxNearestNeighbors(transformed_tracks_embeddings, row[\"embedding\"], k)).cache()\n",
        "    k_neighs.append(df_to_dict(k_neigh))\n",
        "\n",
        "  aggregate_df = reduce(merge_dicts, k_neighs)\n",
        "  return aggregate_df\n",
        "\n",
        "if DEBUG:\n",
        "  # first_playlist = train_df.limit(1).select(F.explode(\"tracks\")).select(\"col.*\").distinct()\n",
        "  first_playlist = train_df.filter(\"pid == 1005\").select(F.explode(\"tracks\")).select(\"col.*\").distinct()\n",
        "  recommendations = extract_similar_songs(first_playlist, playlist_map, model, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6sFCM6RG5Y1",
        "outputId": "1febfc20-bb6f-4afe-963c-9638f31210c1"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting k-neighbors: 100%|██████████| 21/21 [00:08<00:00,  2.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are safe by using python dictionaries since the data will be very small, and so dictionaries will be faster than pyspark dataframes."
      ],
      "metadata": {
        "id": "L-zF2NSClWaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\"The reccomendation dictionary is {sys.getsizeof(recommendations)} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRs9DULWlMvl",
        "outputId": "28ef528c-1645-4941-f4d3-26900f13afd2"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reccomendation dictionary is 4696 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def aggregate_recommendations(recommendations: Dict[str, float | List[float]]) -> Dict[str, float]:\n",
        "  aggregated = {}\n",
        "  for key, value in recommendations.items():\n",
        "    if type(value) is list:\n",
        "      if sum(value) == 0:\n",
        "        continue\n",
        "      aggregated[key] = sum(x for x in value) / len(value)\n",
        "    else:\n",
        "      aggregated[key] = value\n",
        "  return aggregated\n",
        "\n",
        "k = 40\n",
        "aggregated_recs = aggregate_recommendations(recommendations)\n",
        "top_k = sorted(aggregated_recs.items(), key=lambda x: -x[1])[:k]\n",
        "top_k "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbBeYNhKqJgM",
        "outputId": "7bf2e5fa-be34-417b-8e28-f938a395249b"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spotify:track:1uL78kFWxhz3umTMWJu8n5', 0.8208208096471215),\n",
              " ('spotify:track:53mrVsi49rLHIaKBiSvElG', 0.8168825223744853),\n",
              " ('spotify:track:5Z3GHaZ6ec9bsiI5BenrbY', 0.8132319173145136),\n",
              " ('spotify:track:75ZvA4QfFiZvzhj2xkaWAh', 0.8070864863059125),\n",
              " ('spotify:track:2eAZfqOm4EnOF9VvN50Tyc', 0.7995183010757708),\n",
              " ('spotify:track:4knL4iPxPOZjQzTUlELGSY', 0.7946157313966445),\n",
              " ('spotify:track:6EpRaXYhGOB3fj4V2uDkMJ', 0.7732659354967721),\n",
              " ('spotify:track:5tz69p7tJuGPeMGwNTxYuV', 0.7667465890313958),\n",
              " ('spotify:track:0Fv5N0cHBsl4bzCbollCAS', 0.7109277414486239),\n",
              " ('spotify:track:0Qh38w01QRXK6KHIv0e3hb', 0.7036263013869308),\n",
              " ('spotify:track:0bXFIF7iL17TYLyx8JHziM', 0.6843219668511102),\n",
              " ('spotify:track:7F9vK8hNFMml4GtHsaXui6', 0.6755009190076482),\n",
              " ('spotify:track:7bsnTsbiwOymZWjPF9v6Di', 0.6243419279133565),\n",
              " ('spotify:track:0NiXXAI876aGImAd6rTj8w', 0.6206458806073614),\n",
              " ('spotify:track:7iDa6hUg2VgEL1o1HjmfBn', 0.4825406381697772),\n",
              " ('spotify:track:0XLOf9LhyazPX9Ld8jPiUq', 0.46811070998796633),\n",
              " ('spotify:track:6Pw3Gvw4wNMZfTYZa6IQal', 0.46811070998796633),\n",
              " ('spotify:track:0CokSRCu5hZgPxcZBaEzVE', 0.43333333333333335)]"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a prediction, we can put the result in a pyspark dataframe in order to be used later"
      ],
      "metadata": {
        "id": "vpbn30aDl0h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations_df = spark.createDataFrame(data=top_k, schema=[\"track_uri\", \"similarity\"])\n",
        "recommendations_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kukcVZ8hl7qN",
        "outputId": "a7371d85-745c-4460-9ddb-02ce89aee9d6"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+-------------------+\n",
            "|track_uri                           |similarity         |\n",
            "+------------------------------------+-------------------+\n",
            "|spotify:track:1uL78kFWxhz3umTMWJu8n5|0.8208208096471215 |\n",
            "|spotify:track:53mrVsi49rLHIaKBiSvElG|0.8168825223744853 |\n",
            "|spotify:track:5Z3GHaZ6ec9bsiI5BenrbY|0.8132319173145136 |\n",
            "|spotify:track:75ZvA4QfFiZvzhj2xkaWAh|0.8070864863059125 |\n",
            "|spotify:track:2eAZfqOm4EnOF9VvN50Tyc|0.7995183010757708 |\n",
            "|spotify:track:4knL4iPxPOZjQzTUlELGSY|0.7946157313966445 |\n",
            "|spotify:track:6EpRaXYhGOB3fj4V2uDkMJ|0.7732659354967721 |\n",
            "|spotify:track:5tz69p7tJuGPeMGwNTxYuV|0.7667465890313958 |\n",
            "|spotify:track:0Fv5N0cHBsl4bzCbollCAS|0.7109277414486239 |\n",
            "|spotify:track:0Qh38w01QRXK6KHIv0e3hb|0.7036263013869308 |\n",
            "|spotify:track:0bXFIF7iL17TYLyx8JHziM|0.6843219668511102 |\n",
            "|spotify:track:7F9vK8hNFMml4GtHsaXui6|0.6755009190076482 |\n",
            "|spotify:track:7bsnTsbiwOymZWjPF9v6Di|0.6243419279133565 |\n",
            "|spotify:track:0NiXXAI876aGImAd6rTj8w|0.6206458806073614 |\n",
            "|spotify:track:7iDa6hUg2VgEL1o1HjmfBn|0.4825406381697772 |\n",
            "|spotify:track:0XLOf9LhyazPX9Ld8jPiUq|0.46811070998796633|\n",
            "|spotify:track:6Pw3Gvw4wNMZfTYZa6IQal|0.46811070998796633|\n",
            "|spotify:track:0CokSRCu5hZgPxcZBaEzVE|0.43333333333333335|\n",
            "+------------------------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all togheter:"
      ],
      "metadata": {
        "id": "fVBZMo5ym4wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def item_based_recommendation(playlist: DataFrame,playlist_map: DataFrame, model: MinHashLSHModel, k=50):\n",
        "  playlist_songs = playlist.select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "  recommendations = extract_similar_songs(playlist_songs, playlist_map, model, 10)\n",
        "  aggregated_recommendations = aggregate_recommendations(recommendations)\n",
        "  top_k = sorted(aggregated_recommendations.items(), key=lambda x: -x[1])[:k]\n",
        "  recommendations_df = spark.createDataFrame(data=top_k, schema=[\"track_uri\", \"similarity\"])\n",
        "  return recommendations_df\n",
        "\n",
        "if DEBUG:\n",
        "  playlist = train_df.filter(\"pid == 2005\")\n",
        "  result = item_based_recommendation(playlist, playlist_map, model)\n",
        "  result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnHTtod7m7jB",
        "outputId": "4e1fdcfd-8dc0-49b8-9878-5047f5aef5c8"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting k-neighbors: 100%|██████████| 34/34 [00:10<00:00,  3.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+\n",
            "|           track_uri|        similarity|\n",
            "+--------------------+------------------+\n",
            "|spotify:track:1al...|0.8633717628185147|\n",
            "|spotify:track:4zz...|0.8122753639146489|\n",
            "|spotify:track:1wM...|0.8081843971271243|\n",
            "|spotify:track:3Gp...| 0.796220408044011|\n",
            "|spotify:track:3lB...|0.7923481512315753|\n",
            "|spotify:track:7lG...|0.7813832653677002|\n",
            "|spotify:track:7vR...|0.7724312512781818|\n",
            "|spotify:track:7IW...|0.7538107392852691|\n",
            "|spotify:track:39a...|0.7518138665443204|\n",
            "|spotify:track:0fi...|0.7437276588861723|\n",
            "|spotify:track:27v...| 0.738742752960684|\n",
            "|spotify:track:6mj...|0.7263680068200388|\n",
            "|spotify:track:0zM...|0.7133843830389912|\n",
            "|spotify:track:1fn...|0.7133823834089543|\n",
            "|spotify:track:0QL...| 0.712460732490682|\n",
            "|spotify:track:1Np...|0.7052846392233619|\n",
            "|spotify:track:11b...|0.6620996960181076|\n",
            "|spotify:track:6NO...|0.6563514190493702|\n",
            "|spotify:track:1BP...|0.6453017871192537|\n",
            "|spotify:track:7Gg...|0.6162480252764613|\n",
            "+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Evaluation"
      ],
      "metadata": {
        "id": "92e4Ipd-m2TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(recommendations, ground_truth, num_of_recommendations) -> float:\n",
        "    \"\"\"\n",
        "    Calculates precision at k for the recommendations.\n",
        "    \"\"\"\n",
        "    recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").cache()\n",
        "    reccomended_relevant_tracks_count = recommended_relevant_tracks.count() #this can be top_n_results.join in order to be more performant\n",
        "    recommended_relevant_tracks.unpersist()\n",
        "    precision = reccomended_relevant_tracks_count / float(num_of_recommendations)\n",
        "\n",
        "    return precision\n",
        "\n",
        "\n",
        "import math\n",
        "#TODO: make it more efficient somehow\n",
        "def normalized_discounted_cumulative_gain(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "  recommendations = recommendations.orderBy(F.col(\"similarity\").desc())\n",
        "  recommendations_list = recommendations.collect()\n",
        "  cumulative_gain = 0\n",
        "\n",
        "  intersection = recommendations.join(ground_truth, \"track_uri\").count()\n",
        "  if intersection == 0: return 0\n",
        "\n",
        "  ideal_cumulative_gain = 1 + sum((1 / math.log(i, 2)) for i in range(2, 2+intersection))\n",
        "  for index, row in enumerate(recommendations_list):\n",
        "    i = index + 1\n",
        "    is_rel = ground_truth.filter(F.col(\"track_uri\").isin(row.track_uri)).count() > 0\n",
        "    rel = 1 if is_rel else 0\n",
        "    if i == 1:\n",
        "      cumulative_gain += rel\n",
        "    else:\n",
        "      cumulative_gain += (rel / math.log(i, 2))\n",
        "  return cumulative_gain / ideal_cumulative_gain"
      ],
      "metadata": {
        "id": "R3bMn2m6nGdC"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing performance evaluation on a random sample of the test set"
      ],
      "metadata": {
        "id": "c8SUpL2Boghc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(pid: int) -> Tuple[DataFrame, float]:\n",
        "    t1 = time.time()\n",
        "\n",
        "    playlist_train = train_df.filter(f\"pid == {pid}\").cache()\n",
        "    playlist_test = test_df.filter(f\"pid == {pid}\").cache()\n",
        "    ground_truth = playlist_test.select(F.explode(\"tracks\")).select(\"col.*\").cache()\n",
        "    num_of_recommendations = ground_truth.count()\n",
        "\n",
        "    recommendations = item_based_recommendation(playlist_train, playlist_map, model, k=num_of_recommendations)\n",
        "\n",
        "    precision = precision_at_k(recommendations, ground_truth, num_of_recommendations)\n",
        "    gain = normalized_discounted_cumulative_gain(recommendations, ground_truth, num_of_recommendations)\n",
        "\n",
        "    t2 = time.time()\n",
        "    print(f\"Total time: {t2-t1}\")\n",
        "\n",
        "    playlist_train.unpersist()\n",
        "    playlist_test.unpersist()\n",
        "    ground_truth.unpersist()\n",
        "\n",
        "    return playlist_train, playlist_test, ground_truth, recommendations, precision, gain\n",
        "\n",
        "if DEBUG:\n",
        "  train, test, gt, rec, prec, gain  = evaluate(1003)\n",
        "  train.show(), test.show(), gt.show(), rec.show(truncate=False)\n",
        "  print(f\"Precision: {prec}, Gain: {gain}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8ZIqjdSogHJ",
        "outputId": "366338ea-f8f8-4238-eac1-8fc1b6156484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting k-neighbors:  52%|█████▏    | 87/168 [00:10<00:18,  4.45it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "EVALUATION_RESULTS_PATH = os.path.join(GDRIVE_DATA_DIR, \"item_based_evaluation_results\")\n",
        "def perform_evaluation():\n",
        "  SAMPLING_FRACTION = 0.01\n",
        "  sampled_playlists = train_df.sample(False, SAMPLING_FRACTION, seed=42).cache()\n",
        "  results = []\n",
        "  for row in tqdm(sampled_playlists.collect(), desc=\"Performing evaluation\"):\n",
        "      pid = row['pid']\n",
        "      train, test, gt, rec, prec, gain = evaluate(pid)\n",
        "      results.append((prec, gain))\n",
        "  with open(EVALUATION_RESULTS_PATH, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "  return results\n",
        "\n",
        "results = perform_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "DokoAdIipxHi",
        "outputId": "0d50bedc-87c0-421a-bc5f-9a1f6e6012c7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-0314eaebd413>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-130-0314eaebd413>\u001b[0m in \u001b[0;36mperform_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msampled_playlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLING_FRACTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_playlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Performing evaluation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All against all tries"
      ],
      "metadata": {
        "id": "x9gbMKT_lw9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since computing the k nearest neighbors is super slow, I can pre-compute them offline and store them. This will require like 100 years lol."
      ],
      "metadata": {
        "id": "GI8ZtkTmv2qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mh = MinHashLSH(inputCol=\"embedding\", outputCol=\"hashes\", numHashTables=5)\n",
        "model = mh.fit(playlist_map)"
      ],
      "metadata": {
        "id": "uzj7kXRNi6ex"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ZldxXdXlylr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_k_neighbors(playlist_map: DataFrame, model) -> DataFrame:\n",
        "    result = []\n",
        "    transformed_playlist_map = model.transform(playlist_map).cache()\n",
        "    for index, row in enumerate(tqdm(playlist_map.collect(), desc=\"Computing k-neighbors\")):\n",
        "        k_neighs = model.approxNearestNeighbors(transformed_playlist_map, row.embedding, 10).select(\"track_uri\", F.col(\"distCol\").alias(\"similarity\"))\n",
        "        result.append((row.track_uri, k_neighs.collect()))\n",
        "\n",
        "    k_neighs_schema = StructType([\n",
        "        StructField(\"track_uri\", StringType(), nullable=True),\n",
        "        StructField(\"distCol\", FloatType(), nullable=True)\n",
        "    ])\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"track_uri\", StringType(), nullable=True),\n",
        "        StructField(\"k_neighs\", ArrayType(k_neighs_schema), nullable=True)\n",
        "    ])\n",
        "\n",
        "    result_df = spark.createDataFrame(result, schema)\n",
        "    transformed_playlist_map.unpersist()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "result_df = compute_all_k_neighbors(playlist_map, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "tqV8qF-iv-RA",
        "outputId": "f9199d76-7e5d-4741-8f3d-a58be15cb301"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing k-neighbors:   0%|          | 176/681805 [01:27<71:46:32,  2.64it/s]ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n",
            "Computing k-neighbors:   0%|          | 176/681805 [01:27<94:38:14,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-091fe8d14d80>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_all_k_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylist_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-108-091fe8d14d80>\u001b[0m in \u001b[0;36mcompute_all_k_neighbors\u001b[0;34m(playlist_map, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtransformed_playlist_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylist_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylist_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Computing k-neighbors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mk_neighs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_playlist_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"track_uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distCol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_neighs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/feature.py\u001b[0m in \u001b[0;36mapproxNearestNeighbors\u001b[0;34m(self, dataset, key, numNearestNeighbors, distCol)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0madded\u001b[0m \u001b[0mto\u001b[0m \u001b[0mshow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0meach\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"approxNearestNeighbors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumNearestNeighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     def approxSimilarityJoin(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K_NEIGHBOURS_PATH = os.path.join(GDRIVE_DATA_DIR,\"saved_models\", f\"k_neighbours-{NUM_PLAYLISTS}.parquet\")\n",
        "result_df.write.parquet(K_NEIGHBOURS_PATH)"
      ],
      "metadata": {
        "id": "VVFic9gIM7L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old stuff"
      ],
      "metadata": {
        "id": "aoVjgtwlI9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_aggregate(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Transforms the dataframe in order to be compatible for the union with an aggregate df.\n",
        "  \"\"\"\n",
        "  return df.withColumn(\"sum\", F.col(\"similarity\")).withColumnRenamed(\"similarity\", \"squared_sum\")\n",
        "\n",
        "def merge_aggregate_df(aggregate_df: DataFrame, other_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Sums an aggregate df (track_uri, embedding, hashes, squared_sum, sum)\n",
        "  with a simple df (track_uri, embedding, hashes, distCol).\n",
        "  \"\"\"\n",
        "  other_df = transform_to_aggregate(other_df)\n",
        "  merged_df = aggregate_df.unionAll(other_df)\n",
        "  return merged_df.groupBy(\"track_uri\", \"embedding\").agg(\n",
        "    F.sum(F.pow(\"squared_sum\", 2)).alias(\"squared_sum\"),\n",
        "    F.sum(\"sum\").alias(\"sum\"))"
      ],
      "metadata": {
        "id": "6VCjq5dxQvr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_similarity_df(input_vector: SparseVector, playlist_map: DataFrame, similarity_function: Callable) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Returns the similarity df for a single song\n",
        "  \"\"\"\n",
        "  \n",
        "  @F.udf(returnType=FloatType())\n",
        "  def compute_similarity(vector1):\n",
        "    return similarity_function(vector1, input_vector)\n",
        "\n",
        "  result_df = playlist_map.withColumn(\"similarity\", compute_similarity(playlist_map.embedding))\n",
        "  \n",
        "  return result_df\n",
        "\n",
        "def get_top_k_results(track_uri: str, similarity_df: DataFrame, k: int = 20) -> DataFrame:\n",
        "  return similarity_df.filter((F.col(\"similarity\") > 0) & (F.col(\"track_uri\") != track_uri)).orderBy(F.col(\"similarity\").desc()).limit(k)\n",
        "\n",
        "if DEBUG:\n",
        "  first_song_vector = playlist_map.select(\"embedding\").limit(1).first()[0]\n",
        "  first_track_uri = playlist_map.select(\"track_uri\").limit(1).first()[0]\n",
        "  similarity_df = create_similarity_df(first_song_vector, playlist_map, jaccard_similarity)\n",
        "  top_k_results = get_top_k_results(first_track_uri, similarity_df).cache()\n",
        "  top_k_results.show()"
      ],
      "metadata": {
        "id": "QmBLSpdtcylv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_similar_songs(playlist_tracks: DataFrame, k=10) -> DataFrame:\n",
        "  aggregate_df = None\n",
        "  tracks_embedding = playlist_tracks.join(playlist_map, \"track_uri\")\n",
        "  for row in tqdm(tracks_embedding.collect(), desc='Extracting k-neighbors'):\n",
        "    k_neigh = get_top_k_results(\n",
        "        track_uri=row.track_uri,\n",
        "        similarity_df= create_similarity_df(input_vector=row.embedding,\n",
        "                                            playlist_map=playlist_map,\n",
        "                                            similarity_function=jaccard_similarity),\n",
        "        k = 10)\n",
        "    \n",
        "    k_neigh.show()\n",
        "    if aggregate_df == None:\n",
        "      aggregate_df = transform_to_aggregate(k_neigh).cache()\n",
        "    else:\n",
        "      aggregate_df = merge_aggregate_df(aggregate_df, k_neigh).cache()\n",
        "    \n",
        "    aggregate_df.show()\n",
        "  return aggregate_df\n",
        "\n",
        "if DEBUG:\n",
        "  first_playlist = train_df.limit(1).select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "  recommendations = extract_similar_songs(first_playlist, model)"
      ],
      "metadata": {
        "id": "yydgn5XD3SZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations.show()"
      ],
      "metadata": {
        "id": "8scK_YReSrYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_similar_songs(playlist: DataFrame, playlist_map: DataFrame) -> Dict[str, float]:\n",
        "#   \"\"\"\n",
        "#   For each track in the playlist, it extracts the top-k most similar results.\n",
        "#   \"\"\"\n",
        "#   weighted_dict = {}\n",
        "  \n",
        "#   # F.udf(returnType=IntegerType())\n",
        "#   def update_weighted_dict(track_uri: str, similarity: float):\n",
        "#     nonlocal weighted_dict\n",
        "#     if track_uri not in weighted_dict:\n",
        "#       weighted_dict[track_uri] = (similarity, 1)\n",
        "#     else:\n",
        "#       curr_similarity = weighted_dict[track_uri][0]\n",
        "#       count = weighted_dict[track_uri][1]\n",
        "#       weighted_dict[track_uri] = (curr_similarity + similarity, count+1)\n",
        "  \n",
        "#   tracks = playlist.select(F.explode(\"tracks.track_uri\")).withColumnRenamed(\"col\", \"track_uri\")\n",
        "#   tracks_mapped = tracks.join(playlist_map, \"track_uri\")\n",
        "\n",
        "#   for track_row in tqdm(tracks_mapped.collect(), desc=f\"Analyzing tracks\"):\n",
        "#     similarity_df = create_similarity_df(track_row.embedding, playlist_map, jaccard_similarity)\n",
        "#     top_k_results = get_top_k_results(first_track_uri, similarity_df).cache()\n",
        "#     top_k_results.foreach(lambda row: update_weighted_dict(row.track_uri, row.similarity))\n",
        "#     top_k_results.unpersist()\n",
        "  \n",
        "#   return weighted_dict\n",
        "\n",
        "# if DEBUG:\n",
        "#   first_playlist = train_df.limit(1).cache()\n",
        "#   weighted_dict = extract_similar_songs(first_playlist, playlist_map)"
      ],
      "metadata": {
        "id": "K9UmYW-9eGuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_similarity_df(playlist: DataFrame, playlist_map: DataFrame, similarityFunction: Callable):\n",
        "  # Get the current playlist's tracks\n",
        "  playlist_tracks = playlist.select(F.explode(\"tracks.track_uri\")).withColumnRenamed(\"col\", \"track_uri\")\n",
        "  playlist_tracks.show()\n",
        "  # Let's extract the rating vector for each single track\n",
        "  playlist_tracks_mapped = playlist_tracks.join(playlist_map, \"track_uri\")\n",
        "  playlist_tracks_mapped.show()\n",
        "\n",
        "\n",
        "  @F.udf(returnType=FloatType())\n",
        "  def compute_similarity(input_vector: SparseVector, other_vector: SparseVector):\n",
        "    return jaccard_similarity(input_vector, other_vector)\n",
        "\n",
        "  # for row in playlist_tracks_mapped.collect():\n",
        "  similarity_df = playlist_map.withColumn(\"similarity\", compute_similarity(playlist_tracks_mapped.limit(2).embedding, playlist_map.embedding))\n",
        "  # break\n",
        "  \n",
        "  similarity_df.show()\n",
        "  return\n",
        "\n",
        "first_playlsit = train_df.limit(1).cache()\n",
        "create_similarity_df(first_playlsit, playlist_map, jaccard_similarity)"
      ],
      "metadata": {
        "id": "hs0TGZxlWnLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_playlsit.show()"
      ],
      "metadata": {
        "id": "WbCUU0GzYswY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}