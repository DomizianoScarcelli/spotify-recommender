{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VJeY9PpvaHUJ",
        "x9gbMKT_lw9S",
        "aoVjgtwlI9qe"
      ],
      "authorship_tag": "ABX9TyMiteDrCQkKJToX+60hHTjp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21c7a4b5bb3743faac0b6f5d7170cba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47c7996ddfdc45d7b54bf1bda0975a0e",
              "IPY_MODEL_ccad9bcab8284c8487d5b3471734e72f",
              "IPY_MODEL_8223b461758d461095161936067b9e32"
            ],
            "layout": "IPY_MODEL_4464bff8c0c7445592c5d106a0dcbd7e"
          }
        },
        "47c7996ddfdc45d7b54bf1bda0975a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a2f535c08a94209a5e146c09f30206f",
            "placeholder": "​",
            "style": "IPY_MODEL_ee7aaf9bbfec4a91b209a2e3dab25188",
            "value": "Performing evaluation:   1%"
          }
        },
        "ccad9bcab8284c8487d5b3471734e72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce5bbe3e978845f3b7a569aa2d9c4488",
            "max": 1024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03d9318ed7ec415b895e5f857c328430",
            "value": 7
          }
        },
        "8223b461758d461095161936067b9e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0bec19319f340a08f0eee7d179d38fe",
            "placeholder": "​",
            "style": "IPY_MODEL_d9645488340949d7b39b2bdc2dd97d13",
            "value": " 7/1024 [03:02&lt;5:49:36, 20.63s/it]"
          }
        },
        "4464bff8c0c7445592c5d106a0dcbd7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a2f535c08a94209a5e146c09f30206f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee7aaf9bbfec4a91b209a2e3dab25188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce5bbe3e978845f3b7a569aa2d9c4488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03d9318ed7ec415b895e5f857c328430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0bec19319f340a08f0eee7d179d38fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9645488340949d7b39b2bdc2dd97d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/item-based-cf/item_based_CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N3wVgRfHYoOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415ccd46-b65f-458a-e8ae-92b374e4bdfd",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122542 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark -qq\n",
        "!pip install -U -q PyDrive -qq\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGHPv9OqY9MI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from typing import Tuple, Callable, Dict\n",
        "from functools import reduce\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316fff1b-bc4b-4d0d-92f1-811181e1b2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7e72fc-9d60-4d8e-cdb5-d307175b2e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=f6e5c42d0f4c4277e327b9407b8e51cb84c979ea2df2a5625fa2c409b196543a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59e3f44-7266-4339-8cd4-f17dd2c06bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ece167b-15e6-49cf-84d9-039d4a8a8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-04T00:12:25+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f881e41-35da-419e-b44a-180a8fc661b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://8128-35-221-57-162.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff1ed8f-3361-48d4-d607-1962ed72eebc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f2fabec2650>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.app.startTime', '1685837530019'),\n",
              "  ('spark.driver.host', 'ed35b75456fe'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.port', '33237'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.app.submitTime', '1685837529795'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.app.id', 'local-1685837531233'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "# slice_df = spark.read.schema(playlist_schema).json(LITTLE_SLICE_FILE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PEI_1vcSPxOG"
      },
      "outputs": [],
      "source": [
        "# slice_df.select(\"tracks\").first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gnNiOotq3p3f"
      },
      "outputs": [],
      "source": [
        "# slice_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Item-Based Collaborative Filtering"
      ],
      "metadata": {
        "id": "p211b9samoI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Item-Based Collaboartive Filtering is the \"transpose\" approach to user-based CF. This time we won't consider the users' feature vectors, but the items'.\n",
        "An item's rating vector $\\mathbf{r}_i$ is the vector of ratings given to the item $i$ for all the users.\n",
        "\n",
        "Let $m$ be the number of users, $n$ the number of playlists, then $\\mathbf{r}_i \\in \\mathbb{R}^m$ and $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$.\n",
        "\n",
        "In order to make a prediction, we take the set of items $I_u$ rated by the user $u$, and we compute the set $I^k_u$ of the top-$k$ most similar items to $i$ rated by $u$, for each item $i \\in I_u$. Once done that, we average the $k$ rating vectors weighting them by their respective similarity."
      ],
      "metadata": {
        "id": "1pR8c5Ro5_DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slice_df = slice_df.cache()"
      ],
      "metadata": {
        "id": "8n4nUj_wSJeJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = False"
      ],
      "metadata": {
        "id": "wSg4hiaeewgY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PLAYLISTS = 100_000\n",
        "SONGS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-train-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-train-{NUM_PLAYLISTS}.json\") #TODO: Little bug this is songs_df, meaning it hasn't got any info, but we don't actually care.\n",
        "RATING_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-train-{NUM_PLAYLISTS}.txt\")\n",
        "with open(RATING_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  RATING_VECTOR_LENGTH = int(f.read()) + 1\n",
        "\n",
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH)\n",
        "songs_df = spark.read.json(SONGS_INFO_DF)\n",
        "\n",
        "playlist_map_schema = StructType([\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"embedding\", VectorUDT(), True)\n",
        "])\n",
        "PLAYLIST_MAP_PATH = os.path.join(SAVED_DFS_PATH, f\"playlist_map-{NUM_PLAYLISTS}.json\")\n",
        "playlist_map = spark.read.schema(playlist_map_schema).json(PLAYLIST_MAP_PATH)"
      ],
      "metadata": {
        "id": "dXUCLE9XSKsi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "\n",
        "train_df = spark.read.schema(playlist_schema).json(TRAIN_DF_PATH)\n",
        "test_df = spark.read.schema(playlist_schema).json(TEST_DF_PATH)"
      ],
      "metadata": {
        "id": "HnchIh93YdaU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(vector_1: SparseVector, vector_2: SparseVector) -> float:\n",
        "  \"\"\"\n",
        "  Computes the Jaccard Similarity between two sparse binary vectors\n",
        "  \"\"\"\n",
        "  # Convert SparseVectors to sets\n",
        "  set1 = set(vector_1.indices)\n",
        "  set2 = set(vector_2.indices)\n",
        "\n",
        "  # Calculate the intersection and union of the sets\n",
        "  intersection = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "\n",
        "  # Calculate the similarity\n",
        "  similarity = intersection / union\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "eaPyxrxMZClT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity between each couple is intractable, we can cluster the similar tracks in buckets using Locally Sensitive Hashing."
      ],
      "metadata": {
        "id": "Z2QVHgY4WRJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinHashLSH, MinHashLSHModel\n",
        "\n",
        "LSH_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"lsh_model-{NUM_PLAYLISTS}.pickle\")\n",
        "if os.path.exists(LSH_MODEL_PATH):\n",
        "  model = MinHashLSHModel.load(LSH_MODEL_PATH)\n",
        "else:\n",
        "  mh = MinHashLSH(inputCol=\"embedding\", outputCol=\"hashes\", numHashTables=5)\n",
        "  model = mh.fit(playlist_map)\n",
        "  model.save(LSH_MODEL_PATH)\n",
        "\n",
        "model.transform(playlist_map).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roOL0ckQgNKP",
        "outputId": "0bf72506-29eb-4ce3-ba6c-0897fb2633d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|           track_uri|           embedding|              hashes|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|spotify:track:000...|(100001,[28824,60...|[[5.82911703E8], ...|\n",
            "|spotify:track:000...|(100001,[60352],[...|[[9.89451973E8], ...|\n",
            "|spotify:track:000...|(100001,[76926],[...|[[1.770007514E9],...|\n",
            "|spotify:track:000...|(100001,[79686],[...|[[2.94025684E8], ...|\n",
            "|spotify:track:000...|(100001,[14092,21...|[[4.72716012E8], ...|\n",
            "|spotify:track:000...|(100001,[74372],[...|[[1.791877645E9],...|\n",
            "|spotify:track:000...|(100001,[8758],[1...|[[8.00841359E8], ...|\n",
            "|spotify:track:000...|(100001,[70399],[...|[[6.00979172E8], ...|\n",
            "|spotify:track:001...|(100001,[9796,169...|[[1.60811649E8], ...|\n",
            "|spotify:track:001...|(100001,[3698,702...|[[1.3893202E7], [...|\n",
            "|spotify:track:001...|(100001,[92210],[...|[[8.92207576E8], ...|\n",
            "|spotify:track:001...|(100001,[7110,250...|[[2.05286493E8], ...|\n",
            "|spotify:track:001...|(100001,[80893],[...|[[2.033689217E9],...|\n",
            "|spotify:track:001...|(100001,[50318,63...|[[1.969888237E9],...|\n",
            "|spotify:track:001...|(100001,[2270,191...|[[4.63429503E8], ...|\n",
            "|spotify:track:001...|(100001,[10656,31...|[[1.45159947E8], ...|\n",
            "|spotify:track:002...|(100001,[93642],[...|[[7.1715461E8], [...|\n",
            "|spotify:track:002...|(100001,[12674],[...|[[1.335479489E9],...|\n",
            "|spotify:track:002...|(100001,[43238,87...|[[2.8269108E8], [...|\n",
            "|spotify:track:002...|(100001,[1,5324,1...|[[4.81772726E8], ...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the MinHasLSH model is not so fast, let's try with another type of LSH model"
      ],
      "metadata": {
        "id": "Co6K7luPdWoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.feature import BucketedRandomProjectionLSH, BucketedRandomProjectionLSHModel\n",
        "\n",
        "# BRP_LSH_MODEL_PATH = os.path.join(SAVED_DFS_PATH, f\"brp_lsh_model-{NUM_PLAYLISTS}.pickle\")\n",
        "# if os.path.exists(BRP_LSH_MODEL_PATH):\n",
        "#   model = BucketedRandomProjectionLSHModel.load(BRP_LSH_MODEL_PATH)\n",
        "# else:\n",
        "#   brp = BucketedRandomProjectionLSH(inputCol=\"embedding\", outputCol=\"hashes\", numHashTables=5)\n",
        "#   model = brp.fit(playlist_map)\n",
        "#   model.save(BRP_LSH_MODEL_PATH)\n",
        "\n",
        "# model.transform(playlist_map).show()"
      ],
      "metadata": {
        "id": "gv3f7k0vdbaO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_aggregate(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Transforms the dataframe in order to be compatible for the union with an aggregate df.\n",
        "  \"\"\"\n",
        "  return df.withColumn(\"sum\", F.col(\"distCol\")).withColumnRenamed(\"distCol\", \"squared_sum\")\n",
        "\n",
        "def merge_aggregate_df(aggregate_df: DataFrame, other_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Sums an aggregate df (track_uri, embedding, hashes, squared_sum, sum)\n",
        "  with a simple df (track_uri, embedding, hashes, distCol).\n",
        "  \"\"\"\n",
        "  other_df = transform_to_aggregate(other_df)\n",
        "  merged_df = aggregate_df.unionAll(other_df)\n",
        "  return merged_df.groupBy(\"track_uri\", \"embedding\", \"hashes\").agg(\n",
        "    F.sum(F.pow(\"squared_sum\", 2)).alias(\"squared_sum\"),\n",
        "    F.sum(\"sum\").alias(\"sum\"))\n",
        "\n",
        "if DEBUG:\n",
        "  key = playlist_map.first()[1]\n",
        "  neigh = model.approxNearestNeighbors(playlist_map, key, 10)\n",
        "  merged_df_2 = merge_aggregate_df(transform_to_aggregate(neigh), neigh)\n",
        "  merged_df_2.show() "
      ],
      "metadata": {
        "id": "rPFF6Xwj8wed"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def df_to_dict(df: DataFrame) -> DataFrame:\n",
        "  return df.select(\"track_uri\", \"distCol\").rdd.collectAsMap()\n",
        "\n",
        "def merge_dicts(d1: Dict[str, float], d2: Dict[str, float]) -> Dict[str, float]:\n",
        "  for key, value in d2.items():\n",
        "    if key in d1:\n",
        "      if type(d1[key]) is float:\n",
        "        d1[key] = [d1[key]]\n",
        "      d1[key] += [value]\n",
        "    else:\n",
        "      d1[key] = [value]\n",
        "  return d1\n",
        "\n",
        "if DEBUG:\n",
        "  key = playlist_map.first()[1]\n",
        "  neigh = F.broadcast(model.approxNearestNeighbors(playlist_map, key, 10)).cache()\n",
        "  merged_df_2 = merge_dicts(df_to_dict(neigh), df_to_dict(neigh))\n",
        "  merged_df_2"
      ],
      "metadata": {
        "id": "XmUp55bUg8f9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from functools import reduce\n",
        "\n",
        "#TODO: see if the dictionary approach is right\n",
        "def extract_similar_songs(playlist_tracks: DataFrame, playlist_map, model, k=10, disable_pbar=False) -> DataFrame:\n",
        "  aggregate_df = None\n",
        "  tracks_embedding = F.broadcast(playlist_map.join(F.broadcast(playlist_tracks), \"track_uri\").select(\"track_uri\", \"embedding\"))\n",
        "  transformed_tracks_embeddings = model.transform(tracks_embedding).cache()\n",
        "  k_neighs = []\n",
        "\n",
        "  for row in tqdm(tracks_embedding.collect(), desc='Extracting k-neighbors', disable=disable_pbar):\n",
        "    k_neigh = F.broadcast(model.approxNearestNeighbors(transformed_tracks_embeddings, row[\"embedding\"], k)).cache()\n",
        "    k_neighs.append(df_to_dict(k_neigh))\n",
        "\n",
        "  aggregate_df = reduce(merge_dicts, k_neighs)\n",
        "  return aggregate_df\n",
        "\n",
        "if DEBUG:\n",
        "  # first_playlist = train_df.limit(1).select(F.explode(\"tracks\")).select(\"col.*\").distinct()\n",
        "  first_playlist = train_df.filter(\"pid == 1005\").select(F.explode(\"tracks\")).select(\"col.*\").distinct()\n",
        "  recommendations = extract_similar_songs(first_playlist, playlist_map, model, k=10)"
      ],
      "metadata": {
        "id": "p6sFCM6RG5Y1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are safe by using python dictionaries since the data will be very small, and so dictionaries will be faster than pyspark dataframes."
      ],
      "metadata": {
        "id": "L-zF2NSClWaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "  import sys\n",
        "  print(f\"The reccomendation dictionary is {sys.getsizeof(recommendations)} bytes\")"
      ],
      "metadata": {
        "id": "FRs9DULWlMvl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def aggregate_recommendations(recommendations: Dict[str, float | List[float]]) -> Dict[str, float]:\n",
        "  aggregated = {}\n",
        "  for key, value in recommendations.items():\n",
        "    if type(value) is list:\n",
        "      if sum(value) == 0:\n",
        "        continue\n",
        "      aggregated[key] = sum(x for x in value) / len(value)\n",
        "    else:\n",
        "      aggregated[key] = value\n",
        "  return aggregated\n",
        "\n",
        "if DEBUG:\n",
        "  k = 40\n",
        "  aggregated_recs = aggregate_recommendations(recommendations)\n",
        "  top_k = sorted(aggregated_recs.items(), key=lambda x: -x[1])[:k]\n",
        "  top_k "
      ],
      "metadata": {
        "id": "QbBeYNhKqJgM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a prediction, we can put the result in a pyspark dataframe in order to be used later"
      ],
      "metadata": {
        "id": "vpbn30aDl0h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "  recommendations_df = spark.createDataFrame(data=top_k, schema=[\"track_uri\", \"similarity\"])\n",
        "  recommendations_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "kukcVZ8hl7qN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all togheter:"
      ],
      "metadata": {
        "id": "fVBZMo5ym4wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def item_based_recommendation(playlist: DataFrame,playlist_map: DataFrame, model: MinHashLSHModel, k=50):\n",
        "  playlist_songs = playlist.select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "  recommendations = extract_similar_songs(playlist_songs, playlist_map, model, 10, disable_pbar=True)\n",
        "  aggregated_recommendations = aggregate_recommendations(recommendations)\n",
        "  top_k = sorted(aggregated_recommendations.items(), key=lambda x: -x[1])[:k]\n",
        "  recommendations_df = spark.createDataFrame(data=top_k, schema=[\"track_uri\", \"similarity\"])\n",
        "  return recommendations_df\n",
        "\n",
        "if DEBUG:\n",
        "  playlist = train_df.filter(\"pid == 2005\")\n",
        "  result = item_based_recommendation(playlist, playlist_map, model)\n",
        "  result.show()"
      ],
      "metadata": {
        "id": "PnHTtod7m7jB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Evaluation"
      ],
      "metadata": {
        "id": "92e4Ipd-m2TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(recommendations, ground_truth, num_of_recommendations) -> float:\n",
        "    \"\"\"\n",
        "    Calculates precision at k for the recommendations.\n",
        "    \"\"\"\n",
        "    recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").cache()\n",
        "    reccomended_relevant_tracks_count = recommended_relevant_tracks.count() #this can be top_n_results.join in order to be more performant\n",
        "    recommended_relevant_tracks.unpersist()\n",
        "    precision = reccomended_relevant_tracks_count / float(num_of_recommendations)\n",
        "\n",
        "    return precision\n",
        "\n",
        "\n",
        "import math\n",
        "#TODO: make it more efficient somehow\n",
        "def normalized_discounted_cumulative_gain(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "  recommendations = recommendations.orderBy(F.col(\"similarity\").desc())\n",
        "  recommendations_list = recommendations.collect()\n",
        "  cumulative_gain = 0\n",
        "\n",
        "  intersection = recommendations.join(ground_truth, \"track_uri\").count()\n",
        "  if intersection == 0: return 0\n",
        "\n",
        "  ideal_cumulative_gain = 1 + sum((1 / math.log(i, 2)) for i in range(2, 2+intersection))\n",
        "  for index, row in enumerate(recommendations_list):\n",
        "    i = index + 1\n",
        "    is_rel = ground_truth.filter(F.col(\"track_uri\").isin(row.track_uri)).count() > 0\n",
        "    rel = 1 if is_rel else 0\n",
        "    if i == 1:\n",
        "      cumulative_gain += rel\n",
        "    else:\n",
        "      cumulative_gain += (rel / math.log(i, 2))\n",
        "  return cumulative_gain / ideal_cumulative_gain"
      ],
      "metadata": {
        "id": "R3bMn2m6nGdC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing performance evaluation on a random sample of the test set"
      ],
      "metadata": {
        "id": "c8SUpL2Boghc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(pid: int) -> Tuple[DataFrame, float]:\n",
        "    t1 = time.time()\n",
        "\n",
        "    playlist_train = train_df.filter(f\"pid == {pid}\").cache()\n",
        "    playlist_test = test_df.filter(f\"pid == {pid}\").cache()\n",
        "    ground_truth = playlist_test.select(F.explode(\"tracks\")).select(\"col.*\").cache()\n",
        "    num_of_recommendations = ground_truth.count()\n",
        "\n",
        "    recommendations = item_based_recommendation(playlist_train, playlist_map, model, k=num_of_recommendations)\n",
        "\n",
        "    precision = precision_at_k(recommendations, ground_truth, num_of_recommendations)\n",
        "    gain = normalized_discounted_cumulative_gain(recommendations, ground_truth, num_of_recommendations)\n",
        "\n",
        "    t2 = time.time()\n",
        "    print(f\"Total time: {t2-t1}\")\n",
        "\n",
        "    playlist_train.unpersist()\n",
        "    playlist_test.unpersist()\n",
        "    ground_truth.unpersist()\n",
        "\n",
        "    return playlist_train, playlist_test, ground_truth, recommendations, precision, gain\n",
        "\n",
        "if DEBUG:\n",
        "  train, test, gt, rec, prec, gain  = evaluate(1005)\n",
        "  train.show(), test.show(), gt.show(), rec.show(truncate=False)\n",
        "  print(f\"Precision: {prec}, Gain: {gain}\")"
      ],
      "metadata": {
        "id": "Y8ZIqjdSogHJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "EVALUATION_RESULTS_PATH = os.path.join(GDRIVE_DATA_DIR, \"item_based_evaluation_results\")\n",
        "def perform_evaluation():\n",
        "  SAMPLING_FRACTION = 0.01\n",
        "  sampled_playlists = train_df.sample(False, SAMPLING_FRACTION, seed=42).cache()\n",
        "  results = []\n",
        "  for row in tqdm(sampled_playlists.collect(), desc=\"Performing evaluation\"):\n",
        "      pid = row['pid']\n",
        "      train, test, gt, rec, prec, gain = evaluate(pid)\n",
        "      results.append((prec, gain))\n",
        "  with open(EVALUATION_RESULTS_PATH, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "  return results\n",
        "\n",
        "results = perform_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "21c7a4b5bb3743faac0b6f5d7170cba7",
            "47c7996ddfdc45d7b54bf1bda0975a0e",
            "ccad9bcab8284c8487d5b3471734e72f",
            "8223b461758d461095161936067b9e32",
            "4464bff8c0c7445592c5d106a0dcbd7e",
            "8a2f535c08a94209a5e146c09f30206f",
            "ee7aaf9bbfec4a91b209a2e3dab25188",
            "ce5bbe3e978845f3b7a569aa2d9c4488",
            "03d9318ed7ec415b895e5f857c328430",
            "e0bec19319f340a08f0eee7d179d38fe",
            "d9645488340949d7b39b2bdc2dd97d13"
          ]
        },
        "id": "DokoAdIipxHi",
        "outputId": "efebea8e-3990-4b9d-bfa7-cba470210217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Performing evaluation:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21c7a4b5bb3743faac0b6f5d7170cba7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time: 53.75381398200989\n",
            "Total time: 20.871121406555176\n",
            "Total time: 22.799546718597412\n",
            "Total time: 32.0496039390564\n",
            "Total time: 21.05031991004944\n",
            "Total time: 16.939340114593506\n",
            "Total time: 14.554447889328003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All against all tries"
      ],
      "metadata": {
        "id": "x9gbMKT_lw9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since computing the k nearest neighbors is super slow, I can pre-compute them offline and store them. This will require like 100 years lol."
      ],
      "metadata": {
        "id": "GI8ZtkTmv2qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mh = MinHashLSH(inputCol=\"embedding\", outputCol=\"hashes\", numHashTables=5)\n",
        "model = mh.fit(playlist_map)"
      ],
      "metadata": {
        "id": "uzj7kXRNi6ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ZldxXdXlylr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_k_neighbors(playlist_map: DataFrame, model) -> DataFrame:\n",
        "    result = []\n",
        "    transformed_playlist_map = model.transform(playlist_map).cache()\n",
        "    for index, row in enumerate(tqdm(playlist_map.collect(), desc=\"Computing k-neighbors\")):\n",
        "        k_neighs = model.approxNearestNeighbors(transformed_playlist_map, row.embedding, 10).select(\"track_uri\", F.col(\"distCol\").alias(\"similarity\"))\n",
        "        result.append((row.track_uri, k_neighs.collect()))\n",
        "\n",
        "    k_neighs_schema = StructType([\n",
        "        StructField(\"track_uri\", StringType(), nullable=True),\n",
        "        StructField(\"distCol\", FloatType(), nullable=True)\n",
        "    ])\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"track_uri\", StringType(), nullable=True),\n",
        "        StructField(\"k_neighs\", ArrayType(k_neighs_schema), nullable=True)\n",
        "    ])\n",
        "\n",
        "    result_df = spark.createDataFrame(result, schema)\n",
        "    transformed_playlist_map.unpersist()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "result_df = compute_all_k_neighbors(playlist_map, model)"
      ],
      "metadata": {
        "id": "tqV8qF-iv-RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K_NEIGHBOURS_PATH = os.path.join(GDRIVE_DATA_DIR,\"saved_models\", f\"k_neighbours-{NUM_PLAYLISTS}.parquet\")\n",
        "result_df.write.parquet(K_NEIGHBOURS_PATH)"
      ],
      "metadata": {
        "id": "VVFic9gIM7L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old stuff"
      ],
      "metadata": {
        "id": "aoVjgtwlI9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_aggregate(df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Transforms the dataframe in order to be compatible for the union with an aggregate df.\n",
        "  \"\"\"\n",
        "  return df.withColumn(\"sum\", F.col(\"similarity\")).withColumnRenamed(\"similarity\", \"squared_sum\")\n",
        "\n",
        "def merge_aggregate_df(aggregate_df: DataFrame, other_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Sums an aggregate df (track_uri, embedding, hashes, squared_sum, sum)\n",
        "  with a simple df (track_uri, embedding, hashes, distCol).\n",
        "  \"\"\"\n",
        "  other_df = transform_to_aggregate(other_df)\n",
        "  merged_df = aggregate_df.unionAll(other_df)\n",
        "  return merged_df.groupBy(\"track_uri\", \"embedding\").agg(\n",
        "    F.sum(F.pow(\"squared_sum\", 2)).alias(\"squared_sum\"),\n",
        "    F.sum(\"sum\").alias(\"sum\"))"
      ],
      "metadata": {
        "id": "6VCjq5dxQvr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_similarity_df(input_vector: SparseVector, playlist_map: DataFrame, similarity_function: Callable) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Returns the similarity df for a single song\n",
        "  \"\"\"\n",
        "  \n",
        "  @F.udf(returnType=FloatType())\n",
        "  def compute_similarity(vector1):\n",
        "    return similarity_function(vector1, input_vector)\n",
        "\n",
        "  result_df = playlist_map.withColumn(\"similarity\", compute_similarity(playlist_map.embedding))\n",
        "  \n",
        "  return result_df\n",
        "\n",
        "def get_top_k_results(track_uri: str, similarity_df: DataFrame, k: int = 20) -> DataFrame:\n",
        "  return similarity_df.filter((F.col(\"similarity\") > 0) & (F.col(\"track_uri\") != track_uri)).orderBy(F.col(\"similarity\").desc()).limit(k)\n",
        "\n",
        "if DEBUG:\n",
        "  first_song_vector = playlist_map.select(\"embedding\").limit(1).first()[0]\n",
        "  first_track_uri = playlist_map.select(\"track_uri\").limit(1).first()[0]\n",
        "  similarity_df = create_similarity_df(first_song_vector, playlist_map, jaccard_similarity)\n",
        "  top_k_results = get_top_k_results(first_track_uri, similarity_df).cache()\n",
        "  top_k_results.show()"
      ],
      "metadata": {
        "id": "QmBLSpdtcylv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_similar_songs(playlist_tracks: DataFrame, k=10) -> DataFrame:\n",
        "  aggregate_df = None\n",
        "  tracks_embedding = playlist_tracks.join(playlist_map, \"track_uri\")\n",
        "  for row in tqdm(tracks_embedding.collect(), desc='Extracting k-neighbors'):\n",
        "    k_neigh = get_top_k_results(\n",
        "        track_uri=row.track_uri,\n",
        "        similarity_df= create_similarity_df(input_vector=row.embedding,\n",
        "                                            playlist_map=playlist_map,\n",
        "                                            similarity_function=jaccard_similarity),\n",
        "        k = 10)\n",
        "    \n",
        "    k_neigh.show()\n",
        "    if aggregate_df == None:\n",
        "      aggregate_df = transform_to_aggregate(k_neigh).cache()\n",
        "    else:\n",
        "      aggregate_df = merge_aggregate_df(aggregate_df, k_neigh).cache()\n",
        "    \n",
        "    aggregate_df.show()\n",
        "  return aggregate_df\n",
        "\n",
        "if DEBUG:\n",
        "  first_playlist = train_df.limit(1).select(F.explode(\"tracks\")).select(\"col.*\")\n",
        "  recommendations = extract_similar_songs(first_playlist, model)"
      ],
      "metadata": {
        "id": "yydgn5XD3SZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations.show()"
      ],
      "metadata": {
        "id": "8scK_YReSrYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_similar_songs(playlist: DataFrame, playlist_map: DataFrame) -> Dict[str, float]:\n",
        "#   \"\"\"\n",
        "#   For each track in the playlist, it extracts the top-k most similar results.\n",
        "#   \"\"\"\n",
        "#   weighted_dict = {}\n",
        "  \n",
        "#   # F.udf(returnType=IntegerType())\n",
        "#   def update_weighted_dict(track_uri: str, similarity: float):\n",
        "#     nonlocal weighted_dict\n",
        "#     if track_uri not in weighted_dict:\n",
        "#       weighted_dict[track_uri] = (similarity, 1)\n",
        "#     else:\n",
        "#       curr_similarity = weighted_dict[track_uri][0]\n",
        "#       count = weighted_dict[track_uri][1]\n",
        "#       weighted_dict[track_uri] = (curr_similarity + similarity, count+1)\n",
        "  \n",
        "#   tracks = playlist.select(F.explode(\"tracks.track_uri\")).withColumnRenamed(\"col\", \"track_uri\")\n",
        "#   tracks_mapped = tracks.join(playlist_map, \"track_uri\")\n",
        "\n",
        "#   for track_row in tqdm(tracks_mapped.collect(), desc=f\"Analyzing tracks\"):\n",
        "#     similarity_df = create_similarity_df(track_row.embedding, playlist_map, jaccard_similarity)\n",
        "#     top_k_results = get_top_k_results(first_track_uri, similarity_df).cache()\n",
        "#     top_k_results.foreach(lambda row: update_weighted_dict(row.track_uri, row.similarity))\n",
        "#     top_k_results.unpersist()\n",
        "  \n",
        "#   return weighted_dict\n",
        "\n",
        "# if DEBUG:\n",
        "#   first_playlist = train_df.limit(1).cache()\n",
        "#   weighted_dict = extract_similar_songs(first_playlist, playlist_map)"
      ],
      "metadata": {
        "id": "K9UmYW-9eGuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_similarity_df(playlist: DataFrame, playlist_map: DataFrame, similarityFunction: Callable):\n",
        "  # Get the current playlist's tracks\n",
        "  playlist_tracks = playlist.select(F.explode(\"tracks.track_uri\")).withColumnRenamed(\"col\", \"track_uri\")\n",
        "  playlist_tracks.show()\n",
        "  # Let's extract the rating vector for each single track\n",
        "  playlist_tracks_mapped = playlist_tracks.join(playlist_map, \"track_uri\")\n",
        "  playlist_tracks_mapped.show()\n",
        "\n",
        "\n",
        "  @F.udf(returnType=FloatType())\n",
        "  def compute_similarity(input_vector: SparseVector, other_vector: SparseVector):\n",
        "    return jaccard_similarity(input_vector, other_vector)\n",
        "\n",
        "  # for row in playlist_tracks_mapped.collect():\n",
        "  similarity_df = playlist_map.withColumn(\"similarity\", compute_similarity(playlist_tracks_mapped.limit(2).embedding, playlist_map.embedding))\n",
        "  # break\n",
        "  \n",
        "  similarity_df.show()\n",
        "  return\n",
        "\n",
        "first_playlsit = train_df.limit(1).cache()\n",
        "create_similarity_df(first_playlsit, playlist_map, jaccard_similarity)"
      ],
      "metadata": {
        "id": "hs0TGZxlWnLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_playlsit.show()"
      ],
      "metadata": {
        "id": "WbCUU0GzYswY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}