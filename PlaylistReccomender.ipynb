{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/main/PlaylistReccomender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySOgYrVKaDjm"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3wVgRfHYoOs",
        "outputId": "6ad8a016-27fe-4a83-d46b-eb0c06405ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u362-ga-0ubuntu1~20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive \n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "cellView": "form",
        "id": "CGHPv9OqY9MI"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fb2fcb05-b67e-40b3-e874-0608e1869247"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-378f77b53160>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkTutorial, master=local[*]) created by __init__ at <ipython-input-4-378f77b53160>:12 "
          ]
        }
      ],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f52ceb8-e5a9-4e1c-b0da-d493239510c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ad93cb-fd96-4b50-fc5f-251d0ca82524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ac0b76-b2fa-4f8b-d226-45d48f412ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b47470e-3e91-4514-b53f-fbe770d565ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-11T23:25:39+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc35fc08-e022-4f8d-81af-42522b833fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://4d9d-34-141-140-215.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6a64c1-4916-4ee5-fc8b-d3c2c95edaf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f44e3c115a0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.driver.port', '42775'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              "  ('spark.app.startTime', '1683846209830'),\n",
              "  ('spark.app.submitTime', '1683846209602'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.host', '625b63faea23'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true'),\n",
              "  ('spark.app.id', 'local-1683846213483')])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "# slice_df = spark.read.schema(playlist_schema).json(LITTLE_SLICE_FILE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "PEI_1vcSPxOG"
      },
      "outputs": [],
      "source": [
        "# slice_df.select(\"tracks\").first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "gnNiOotq3p3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8494a85a-8cfa-4b39-bde3-c0ab7471db9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|          name|collaborative|  pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+--------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|         Ratch|        false|45000| 1508976000|        88|        70|            1|[{0, Beyoncé, spo...|       50|   20047039|         48|\n",
            "|  slow it down|        false|45001| 1505952000|        80|        77|            1|[{0, Twinbed, spo...|       20|   20365984|         65|\n",
            "|    Phat Beats|        false|45002| 1466640000|        24|        15|            5|[{0, Baths, spoti...|       16|    5127143|         14|\n",
            "|           ✌🏽|        false|45003| 1509148800|        77|        63|            3|[{0, Owl City, sp...|       50|   17201663|         54|\n",
            "|          💘💘|        false|45004| 1479081600|       102|        78|            1|[{0, Rihanna, spo...|       35|   23364932|         56|\n",
            "|   Summer 2015|         true|45005| 1459555200|       126|       114|            7|[{0, Tiësto, spot...|       22|   30131677|         99|\n",
            "|Summer Country|        false|45006| 1492560000|       246|       166|            1|[{0, Kenny Chesne...|       13|   52836874|         91|\n",
            "|          Jazz|        false|45007| 1478044800|        15|        14|            1|[{0, Freddie Hubb...|        3|    6792929|         13|\n",
            "|       Hip Hop|        false|45008| 1503619200|       160|       113|            1|[{0, Fugees, spot...|       39|   37111155|         64|\n",
            "|    dance jams|        false|45009| 1469750400|        19|        16|            1|[{0, Justin Biebe...|        5|    4201172|         16|\n",
            "|   Inspiration|        false|45010| 1496188800|        30|        30|            3|[{0, The Staves, ...|        7|    7466099|         27|\n",
            "|           <33|        false|45011| 1504396800|        18|        16|            5|[{0, KYLE, spotif...|       10|    4106160|         15|\n",
            "|    Christmas |        false|45012| 1387756800|        42|        20|            1|[{0, Louis Armstr...|        2|    9704283|         26|\n",
            "|   Summer 2k17|        false|45013| 1500249600|        52|        51|            3|[{0, Drake, spoti...|       30|   11619016|         50|\n",
            "|         sleep|        false|45014| 1487030400|       164|       111|            1|[{0, TK N Cash, s...|       37|   38449124|         84|\n",
            "|           SKA|        false|45015| 1507593600|        50|        44|            1|[{0, The Slackers...|       15|    9435544|         34|\n",
            "|       Groovin|        false|45016| 1505347200|        88|        80|            1|[{0, of Montreal,...|       36|   23097679|         68|\n",
            "|     Play ME!!|        false|45017| 1477353600|        42|        40|            1|[{0, Jessie J, sp...|       15|    9737901|         35|\n",
            "|   Alternative|        false|45018| 1509408000|        47|        43|            1|[{0, All Time Low...|       36|   10622933|         35|\n",
            "|Rap En Español|        false|45019| 1503100800|       138|        25|            1|[{0, Cartel De Sa...|        4|   32636526|         11|\n",
            "+--------------+-------------+-----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "slice_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaSyTUepySNu"
      },
      "source": [
        "# User-Based Collaborative Filtering\n",
        "Note: The users are the playlists, the items are the songs and the ratings are 0 if the song is not in the playlist, 1 otherwise.\n",
        "\n",
        "We have to define a function $sim(u,v)$ that defines the similarity between two users based on their ratings.\n",
        "\n",
        "We represent the ratings $r_u \\in \\mathbb{R}^n$ as the $n$ dimensional vector that represents the ratings of the user $u$, where $n$ is the number of total songs in the dataset.\n",
        "\n",
        "As the similarity function we can use Jaccard similarity.\n",
        "\\begin{equation}\n",
        "sim(u,v) = J(r_u, r_v) = \\frac{|r_u \\cap r_v|}{|r_u \\cup r_v|}\n",
        "\\end{equation}\n",
        "\n",
        "Jaccard similarity ignores rating values, but we don't care here since the ratings are binary. In case of discrete value ratings we can use cosine similarity, or better pearson's correlation.\n",
        "\n",
        "Done that, and defined as ${U^k}$ the neighborhood of $u$ ($k$ most similar users to $u$), we define the set of items rated by $u$'s neighborhood as\n",
        "\n",
        "\\begin{equation}\n",
        "I^k = \\{i \\in I : \\mathbf{r_{u,i}} \\downarrow \\land u \\in U^k\\}\n",
        "\\end{equation}\n",
        "\n",
        "The rating for the item $i$ to the user $u$ will just be $\\mathbf{r_u[i]}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Ym9TWjvkGGOa"
      },
      "outputs": [],
      "source": [
        "RATING_VECTOR_FILE_PATH = os.path.join(SAVED_DFS_PATH, \"fake_parquet.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "-XqhUqgS1Qp0"
      },
      "outputs": [],
      "source": [
        "def dense_to_sparse(dense: DenseVector) -> SparseVector:\n",
        "  nonzero_indices = np.nonzero(np.array(dense))[0]\n",
        "  nonzero_values = np.array(dense)[nonzero_indices]\n",
        "  sparse_vector = SparseVector(len(dense), nonzero_indices.tolist(), nonzero_values.tolist())\n",
        "  return sparse_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "6tvZAyYD4NXA"
      },
      "outputs": [],
      "source": [
        "def get_all_songs(playlist_df: DataFrame, set_in_playlist: bool = False) -> DataFrame:\n",
        "   all_songs = playlist_df.select(explode(\"tracks.track_uri\").alias(\"track_uri\")).distinct()\n",
        "   if set_in_playlist:\n",
        "     all_songs = all_songs.withColumn(\"in_playlist\", lit(1))\n",
        "   return all_songs\n",
        "  \n",
        "def get_songs_info(playlist_df: DataFrame, set_in_playlist: bool = False) -> DataFrame:\n",
        "   all_songs = playlist_df.select(explode(\"tracks\")).select(\"col.*\").drop(\"pos\").distinct()\n",
        "   if set_in_playlist:\n",
        "     all_songs = all_songs.withColumn(\"in_playlist\", lit(1))\n",
        "   return all_songs\n",
        "\n",
        "songs_df = get_all_songs(slice_df)\n",
        "songs_df.createOrReplaceTempView(\"SONGS\")\n",
        "\n",
        "songs_df = spark.sql(\"\"\"\n",
        "SELECT \n",
        "    row_number() OVER (\n",
        "        PARTITION BY '' \n",
        "        ORDER BY '' \n",
        "    ) as pos,\n",
        "    *\n",
        "FROM \n",
        "    SONGS\n",
        "\"\"\")\n",
        "\n",
        "songs_df = songs_df.sort(\"track_uri\")\n",
        "\n",
        "songs_info_df = get_songs_info(slice_df)\n",
        "\n",
        "songs_info_df = songs_info_df.join(songs_df, \"track_uri\", \"left\")\n",
        "\n",
        "RATING_VECTOR_LENGTH = songs_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "songs_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVf4OZFDeq3J",
        "outputId": "c94489e1-46ec-42af-825b-f24646f698f8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "|pos|           track_uri|\n",
            "+---+--------------------+\n",
            "|  1|spotify:track:1mr...|\n",
            "|  2|spotify:track:1Uv...|\n",
            "|  3|spotify:track:4WR...|\n",
            "|  4|spotify:track:7B6...|\n",
            "|  5|spotify:track:2Gy...|\n",
            "|  6|spotify:track:7AO...|\n",
            "|  7|spotify:track:48Z...|\n",
            "|  8|spotify:track:1Um...|\n",
            "|  9|spotify:track:7MO...|\n",
            "| 10|spotify:track:27P...|\n",
            "| 11|spotify:track:6lt...|\n",
            "| 12|spotify:track:1yz...|\n",
            "| 13|spotify:track:5Mz...|\n",
            "| 14|spotify:track:3BU...|\n",
            "| 15|spotify:track:4Cl...|\n",
            "| 16|spotify:track:2dN...|\n",
            "| 17|spotify:track:341...|\n",
            "| 18|spotify:track:7ja...|\n",
            "| 19|spotify:track:4eQ...|\n",
            "| 20|spotify:track:6fy...|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the dataframe in order to associate to each track_uri an integer, that will represent the position of the track in the rating_vector. This is useful in order to avoid doing a lot of joins when generating the rating_vectors."
      ],
      "metadata": {
        "id": "X5iG9Mus6VWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def map_track_df_to_pos(playlist_df: DataFrame, mapping: DataFrame) -> List[DataFrame]:\n",
        "#   songs_df_list = [get_all_songs(spark.createDataFrame([row])) for row in tqdm(slice_df.collect(), desc=\"Creating list of dataframes\")]\n",
        "#   track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "#   track_uri_to_id_udf = udf(lambda x: track_uri_to_id.get(x), IntegerType())\n",
        "#   songs_df_mapped_list = []\n",
        "\n",
        "#   for df in tqdm(songs_df_list, desc=\"Mapping uris to pos\"):\n",
        "#       df = df.withColumn('pos', track_uri_to_id_udf(col('track_uri')))\n",
        "#       songs_df_mapped_list.append(df)\n",
        "  \n",
        "#   return songs_df_mapped_list\n",
        "\n",
        "# songs_df_mapped_list = map_track_df_to_pos(slice_df, songs_df)"
      ],
      "metadata": {
        "id": "Zu7sLSmn_QW9"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType, ArrayType\n",
        "\n",
        "def map_track_df_to_pos_2(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "\n",
        "    track_uri_to_id_udf = udf(lambda x: track_uri_to_id.get(x), IntegerType())\n",
        "\n",
        "    # Define a UDF to map track URIs to integer IDs within each element of the list\n",
        "    \n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "      for row in tracks:\n",
        "        uri = track_uri_to_id.get(row.track_uri)\n",
        "        pos_list.add(uri)\n",
        "\n",
        "      return SparseVector(RATING_VECTOR_LENGTH, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    map_track_uri_udf = udf(lambda tracks: extract_vector(tracks), returnType=VectorUDT())\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', map_track_uri_udf(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "mapped_slice_df = map_track_df_to_pos_2(slice_df, songs_df)"
      ],
      "metadata": {
        "id": "MFwQWIZ1Bddv"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_slice_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "skXA_YD2EUmj",
        "outputId": "9b908a38-48d1-4a01-9494-91a4988c0452"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-7cd9e809bc19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmapped_slice_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-99-06603cb2955a>\", line 20, in <lambda>\n  File \"<ipython-input-99-06603cb2955a>\", line 18, in extract_vector\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 634, in __init__\n    np.max(self.indices) < self.size\nAssertionError: Index 681805 is out of the size of vector with size=681805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "GDOER74y5WEJ"
      },
      "outputs": [],
      "source": [
        "def _create_rating_df(playlist_row: Row, songs_df: DataFrame) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Creates a dataframe that represents the \"ratings\" for a playlist in the dataframe\n",
        "  \"\"\"\n",
        "  playlist_row = spark.createDataFrame([playlist_row], playlist_schema)\n",
        "  playlist_uris = get_all_songs(playlist_row)\n",
        "\n",
        "  joined = songs_df.join(playlist_uris, on=\"track_uri\", how=\"right\")\n",
        "  return joined\n",
        "\n",
        "\n",
        "def _check_songs_ordering(playlist_row: DataFrame, songs_df: DataFrame) -> bool:\n",
        "  \"\"\"\n",
        "  Returns a boolean that indicates if the ordering in the songs_df and rating_df is the same\n",
        "  \"\"\"\n",
        "  playlist_row = spark.createDataFrame([playlist_row], playlist_schema)\n",
        "  playlist_uris = get_all_songs(playlist_row, True).withColumnRenamed(\"in_playlist\", \"isin\")\n",
        "\n",
        "  joined = songs_df.join(playlist_uris, on=\"track_uri\", how=\"right\")\n",
        "  joined_left = songs_df.join(playlist_uris, on=\"track_uri\", how=\"left\").filter(\"isin == 1\")\n",
        "  assert joined.collect() == joined_left.collect(), f\"The order of songs_df is different from the order of rating_df!\"\n",
        "\n",
        "# def _extract_rating_vector(rating_df: DataFrame) -> SparseVector:\n",
        "#   \"\"\"\n",
        "#   Extracts the rating vectors for each playlist \n",
        "#   \"\"\"\n",
        "#   dense_vector = DenseVector([row.isin for row in rating_df.select(\"isin\").collect()])\n",
        "#   return dense_to_sparse(dense_vector)\n",
        "\n",
        "def _extrac_sparse_rating_vector(rating_df: DataFrame) -> SparseVector:\n",
        "  indices = np.sort([row.pos for row in rating_df.collect()])\n",
        "  return SparseVector(RATING_VECTOR_LENGTH, indices, np.ones(indices.shape[0]) )\n",
        "\n",
        "def rating_vector_from_row(playlist_row: Row, songs_df: DataFrame):\n",
        "  \"\"\"\n",
        "  Pipelines togheter create_rating_df and extract_rating_vector.\n",
        "  \"\"\"\n",
        "  rating_df_1 = _create_rating_df(playlist_row, songs_df)\n",
        "  rating_vector_1 = _extrac_sparse_rating_vector(rating_df_1)\n",
        "  return rating_vector_1\n",
        "\n",
        "# t1 = time.time() \n",
        "\n",
        "# rating_vector_1 = rating_vector_from_row(slice_df.first(), songs_df)\n",
        "\n",
        "# t2 = time.time()\n",
        "\n",
        "# t2 - t1, rating_vector_1, type(rating_vector_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5Nhc0hwgDQ6X"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(vector_1: SparseVector, vector_2: SparseVector) -> float:\n",
        "  # Convert SparseVectors to sets\n",
        "  set1 = set(vector_1.indices)\n",
        "  set2 = set(vector_2.indices)\n",
        "\n",
        "  # Calculate the intersection and union of the sets\n",
        "  intersection = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "\n",
        "  # Calculate the similarity\n",
        "  similarity = intersection / union\n",
        "\n",
        "  return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "yuDooTJ4_7Pz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "1493898c-3110-42ff-a13a-0e57a1e3d25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-8004947eab18>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mrv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrating_vectors_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_1.playlist_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"playlist_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_1.rating_vector\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rating_vector\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mrating_vectors_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_rating_vectors_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mrating_vectors_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRATING_VECTOR_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-8004947eab18>\u001b[0m in \u001b[0;36mcreate_rating_vectors_df\u001b[0;34m(playlists_df)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mrating_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mplaylist_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylists_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Creating rating vectors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrating_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrating_vector_from_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylist_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msongs_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaylist_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplaylist_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrating_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def create_rating_vectors_df(playlists_df: DataFrame) -> DataFrame:\n",
        "  rating_vectors = []\n",
        "\n",
        "  for playlist_row in tqdm(playlists_df.collect(), desc=\"Creating rating vectors\"):\n",
        "    rating_vector = rating_vector_from_row(playlist_row, songs_df)\n",
        "    new_row = Row(playlist_id=playlist_row.pid, rating_vector=rating_vector)\n",
        "    rating_vectors.append([new_row])\n",
        "  return spark.createDataFrame(rating_vectors)\n",
        "\n",
        "if os.path.exists(RATING_VECTOR_FILE_PATH):\n",
        "  # rv_schema = StructType([StructField('playlist_id', LongType(), True), StructField('rating_vector', pyspark.ml.linalg.VectorUDT(), True)])\n",
        "  rating_vectors_df = spark.read.parquet(RATING_VECTOR_FILE_PATH)\n",
        "  rv_df = rating_vectors_df.select(col(\"_1.playlist_id\").alias(\"playlist_id\"), col(\"_1.rating_vector\").alias(\"rating_vector\"))\n",
        "else:\n",
        "  rating_vectors_df = create_rating_vectors_df(slice_df)\n",
        "  rating_vectors_df.write.parquet(RATING_VECTOR_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGjEbkYTAI3h"
      },
      "outputs": [],
      "source": [
        "def create_similarity_df(input_vector: DataFrame, rating_vectors_df: DataFrame, similarityFunction: Callable) -> DataFrame:\n",
        "  rv_df_input = rating_vectors_df.crossJoin(input_vector)\n",
        "  similarity_udf = udf(similarityFunction, returnType='double')\n",
        "  result_df = rv_df_input.withColumn(\"similarity\", similarity_udf(rv_df_input[\"input_vector\"], rv_df_input[\"rating_vector\"]))\n",
        "  return result_df\n",
        "\n",
        "first_playlist_vector = rv_df.limit(1).select(\"rating_vector\").withColumnRenamed(\"rating_vector\",\"input_vector\")\n",
        "result_df = create_similarity_df(first_playlist_vector, rv_df, jaccard_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12CY_fuNFXKr"
      },
      "source": [
        "Curse of dimensionality! We can see that each playlist is very dissimilar from each other playlist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrPRMP-rINkc"
      },
      "outputs": [],
      "source": [
        "rv_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNoSOnIqCLm8"
      },
      "outputs": [],
      "source": [
        "first_playlist_pid = rv_df.limit(1).select(\"playlist_id\")\n",
        "K = 20\n",
        "top_k_results = result_df.filter( (col('playlist_id') != 1000)).orderBy(col(\"similarity\").desc()).limit(K)\n",
        "top_k_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_eZCcOo7HCW"
      },
      "source": [
        "Making a prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.linalg import VectorUDT\n",
        "\n",
        "# def add_sparse_vectors(accumulator: SparseVector, vector: SparseVector, weight: float) -> SparseVector:\n",
        "#     accumulator_vec = accumulator.toArray() \n",
        "#     array_2 = vector.toArray() * weight\n",
        "\n",
        "#     summed_array = accumulator_vec + array_2\n",
        "\n",
        "#     values = [value for value in summed_array if value != 0]\n",
        "#     sorted_indices = [index for index, value in enumerate(summed_array) if value != 0]\n",
        "#     return SparseVector(accumulator_vec.size, sorted_indices, values)\n",
        "\n",
        "# @udf(returnType=VectorUDT())\n",
        "# def accumulate_sparse_vectors(accumulator, rating_vector, similarity):\n",
        "#     summed_vector = add_sparse_vectors(accumulator, rating_vector, similarity)\n",
        "#     return summed_vector\n",
        "\n",
        "# df = top_k_results.withColumn('accumulated_vector', accumulate_sparse_vectors(top_k_results[\"rating_vector\"], top_k_results[\"rating_vector\"], top_k_results['similarity']))"
      ],
      "metadata": {
        "id": "ePbkSVa3ONgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_top_k_results(top_k_results: DataFrame):\n",
        "  accumulator = np.zeros(RATING_VECTOR_LENGTH)\n",
        "  top_k = top_k_results.collect()\n",
        "  for row in top_k:\n",
        "    accumulator += row.rating_vector.toArray() * row.similarity\n",
        "\n",
        "  values = [value for value in accumulator if value != 0]\n",
        "  sorted_indices = [index for index, value in enumerate(accumulator) if value != 0]\n",
        "  return SparseVector(accumulator.size, sorted_indices, values)\n",
        "\n",
        "accumulated_vector = accumulate_top_k_results(top_k_results)"
      ],
      "metadata": {
        "id": "nDEnlGrCcOHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @udf(returnType=\"int\")\n",
        "# def compute_vector_len(rating_vector):\n",
        "#     return len(rating_vector.indices)\n",
        "# final_df = df.withColumn('accumulated_vector_len', compute_vector_len(df[\"rating_vector\"]))"
      ],
      "metadata": {
        "id": "d6U00q3oR_6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accumulated_vector = final_df.orderBy(col(\"accumulated_vector_len\").desc()).first().accumulated_vector"
      ],
      "metadata": {
        "id": "IsaCrFnuSori"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_values(vector: SparseVector, n:int) -> List[int]:\n",
        "  elements = list(enumerate(vector.toArray()))\n",
        "  sorted_elements = sorted(elements, key=lambda x: x[1], reverse=True)\n",
        "  top_n_indices = [(index, confidence) for index, confidence in sorted_elements[:n]]\n",
        "  return top_n_indices\n",
        "\n",
        "top_n_reccomendations = get_top_n_values(accumulated_vector, 10)"
      ],
      "metadata": {
        "id": "keTpUBIXTdHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n_reccomendations"
      ],
      "metadata": {
        "id": "iE9TIsj5UGyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def song_info_from_index(index: int, confidence: float) -> Row:\n",
        "  song_info = songs_info_df.filter(f\"pos == {index}\").withColumn(\"confidence\", lit(confidence)).first()\n",
        "  return song_info\n",
        "\n",
        "songs_info = [song_info_from_index(index, confidence) for index, confidence in top_n_reccomendations]"
      ],
      "metadata": {
        "id": "ZJOyep5RUMGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reccomendations = spark.createDataFrame(songs_info)"
      ],
      "metadata": {
        "id": "D1a5Nhl8d6Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slice_df.filter(f\"pid == {first_playlist_pid.first().playlist_id}\").show()"
      ],
      "metadata": {
        "id": "UvJkd958g_dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reccomendations.show()"
      ],
      "metadata": {
        "id": "Y8wYERZ6X-Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-UNeSG31Hjw"
      },
      "source": [
        "#Item-Based Collaborative Filtering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfickiR9jDW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlejJ_MFrB9"
      },
      "source": [
        "# Fighting against the curse of dimensionality: Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnioMF6_KGfr"
      },
      "source": [
        "We want to define $\\mathbf{x}_u \\in \\mathbb{R}^d$ $d$-dimensional vector that represents the user $u$, and $\\mathbf{w}_i \\in \\mathbb{R}^d$ vector that represent the item $i$.\n",
        "\n",
        "We then can estimate the rating of user $u$ for the item $i$ by computing\n",
        "\\begin{equation}\n",
        "\\hat{r}_{u, i}=\\mathbf{x}_u^T \\cdot \\mathbf{w}_i=\\sum_{j=1}^d x_{u, j} w_{j, i}\n",
        "\\end{equation}\n",
        "Or, in matrix notation,\n",
        "\n",
        "\\begin{equation}\n",
        "\\underbrace{R}_{m \\times n} =\n",
        "\\underbrace{X}_{m \\times d}\n",
        "\\underbrace{W^T}_{d \\times n}\n",
        "\\end{equation}\n",
        "\n",
        "### How to learn $X$ and $W$\n",
        "The matrix $R$ is partially known and filled with the observations inside the dataset $\\mathcal{D}$. In order to learn the latent factor representations $X$ and $W$, we minimize the following loss function:\n",
        "\\begin{equation}\n",
        "L(X, W)=\\sum_{(u, i) \\in \\mathcal{D}}\\underbrace{\\left(r_{u, i}-\\mathbf{x}_u^T \\cdot \\mathbf{w}_i\\right)^2}_{\\text{squared error term}}+\\underbrace{\\lambda\\left(\\sum_{u \\in \\mathcal{D}}\\left\\|\\mathbf{x}_u\\right\\|^2+\\sum_{i \\in \\mathcal{D}}\\left\\|\\mathbf{w}_i\\right\\|^2\\right)}_{\\text{regularization term}}\n",
        "\\end{equation}\n",
        "\n",
        "We can then minimize the loss using Stochastic Gradient Descent or Alternating Least Squares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el7BsOEsjSgN"
      },
      "source": [
        "# Matrix Factorization\n",
        "Generate a matrix Y where each column represent a playlist and each row represent a song, the (i,j) entry will be 1 if the playlist contains the song, 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KsENF4IfRks"
      },
      "outputs": [],
      "source": [
        "# Throw error in order to not execute the following code\n",
        "raise ValueError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1PEWA1Xjbb2"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import explode\n",
        "spark.conf.set(\"spark.sql.pivotMaxValues\", 1000000)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "# Get a DataFrame of only the relevant columns from the playlist schema\n",
        "# playlists = slice_df.select(\"pid\", \"tracks.track_uri\")\n",
        "# playlists = playlists.select(\"pid\", explode(\"track_uri\").alias(\"song_uri\"))\n",
        "# playlists = playlists.withColumn(\"song_id\", dense_rank().over(Window.orderBy(\"song_uri\")))\n",
        "# plaulists = playlists.withColumn(\"rating\", lit(1))\n",
        "# playlists.count()\n",
        "\n",
        "from pyspark.sql.functions import expr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCEit6ZXgClg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode\n",
        "import random\n",
        "tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"pid\", \"track.track_uri\")\n",
        "tracks_df = tracks_df.withColumn(\"rating\", lit(1))\n",
        "# tracks_df = tracks_df.withColumn(\"rating\", (rand() * 10 + 1).cast(\"integer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzl4_KPigJyN"
      },
      "outputs": [],
      "source": [
        "tracks_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrNo-NacpGXC"
      },
      "outputs": [],
      "source": [
        "# # Explode the tracks array column into multiple rows\n",
        "# # tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\"))\n",
        "# # tracks_df = slice_df.select(\"pid\", \"tracks\", \"tracks\")\n",
        "# tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"pid\", \"track.track_uri\", \"track.pos\")\n",
        "\n",
        "# # Select relevant columns and add a rating column with value 1\n",
        "# playlist_track_df = tracks_df.withColumn(\"rating\", lit(1))\n",
        "\n",
        "# # Get distinct track_uri values and join with playlist_track_df\n",
        "# all_tracks_df = slice_df.select(explode(\"tracks\").alias(\"track\")).select(\"track.track_uri\").distinct()\n",
        "# all_playlists_df = slice_df.select(\"pid\").distinct()\n",
        "\n",
        "# all_against_all = all_tracks_df.join(all_playlists_df).distinct()\n",
        "\n",
        "# from pyspark.sql.functions import when, col\n",
        "\n",
        "# # playlist_track_rating_df = playlist_track_df.join(all_against_all, [\"pid\", \"track_uri\"], \"left_outer\") \\\n",
        "# #     .withColumn(\"rating\", when(col(\"pos\").isNull(), 0).otherwise(1))\n",
        "\n",
        "# playlist_track_rating_df = all_against_all.join(playlist_track_df, [\"pid\", \"track_uri\"], \"left_outer\") \\\n",
        "#     .withColumn(\"rating\", when(col(\"pos\").isNull(), 0).otherwise(1)) \\\n",
        "#     .drop(\"pos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_g71QpbzP9G"
      },
      "outputs": [],
      "source": [
        "playlist_track_rating_df = tracks_df.withColumn(\"song_id\", dense_rank().over(Window.orderBy(\"track_uri\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFR59iIwsuAv"
      },
      "outputs": [],
      "source": [
        "playlist_track_rating_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E86sNXYxTJP"
      },
      "outputs": [],
      "source": [
        "als = ALS(userCol=\"pid\", itemCol=\"song_id\", ratingCol=\"rating\", nonnegative=True, coldStartStrategy=\"drop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKxgkuM0GsVf"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "def train_test_split(df: DataFrame, split_ratio: float, seed: Optional[int] = None) -> Tuple[DataFrame, DataFrame]:\n",
        "  random.seed(seed)\n",
        "  distinct_pids = df.select(\"pid\").distinct().rdd.map(lambda x: x[0]).collect()\n",
        "  random.shuffle(distinct_pids)\n",
        "  split_index = int(len(distinct_pids) * split_ratio)\n",
        "  train_pids = distinct_pids[:split_index]\n",
        "  test_pids = distinct_pids[split_index:]\n",
        "  train_df = df.filter(col(\"pid\").isin(train_pids))\n",
        "  test_df = df.filter(col(\"pid\").isin(test_pids))\n",
        "  return train_df, test_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf82Hstmxb7i"
      },
      "outputs": [],
      "source": [
        "training, test = playlist_track_rating_df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r0g7uPzH19c"
      },
      "outputs": [],
      "source": [
        "model = als.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S3oNC7AIFXY"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwciStIaibF-"
      },
      "outputs": [],
      "source": [
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_FWXza9ycMU"
      },
      "outputs": [],
      "source": [
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJfWDUMSh8Nd"
      },
      "outputs": [],
      "source": [
        "predictions.filter(col(\"prediction\") != \"NaN\").count(), predictions.filter(col(\"prediction\") == \"NaN\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47oQcRQdm-qy"
      },
      "outputs": [],
      "source": [
        "rmse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = playlist_track_rating_df.select(\"pid\").distinct().limit(1)\n",
        "subUserRecs = model.recommendForUserSubset(subset, 10)"
      ],
      "metadata": {
        "id": "q2eoCHXqLNU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset.show()"
      ],
      "metadata": {
        "id": "Tya3FyOxLn0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subUserRecs.show(truncate=False)"
      ],
      "metadata": {
        "id": "u5lfCDhiLZjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def song_name_from_id(song_id: int, reverse_lookup: DataFrame) -> str:\n",
        "  return \n",
        "  \n",
        "def interpretRecommendation(recommended_result: DataFrame) -> str:\n",
        "  return"
      ],
      "metadata": {
        "id": "uyvBEVF8MP30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMvzQsXnykMX"
      },
      "outputs": [],
      "source": [
        "userRecs = model.recommendForAllUsers(1).orderBy(\"recommendations\")\n",
        "userRecs.show(truncate=False)\n",
        "userRecs.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKrGgg35mjmS"
      },
      "outputs": [],
      "source": [
        "slice_df.filter(col(\"pid\") == 1710).select(explode(\"tracks.track_name\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mF1ERfgmyFU"
      },
      "outputs": [],
      "source": [
        "track_uris = playlist_track_rating_df.filter(col(\"song_id\") == 588).select(\"track_uri\")\n",
        "track_uris.first()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}