{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/nn-model/user_based_CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we configure the environment. Since I alternated from Google Colab to Local development, I define a LOCAL variable that allows me to know in which environment I am. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "def is_running_on_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "LOCAL = not is_running_on_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3wVgRfHYoOs",
        "outputId": "dcb30a00-e042-4c4e-e4f0-95951f55460e"
      },
      "outputs": [],
      "source": [
        "#@title Download necessary libraries\n",
        "if not LOCAL:\n",
        "    !pip install pyspark -qq\n",
        "    !pip install -U -q PyDrive -qq\n",
        "    !apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "CGHPv9OqY9MI"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark import SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if not LOCAL:\n",
        "    from google.colab import drive\n",
        "\n",
        "from typing import Tuple, Callable, List\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "if not LOCAL:\n",
        "    JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    GDRIVE_DIR = \"/content/drive\"\n",
        "    GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "    GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "    DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "    AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "    LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "    SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "    LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "    MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "    SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "else:\n",
        "    GDRIVE_DATA_DIR = os.path.abspath(\"./data\")\n",
        "    SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "    SAVED_MODELS = os.path.join(GDRIVE_DATA_DIR, \"saved_models\")\n",
        "    DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"full_dataset\")\n",
        "    SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "    JAVA_HOME = \"/opt/homebrew/opt/openjdk\"\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/29 18:09:28 WARN Utils: Your hostname, MacBook-Air-di-Domiziano.local resolves to a loopback address: 127.0.0.1; using 192.168.1.175 instead (on interface en0)\n",
            "23/06/29 18:09:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/06/29 18:09:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/06/29 18:09:28 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
          ]
        }
      ],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsX5d-YXZ2Ul",
        "outputId": "5a5dd440-edea-47ea-f469-9a4f043335c5"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlxfJiBSZ6ju",
        "outputId": "bc4c8bba-256b-4b45-d45d-166d9a5460ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x10f660070>,\n",
              " [('spark.app.submitTime', '1688054968219'),\n",
              "  ('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.startTime', '1688054968300'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.app.id', 'local-1688054968800'),\n",
              "  ('spark.driver.host', '192.168.1.175'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.driver.port', '64161'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Load DataFrame"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the `DataFrame` schemas and load the primary `DataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WaSyTUepySNu"
      },
      "source": [
        "# User-Based Collaborative Filtering\n",
        "Note: The users are the playlists, the items are the songs and the ratings are 0 if the song is not in the playlist, 1 otherwise.\n",
        "\n",
        "We have to define a function $sim(u,v)$ that defines the similarity between two users based on their ratings.\n",
        "\n",
        "We represent the ratings $r_u \\in \\mathbb{R}^n$ as the $n$ dimensional vector that represents the ratings of the user $u$, where $n$ is the number of total songs in the dataset.\n",
        "\n",
        "As the similarity function we can use Jaccard similarity.\n",
        "\\begin{equation}\n",
        "sim(u,v) = J(r_u, r_v) = \\frac{|r_u \\cap r_v|}{|r_u \\cup r_v|}\n",
        "\\end{equation}\n",
        "\n",
        "Jaccard similarity ignores rating values, but we don't care here since the ratings are binary. In case of discrete value ratings we can use cosine similarity, or better pearson's correlation.\n",
        "\n",
        "Done that, and defined as ${U^k}$ the neighborhood of $u$ ($k$ most similar users to $u$), we define the set of items rated by $u$'s neighborhood as\n",
        "\n",
        "\\begin{equation}\n",
        "I^k = \\{i \\in I : \\mathbf{r_{u,i}} \\downarrow \\land u \\in U^k\\}\n",
        "\\end{equation}\n",
        "\n",
        "The rating for the item $i$ to the user $u$ will just be $\\mathbf{r_u[i]}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dawV4e6yOCo4"
      },
      "outputs": [],
      "source": [
        "DEBUG = False #If True, execute code that helps to debug the code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the song embedding encodings, and the `DataFrame` that maps each song to a position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ym9TWjvkGGOa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "NUM_PLAYLISTS = 100_000\n",
        "SONGS_EMBEDDINGS_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_embeddings-{NUM_PLAYLISTS}.json\")\n",
        "SONGS_INFO_DF = os.path.join(SAVED_DFS_PATH, f\"songs_info_df-{NUM_PLAYLISTS}.json\") #TODO: Little bug this is songs_df, meaning it hasn't got any info, but we don't actually care.\n",
        "RATING_VECTOR_LENGTH_PATH = os.path.join(SAVED_DFS_PATH, f\"songs_vector_length-{NUM_PLAYLISTS}.txt\")\n",
        "with open(RATING_VECTOR_LENGTH_PATH, \"r\") as f:\n",
        "  RATING_VECTOR_LENGTH = int(f.read())\n",
        "\n",
        "songs_embeddings = spark.read.schema(playlist_schema_mapped).json(SONGS_EMBEDDINGS_PATH)\n",
        "song_pos_mapping = spark.read.json(SONGS_INFO_DF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9n1ed0ryz3m",
        "outputId": "134d7eb5-40a2-4c5c-80b8-34c477451d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|         name|collaborative| pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|       disney|        false|1000| 1457827200|       189|        16|            1|(681805,[126,1903...|        4|   31428282|         65|\n",
            "|Indie Electro|        false|1001| 1417824000|       165|        18|            2|(681805,[17322,18...|        2|   38241566|          8|\n",
            "|  jack & jack|        false|1002| 1465430400|        17|        14|            1|(681805,[4591,952...|        3|    3549358|          3|\n",
            "|        vibes|        false|1003| 1498435200|       225|       195|            2|(681805,[392,431,...|       91|   51242585|        157|\n",
            "|        Indie|        false|1004| 1498608000|       165|       118|            1|(681805,[196,425,...|       74|   42601098|         92|\n",
            "|      college|        false|1005| 1504310400|        28|        26|            1|(681805,[899,6937...|        2|    5870710|         25|\n",
            "|    Summer 15|        false|1006| 1471996800|        80|        74|            2|(681805,[1221,351...|       40|   19652753|         67|\n",
            "|    New music|        false|1007| 1509148800|        58|        48|            1|(681805,[1070,207...|       20|   13158862|         38|\n",
            "|          Now|        false|1008| 1505692800|        84|        82|            3|(681805,[1709,371...|       36|   18432208|         79|\n",
            "|         Jams|        false|1009| 1507680000|        41|        36|            1|(681805,[82584,83...|       25|    9413467|         31|\n",
            "|    Party Mix|        false|1010| 1464393600|        74|        72|            2|(681805,[124,397,...|       11|   17853470|         63|\n",
            "|         LOCO|        false|1011| 1491868800|        97|        90|           18|(681805,[753,6951...|       27|   20635206|         82|\n",
            "|   Bruno Mars|        false|1012| 1506211200|        18|         7|            1|(681805,[1226,441...|        3|    3974946|          3|\n",
            "|       Breezy|        false|1013| 1490140800|        67|        55|            1|(681805,[836,3685...|       42|   14724585|         45|\n",
            "|      Fall 16|        false|1014| 1496793600|        83|        64|            3|(681805,[533,900,...|       27|   21004989|         49|\n",
            "|          SLO|        false|1015| 1498262400|        53|        32|            1|(681805,[5466,109...|        3|   12136870|         19|\n",
            "|       BANGAZ|        false|1016| 1508716800|       198|       141|            1|(681805,[65,575,7...|      104|   43843897|         89|\n",
            "|  Piano Music|        false|1017| 1401148800|        29|        21|            1|(681805,[2246,146...|        5|    6992643|         12|\n",
            "|  Beach Music|        false|1018| 1492387200|       130|       106|           14|(681805,[2564,328...|       27|   30811124|         70|\n",
            "|    Slow jamz|        false|1019| 1505779200|        43|        42|            1|(681805,[2081,238...|       15|   11198025|         32|\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "songs_embeddings.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "|pos|           track_uri|\n",
            "+---+--------------------+\n",
            "|  0|spotify:track:1mr...|\n",
            "|  1|spotify:track:1Uv...|\n",
            "|  2|spotify:track:4WR...|\n",
            "|  3|spotify:track:7B6...|\n",
            "|  4|spotify:track:2Gy...|\n",
            "|  5|spotify:track:7AO...|\n",
            "|  6|spotify:track:48Z...|\n",
            "|  7|spotify:track:1Um...|\n",
            "|  8|spotify:track:7MO...|\n",
            "|  9|spotify:track:27P...|\n",
            "| 10|spotify:track:6lt...|\n",
            "| 11|spotify:track:1yz...|\n",
            "| 12|spotify:track:5Mz...|\n",
            "| 13|spotify:track:3BU...|\n",
            "| 14|spotify:track:4Cl...|\n",
            "| 15|spotify:track:2dN...|\n",
            "| 16|spotify:track:341...|\n",
            "| 17|spotify:track:7ja...|\n",
            "| 18|spotify:track:4eQ...|\n",
            "| 19|spotify:track:6fy...|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "song_pos_mapping.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X5iG9Mus6VWy"
      },
      "source": [
        "Preprocessing the dataframe in order to associate to each `track_uri` an integer, that will represent the position of the track in the `rating_vector`. This is the same function that generates the `songs_embeddings`, but I also need it here because I need to convert the input playlist to continuate when doing performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MFwQWIZ1Bddv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "track_uri_to_id = song_pos_mapping.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "def map_track_df_to_pos(playlist_df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    @F.udf(returnType=VectorUDT())\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(track_uri_to_id.get(row.track_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(RATING_VECTOR_LENGTH, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', extract_vector(F.col('tracks')))\n",
        "\n",
        "    return mapped_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I could transform the `song_pos_mapping` into a python dictionary, since it requires very little memory (about 20 MB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hWyQZJ5JrDjR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the track_uri -> position mapping dictionary is 20.971608 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"The size of the track_uri -> position mapping dictionary is {} MB\".format(sys.getsizeof(track_uri_to_id) / 1_000_000))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the similarity function. Since we are dealing with binary vectors, we can use Jaccard Similarity, since we don't need the information about the single values in the vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5Nhc0hwgDQ6X"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(vector_1: SparseVector, vector_2: SparseVector) -> float:\n",
        "  \"\"\"\n",
        "  Computes the Jaccard Similarity between two sparse binary vectors\n",
        "  \"\"\"\n",
        "  set1 = set(vector_1.indices)\n",
        "  set2 = set(vector_2.indices)\n",
        "\n",
        "  intersection = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "\n",
        "  similarity = intersection / union\n",
        "\n",
        "  return similarity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dWMpoMHrpxxf"
      },
      "source": [
        "Creating a function that gets in input the playlist to continue, and returns a Dataframe that indicates its similarity with each other playlist in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SGjEbkYTAI3h"
      },
      "outputs": [],
      "source": [
        "def create_similarity_df(input_vector: DataFrame, rating_vectors_df: DataFrame, similarityFunction: Callable) -> DataFrame:  \n",
        "  \"\"\"\n",
        "  Given a DataFrame with only one row that represents the vector representation of the playlist to continuate, it returns a dataframe containing the similarity between that vector and each\n",
        "  other playlist vector in the dataset.\n",
        "\n",
        "  - input_vector: A DataFrame with only one Row, that is the SparseVector representing the input playlist\n",
        "  - rating_vectors_df: A Dataframe that contains the playlists and their respective vector representation\n",
        "  \"\"\"\n",
        "\n",
        "  input_vector_cached = input_vector.cache()\n",
        "  input_vector = input_vector.first()[0]\n",
        "  \n",
        "  @F.udf(returnType=FloatType())\n",
        "  def compute_similarity(vector1):\n",
        "    return jaccard_similarity(vector1, input_vector)\n",
        "\n",
        "  result_df = rating_vectors_df.withColumn(\"similarity\", compute_similarity(rating_vectors_df.rating_vector))\n",
        "\n",
        "  input_vector_cached.unpersist()\n",
        "  \n",
        "  return result_df\n",
        "\n",
        "if DEBUG:\n",
        "  rv_df = songs_embeddings.withColumnRenamed(\"tracks\", \"rating_vector\")\n",
        "  # Just to show, we take the first playlist as the playlist to be continued \n",
        "  first_playlist_vector = rv_df.limit(1).select(\"rating_vector\").withColumnRenamed(\"rating_vector\",\"input_vector\")\n",
        "  result_df = create_similarity_df(first_playlist_vector, rv_df, jaccard_similarity)\n",
        "  result_df.cache()\n",
        "  result_df.orderBy(F.col(\"similarity\").desc()).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "12CY_fuNFXKr"
      },
      "source": [
        "If we filter the playlists that have a strictly positive similarity with the input playlist, and order them by descending similarity, we can see that the name (that we assume is very informative for the content of the playlist) is very similar, meaning that the algorithm seems to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NrPRMP-rINkc"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    result_df.filter(\"similarity > 0\").orderBy(F.col(\"similarity\").desc()).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppy5-Y19swvG"
      },
      "source": [
        "Now, in order to suggest some songs to continuate the input playlist, let's take the $k$ top most similar playlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wNoSOnIqCLm8"
      },
      "outputs": [],
      "source": [
        "def get_top_k_results(playlist_pid: int, similarity_df: DataFrame, k: int = 20) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given the playlist PID and the DataFrames of similarity relative to that playlist, it returns a DataFrame containing the top k most similar playlists.\n",
        "\n",
        "  - playlist_pid: \n",
        "  - similarity_df: \n",
        "  \"\"\"\n",
        "  return similarity_df.filter((F.col(\"similarity\") > 0) & (F.col(\"pid\") != playlist_pid)).orderBy(F.col(\"similarity\").desc()).limit(k)\n",
        "\n",
        "if DEBUG:\n",
        "  first_playlist_pid = rv_df.limit(1).select(\"pid\").first().pid\n",
        "  top_k_results = get_top_k_results(first_playlist_pid, result_df)\n",
        "  top_k_results.cache()\n",
        "  top_k_results.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nGUtRgGutIgZ"
      },
      "source": [
        "We want to obtain a single embedding for all the $K$ top most similar playlists, that will be the rating vector. We can then pick the indices of the $n$ top greatest values form this vector, and those will be the $n$ songs that we will reccomend.\n",
        "\n",
        "In order to aggregate the $k$ embeddings into a single one, I decided to take an average, weighted by the similarity value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nDEnlGrCcOHp"
      },
      "outputs": [],
      "source": [
        "def get_input_rating_vector(similarity_df: DataFrame) -> SparseVector:\n",
        "  \"\"\"\n",
        "  Given DataFrames of similarities ordered by similarity in descending order, it returns the vector representation of the first row, which\n",
        "  is the vector of the input playlist (similarity 1.0)\n",
        "  \"\"\"\n",
        "  return similarity_df.limit(1).select(\"input_vector\").collect()[0].input_vector\n",
        "\n",
        "def accumulate_top_k_results(top_k_results: DataFrame, input_vector: np.ndarray) -> DataFrame:\n",
        "  \"\"\"\n",
        "  Given a DataFrame that represents the top-k most similar playlists to the input playlist, it returns a\n",
        "  DataFrame that contains a single row, representing the aggregation of all the playlists' vectors.\n",
        "  \n",
        "  The aggregation is just the sum of the vectors, weighted by the relative similarity, and normalized by dividing it by the sum of similarities.\n",
        "  \"\"\"\n",
        "\n",
        "  @F.udf(returnType=VectorUDT())\n",
        "  def sum_vector(sparse_vectors, similarities):\n",
        "    similarities = np.array(similarities)\n",
        "    sparse_vectors = np.array(sparse_vectors)\n",
        "    acc = np.dot(sparse_vectors.T, similarities) #Compute the sum(vector * similarity) for each vector and similarity\n",
        "    acc /= similarities.sum() #Normalize the vector\n",
        "    acc -= (input_vector * acc) #If a song is present in the input playlist, don't consider it\n",
        "    return SparseVector(acc.size, np.nonzero(acc)[0], acc[np.nonzero(acc)])\n",
        "\n",
        "  return top_k_results.agg(sum_vector(F.collect_list('rating_vector'), F.collect_list(\"similarity\")).alias('summed'))\n",
        "\n",
        "if DEBUG:\n",
        "  t1 = time.time()\n",
        "  input_vector = first_playlist_vector.first()[0]\n",
        "  accumulated_vector_df = accumulate_top_k_results(top_k_results, input_vector)\n",
        "  accumulated_vector_df.cache()\n",
        "  accumulated_vector_df.show(truncate=False)\n",
        "  t2 = time.time()\n",
        "  print(t2-t1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that I have the aggregated vector, I can just take the top-$n$ indices that corresponds to the higher values, and those would be the indices of the songs that are the most relevant to the input playlist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "keTpUBIXTdHg"
      },
      "outputs": [],
      "source": [
        "@F.udf(returnType=ArrayType(\n",
        "    StructType([\n",
        "      StructField(\"pos\", IntegerType(), False),\n",
        "      StructField(\"confidence\", FloatType(), False)\n",
        "])))\n",
        "def get_top_n_values(vector: SparseVector, n: int) -> List[Tuple[int, float]]:\n",
        "  \"\"\"\n",
        "  Given the aggregated vector, it returns a list of tuples that map the index of the song to the confidence that that song is relevant for the input playlist.\n",
        "  The index are only of the top-n most confident results, and are ordered by confidence.\n",
        "  \"\"\"\n",
        "  sorted_elements = vector.toArray().tolist()\n",
        "  top_n_indices = sorted(range(len(sorted_elements)), key=lambda i: sorted_elements[i], reverse=True)[:n]\n",
        "  return [(index, sorted_elements[index]) for index in top_n_indices]\n",
        "\n",
        "if DEBUG:\n",
        "  t1 = time.time()\n",
        "  top_n_reccomendations = accumulated_vector_df.withColumn(\"top_n_recommendations\", get_top_n_values(F.col(\"summed\")), F.lit(10)).select(F.explode(\"top_n_recommendations\")).select(\"col.*\")\n",
        "  top_n_reccomendations.show()\n",
        "  t2 = time.time()\n",
        "  print(t2-t1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I can join the indices to the `songs_info_df` DataFrame in order to obtain the `track_uri` of the recommended tracks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZJOyep5RUMGV"
      },
      "outputs": [],
      "source": [
        "def recommendation_song_info(recommendation: DataFrame, songs_info_df: DataFrame) -> DataFrame:\n",
        "  return recommendation.join(songs_info_df, \"pos\")\n",
        "\n",
        "if DEBUG:\n",
        "  t1 = time.time()\n",
        "  songs_info = recommendation_song_info(top_n_reccomendations, song_pos_mapping)\n",
        "  songs_info.show()\n",
        "  t2 = time.time()\n",
        "  print(t2-t1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CFmlqOAttu32"
      },
      "source": [
        "### Putting it all togheter\n",
        "We now define a single function that will get a playlist in input and will reccomend $n$ songs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jfiQn5_rt827"
      },
      "outputs": [],
      "source": [
        "def user_based_recommendation(playlist: DataFrame, \n",
        "                              mapped_slice_df: DataFrame, \n",
        "                              n:int = 50,\n",
        "                              k: int = 20) -> DataFrame:\n",
        "  \n",
        "  \"\"\"\n",
        "  Given a DataFrame with a single row that represents the input playlist, generate the DataFrame of n recommendations.\n",
        "  \"\"\"\n",
        "                              \n",
        "  rv_df = mapped_slice_df.withColumnRenamed(\"tracks\", \"rating_vector\").cache()\n",
        "  playlist_vector = map_track_df_to_pos(playlist).select(\"tracks\").withColumnRenamed(\"tracks\", \"input_vector\").cache()\n",
        "  similarity_df = create_similarity_df(playlist_vector, rv_df, jaccard_similarity).cache()\n",
        "  top_k_results = get_top_k_results(playlist.first().pid, similarity_df, k=k).cache()\n",
        "  input_vector = playlist_vector.select(\"input_vector\").first()[0].toArray()\n",
        "  accumulated_vector_df = accumulate_top_k_results(top_k_results, input_vector).cache()\n",
        "  top_n_indices = accumulated_vector_df\\\n",
        "                  .withColumn(\"top_n_recommendations\", get_top_n_values(F.col(\"summed\"), F.lit(n)))\\\n",
        "                  .select(F.explode(\"top_n_recommendations\"))\\\n",
        "                  .select(\"col.*\").cache()\n",
        "\n",
        "  recommended_songs_info = recommendation_song_info(top_n_indices, song_pos_mapping).cache() \n",
        "\n",
        "  playlist_vector.unpersist()\n",
        "  similarity_df.unpersist()\n",
        "  top_k_results.unpersist()\n",
        "  accumulated_vector_df.unpersist()\n",
        "  top_n_indices.unpersist()\n",
        "  return recommended_songs_info\n",
        "  \n",
        "\n",
        "if DEBUG:\n",
        "  #Collect and createDataFrame because operations on limit(1) take as long as the entire slice_df, don't know why\n",
        "  playlist = spark.createDataFrame(slice_df.filter(\"pid == 1010\").limit(1).collect())\n",
        "  final_recommendation = user_based_recommendation(playlist, songs_embeddings, jaccard_similarity, n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    final_recommendation.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eM7ErDxxt69B"
      },
      "source": [
        "## Performance Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since now we operated on the entire dataset, but for performance evaluation we will generate some recommendations for each playlist in the training set, and then compare the recommended songs with the songs in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GaY6U3hajPVM"
      },
      "outputs": [],
      "source": [
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"train_df-{NUM_PLAYLISTS}.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, f\"test_df-{NUM_PLAYLISTS}.json\")\n",
        "train_df = spark.read.schema(playlist_schema).json(TRAIN_DF_PATH)\n",
        "test_df = spark.read.schema(playlist_schema).json(TEST_DF_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-vEt0SPkqu_",
        "outputId": "413c6285-201b-4edd-ef87-2e3ed37104b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|         name|collaborative| pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|       disney|        false|1000| 1457827200|       189|        16|            1|[{31, Daughters o...|        4|   31428282|         65|\n",
            "|Indie Electro|        false|1001| 1417824000|       165|        18|            2|[{117, Boards of ...|        2|   38241566|          8|\n",
            "|  jack & jack|        false|1002| 1465430400|        17|        14|            1|[{14, Jack & Jack...|        3|    3549358|          3|\n",
            "|        vibes|        false|1003| 1498435200|       225|       195|            2|[{119, PREP, spot...|       91|   51242585|        157|\n",
            "|        Indie|        false|1004| 1498608000|       165|       118|            1|[{117, LCD Sounds...|       74|   42601098|         92|\n",
            "|      college|        false|1005| 1504310400|        28|        26|            1|[{22, Kid Ink, sp...|        2|    5870710|         25|\n",
            "|    Summer 15|        false|1006| 1471996800|        80|        74|            2|[{9, Kygo, spotif...|       40|   19652753|         67|\n",
            "|    New music|        false|1007| 1509148800|        58|        48|            1|[{17, Kodak Black...|       20|   13158862|         38|\n",
            "|          Now|        false|1008| 1505692800|        84|        82|            3|[{30, Mullally, s...|       36|   18432208|         79|\n",
            "|         Jams|        false|1009| 1507680000|        41|        36|            1|[{12, Alabama Sha...|       25|    9413467|         31|\n",
            "|    Party Mix|        false|1010| 1464393600|        74|        72|            2|[{39, Stevie Wond...|       11|   17853470|         63|\n",
            "|         LOCO|        false|1011| 1491868800|        97|        90|           18|[{33, Fat Joe, sp...|       27|   20635206|         82|\n",
            "|   Bruno Mars|        false|1012| 1506211200|        18|         7|            1|[{13, Bruno Mars,...|        3|    3974946|          3|\n",
            "|       Breezy|        false|1013| 1490140800|        67|        55|            1|[{7, Jon Bellion,...|       42|   14724585|         45|\n",
            "|      Fall 16|        false|1014| 1496793600|        83|        64|            3|[{61, Kings of Le...|       27|   21004989|         49|\n",
            "|          SLO|        false|1015| 1498262400|        53|        32|            1|[{34, Todd Snider...|        3|   12136870|         19|\n",
            "|       BANGAZ|        false|1016| 1508716800|       198|       141|            1|[{185, Russ, spot...|      104|   43843897|         89|\n",
            "|  Piano Music|        false|1017| 1401148800|        29|        21|            1|[{26, Jennifer Th...|        5|    6992643|         12|\n",
            "|  Beach Music|        false|1018| 1492387200|       130|       106|           14|[{24, Zion & Lenn...|       27|   30811124|         70|\n",
            "|    Slow jamz|        false|1019| 1505779200|        43|        42|            1|[{27, Bone Thugs-...|       15|   11198025|         32|\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|         name|collaborative| pid|modified_at|num_tracks|num_albums|num_followers|              tracks|num_edits|duration_ms|num_artists|\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "|       disney|        false|1000| 1457827200|       189|        16|            1|[{184, Various Ar...|        4|   31428282|         65|\n",
            "|Indie Electro|        false|1001| 1417824000|       165|        18|            2|[{135, Boards of ...|        2|   38241566|          8|\n",
            "|  jack & jack|        false|1002| 1465430400|        17|        14|            1|[{0, Jack & Jack,...|        3|    3549358|          3|\n",
            "|        vibes|        false|1003| 1498435200|       225|       195|            2|[{9, Marc E. Bass...|       91|   51242585|        157|\n",
            "|        Indie|        false|1004| 1498608000|       165|       118|            1|[{135, Fleet Foxe...|       74|   42601098|         92|\n",
            "|      college|        false|1005| 1504310400|        28|        26|            1|[{9, Kesha, spoti...|        2|    5870710|         25|\n",
            "|    Summer 15|        false|1006| 1471996800|        80|        74|            2|[{30, Jeremih, sp...|       40|   19652753|         67|\n",
            "|    New music|        false|1007| 1509148800|        58|        48|            1|[{0, Post Malone,...|       20|   13158862|         38|\n",
            "|          Now|        false|1008| 1505692800|        84|        82|            3|[{73, PredZ, spot...|       36|   18432208|         79|\n",
            "|         Jams|        false|1009| 1507680000|        41|        36|            1|[{24, Foster The ...|       25|    9413467|         31|\n",
            "|    Party Mix|        false|1010| 1464393600|        74|        72|            2|[{4, The Killers,...|       11|   17853470|         63|\n",
            "|         LOCO|        false|1011| 1491868800|        97|        90|           18|[{62, Boostee, sp...|       27|   20635206|         82|\n",
            "|   Bruno Mars|        false|1012| 1506211200|        18|         7|            1|[{0, Bruno Mars, ...|        3|    3974946|          3|\n",
            "|       Breezy|        false|1013| 1490140800|        67|        55|            1|[{36, Troye Sivan...|       42|   14724585|         45|\n",
            "|      Fall 16|        false|1014| 1496793600|        83|        64|            3|[{30, The Arcs, s...|       27|   21004989|         49|\n",
            "|          SLO|        false|1015| 1498262400|        53|        32|            1|[{19, Bruce Sprin...|        3|   12136870|         19|\n",
            "|       BANGAZ|        false|1016| 1508716800|       198|       141|            1|[{65, Young Thug,...|      104|   43843897|         89|\n",
            "|  Piano Music|        false|1017| 1401148800|        29|        21|            1|[{27, Jennifer Th...|        5|    6992643|         12|\n",
            "|  Beach Music|        false|1018| 1492387200|       130|       106|           14|[{55, Héctor \"El ...|       27|   30811124|         70|\n",
            "|    Slow jamz|        false|1019| 1505779200|        43|        42|            1|[{37, R. Kelly, s...|       15|   11198025|         32|\n",
            "+-------------+-------------+----+-----------+----------+----------+-------------+--------------------+---------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.show()\n",
        "test_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "86dFBXK20uqu"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculates precision at k for the recommendations.\n",
        "    \"\"\"\n",
        "    recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").cache()\n",
        "    reccomended_relevant_tracks_count = recommended_relevant_tracks.count() #this can be top_n_results.join in order to be more performant\n",
        "    recommended_relevant_tracks.unpersist()\n",
        "    precision = reccomended_relevant_tracks_count / float(num_of_recommendations)\n",
        "\n",
        "    return precision\n",
        "\n",
        "def normalized_discounted_cumulative_gain(recommendations: DataFrame, ground_truth: DataFrame, num_of_recommendations: int) -> float:\n",
        "  \"\"\"\n",
        "  Calculates the Normalized Discounted Cumulative Gain between the DataFrame of recommendations and the DataFrame of ground truth.\n",
        "  \"\"\"\n",
        "  recommendations = recommendations.orderBy(F.col(\"confidence\").desc())\n",
        "  recommendations_list = recommendations.collect()\n",
        "  cumulative_gain = 0\n",
        "\n",
        "  intersection = recommendations.join(ground_truth, \"track_uri\").count()\n",
        "  if intersection == 0: return 0\n",
        "\n",
        "  ideal_cumulative_gain = 1 + np.array([(1 / math.log(i, 2)) for i in range(2, 2+intersection)]).sum()\n",
        "  for index, row in enumerate(recommendations_list):\n",
        "    i = index + 1\n",
        "    is_rel = ground_truth.filter(F.col(\"track_uri\").isin(row.track_uri)).count() > 0\n",
        "    rel = 1 if is_rel else 0\n",
        "    if i == 1:\n",
        "      cumulative_gain += rel\n",
        "    else:\n",
        "      cumulative_gain += (rel / math.log(i, 2))\n",
        "  return cumulative_gain / ideal_cumulative_gain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the function that will perform the evaluation pipeline for a single playlist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "J8kbN-2Emdmz"
      },
      "outputs": [],
      "source": [
        "def evaluate(pid: int) -> Tuple[DataFrame, float]:\n",
        "    \"\"\"\n",
        "    Perform the evaluation for a given playlist.\n",
        "    \"\"\"\n",
        "    playlist_train = train_df.filter(f\"pid == {pid}\").cache()\n",
        "    playlist_test = test_df.filter(f\"pid == {pid}\").cache()\n",
        "    ground_truth = playlist_test.select(F.explode(\"tracks\")).select(\"col.*\").cache()\n",
        "    num_of_recommendations = ground_truth.count()\n",
        "    recommendations = user_based_recommendation(playlist_train, \n",
        "                                                songs_embeddings, \n",
        "                                                n=num_of_recommendations,\n",
        "                                                k = 10).cache()\n",
        "    precision = precision_at_k(recommendations, ground_truth, num_of_recommendations)\n",
        "    gain = normalized_discounted_cumulative_gain(recommendations, ground_truth, num_of_recommendations)\n",
        "\n",
        "    playlist_train.unpersist()\n",
        "    playlist_test.unpersist()\n",
        "    ground_truth.unpersist()\n",
        "    return playlist_train, playlist_test, ground_truth, recommendations, precision, gain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finally perform the evaluation on a set of playlists. I decided to evaluate the algorithm on 1000 playlists. Since each evaluation step takes about 30 seconds (30,000 seconds are about 8 hours), I used checkpointing in order to interrupt the evaluation and restart it from the same point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CEJCTq-eqJyk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/29 18:11:07 WARN CacheManager: Asked to cache already cached data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8532e3926e2e4fb5acb6264f9716437e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Performing evaluation:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/29 18:11:07 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:07 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:07 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:07 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:08 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:08 WARN CacheManager: Asked to cache already cached data.        \n",
            "23/06/29 18:11:08 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:09 WARN CacheManager: Asked to cache already cached data.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.07692307692307693, 0.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/29 18:11:29 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:30 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:31 WARN CacheManager: Asked to cache already cached data.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/06/29 18:11:47 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:47 WARN CacheManager: Asked to cache already cached data.\n",
            "23/06/29 18:11:48 WARN CacheManager: Asked to cache already cached data.\n",
            "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 8) / 8]\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/dov/miniconda3/envs/dl/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/Users/dov/miniconda3/envs/dl/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/Users/dov/miniconda3/envs/dl/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m   sampled_playlists\u001b[39m.\u001b[39munpersist()\n\u001b[1;32m     23\u001b[0m   \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m---> 26\u001b[0m results \u001b[39m=\u001b[39m perform_evaluation()\n",
            "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mperform_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m CHECKPOINT_RESULTS \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(GDRIVE_DATA_DIR, \u001b[39m\"\u001b[39m\u001b[39muser_base_evaluation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUB_evaluation_results_check_\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m pid \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m train, test, gt, rec, prec, gain \u001b[39m=\u001b[39m evaluate(pid)\n\u001b[1;32m     14\u001b[0m results\u001b[39m.\u001b[39mappend((prec, gain))\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m((prec, gain))\n",
            "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(pid)\u001b[0m\n\u001b[1;32m      8\u001b[0m num_of_recommendations \u001b[39m=\u001b[39m ground_truth\u001b[39m.\u001b[39mcount()\n\u001b[1;32m      9\u001b[0m recommendations \u001b[39m=\u001b[39m user_based_recommendation(playlist_train, \n\u001b[1;32m     10\u001b[0m                                             songs_embeddings, \n\u001b[1;32m     11\u001b[0m                                             n\u001b[39m=\u001b[39mnum_of_recommendations,\n\u001b[1;32m     12\u001b[0m                                             k \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mcache()\n\u001b[0;32m---> 13\u001b[0m precision \u001b[39m=\u001b[39m precision_at_k(recommendations, ground_truth, num_of_recommendations)\n\u001b[1;32m     14\u001b[0m gain \u001b[39m=\u001b[39m normalized_discounted_cumulative_gain(recommendations, ground_truth, num_of_recommendations)\n\u001b[1;32m     16\u001b[0m playlist_train\u001b[39m.\u001b[39munpersist()\n",
            "Cell \u001b[0;32mIn[29], line 6\u001b[0m, in \u001b[0;36mprecision_at_k\u001b[0;34m(recommendations, ground_truth, num_of_recommendations)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mCalculates precision at k for the recommendations.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m recommended_relevant_tracks \u001b[39m=\u001b[39m recommendations\u001b[39m.\u001b[39mjoin(ground_truth, \u001b[39m\"\u001b[39m\u001b[39mtrack_uri\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcache()\n\u001b[0;32m----> 6\u001b[0m reccomended_relevant_tracks_count \u001b[39m=\u001b[39m recommended_relevant_tracks\u001b[39m.\u001b[39;49mcount() \u001b[39m#this can be top_n_results.join in order to be more performant\u001b[39;00m\n\u001b[1;32m      7\u001b[0m recommended_relevant_tracks\u001b[39m.\u001b[39munpersist()\n\u001b[1;32m      8\u001b[0m precision \u001b[39m=\u001b[39m reccomended_relevant_tracks_count \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(num_of_recommendations)\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcount())\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 100:=================================================>       (7 + 1) / 8]\r"
          ]
        }
      ],
      "source": [
        "LAST_CHECKPOINT_INDEX = 0\n",
        "EVALUATION_RESULTS_PATH = os.path.join(GDRIVE_DATA_DIR, f\"user_base_evaluation\", \"UB_evaluation_results_FINAL\")\n",
        "def perform_evaluation():\n",
        "  SAMPLING_FRACTION = 0.01\n",
        "  sampled_playlists = train_df.sample(False, SAMPLING_FRACTION, seed=42).cache()\n",
        "\n",
        "  results = []\n",
        "  for index, row in enumerate(tqdm(sampled_playlists.collect(), desc=\"Performing evaluation\")):\n",
        "      if index <= LAST_CHECKPOINT_INDEX: continue \n",
        "      \n",
        "      CHECKPOINT_RESULTS = os.path.join(GDRIVE_DATA_DIR, \"user_base_evaluation\", f\"UB_evaluation_results_check_{index}\")\n",
        "      pid = row['pid']\n",
        "      train, test, gt, rec, prec, gain = evaluate(pid)\n",
        "      results.append((prec, gain))\n",
        "      print((prec, gain))\n",
        "      if index % 10 == 0:\n",
        "         with open(CHECKPOINT_RESULTS, \"w\") as f:\n",
        "            json.dump(results, f)\n",
        "  with open(EVALUATION_RESULTS_PATH, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "  \n",
        "  sampled_playlists.unpersist()\n",
        "  return results\n",
        "\n",
        "\n",
        "results = perform_evaluation()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the `results`, average them and see how the model performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvIrTvqfrzZ",
        "outputId": "1fe047e4-1352-434f-efc1-7d77293c6277"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.10053558551616848, 0.28256513840537667)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LAST_CHECKPOINT_RESULTS = os.path.join(\"./data\", \"user_base_evaluation\", f\"evaluation_results_check_500\")\n",
        "with open(LAST_CHECKPOINT_RESULTS, \"r\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "avg_prec = np.array(results).mean()\n",
        "avg_prec, avg_gain = 0, 0\n",
        "for prec, gain in results:\n",
        "  avg_prec += prec\n",
        "  avg_gain += gain \n",
        "tot = len(results)\n",
        "avg_prec /= tot\n",
        "avg_gain /= tot\n",
        "avg_prec, avg_gain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model has a precision of about 0.10 and a NDCG of 0.28, which isn't bad taking into account that state of the art models with 1M playlists for train have 0.2 precision and 0.38 NDCG."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlejJ_MFrB9"
      },
      "source": [
        "# Fighting against the curse of dimensionality: Matrix Factorization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gnioMF6_KGfr"
      },
      "source": [
        "We want to define $\\mathbf{x}_u \\in \\mathbb{R}^d$ $d$-dimensional vector that represents the user $u$, and $\\mathbf{w}_i \\in \\mathbb{R}^d$ vector that represent the item $i$.\n",
        "\n",
        "We then can estimate the rating of user $u$ for the item $i$ by computing\n",
        "\\begin{equation}\n",
        "\\hat{r}_{u, i}=\\mathbf{x}_u^T \\cdot \\mathbf{w}_i=\\sum_{j=1}^d x_{u, j} w_{j, i}\n",
        "\\end{equation}\n",
        "Or, in matrix notation,\n",
        "\n",
        "\\begin{equation}\n",
        "\\underbrace{R}_{m \\times n} =\n",
        "\\underbrace{X}_{m \\times d}\n",
        "\\underbrace{W^T}_{d \\times n}\n",
        "\\end{equation}\n",
        "\n",
        "### How to learn $X$ and $W$\n",
        "The matrix $R$ is partially known and filled with the observations inside the dataset $\\mathcal{D}$. In order to learn the latent factor representations $X$ and $W$, we minimize the following loss function:\n",
        "\\begin{equation}\n",
        "L(X, W)=\\sum_{(u, i) \\in \\mathcal{D}}\\underbrace{\\left(r_{u, i}-\\mathbf{x}_u^T \\cdot \\mathbf{w}_i\\right)^2}_{\\text{squared error term}}+\\underbrace{\\lambda\\left(\\sum_{u \\in \\mathcal{D}}\\left\\|\\mathbf{x}_u\\right\\|^2+\\sum_{i \\in \\mathcal{D}}\\left\\|\\mathbf{w}_i\\right\\|^2\\right)}_{\\text{regularization term}}\n",
        "\\end{equation}\n",
        "\n",
        "We can then minimize the loss using Stochastic Gradient Descent or Alternating Least Squares."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "el7BsOEsjSgN"
      },
      "source": [
        "# Matrix Factorization\n",
        "Generate a matrix Y where each column represent a playlist and each row represent a song, the (i,j) entry will be 1 if the playlist contains the song, 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KsENF4IfRks"
      },
      "outputs": [],
      "source": [
        "# Throw error in order to not execute the following code\n",
        "raise ValueError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1PEWA1Xjbb2"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import explode\n",
        "spark.conf.set(\"spark.sql.pivotMaxValues\", 1000000)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import expr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCEit6ZXgClg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode\n",
        "import random\n",
        "tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"pid\", \"track.track_uri\")\n",
        "tracks_df = tracks_df.withColumn(\"rating\", F.lit(1))\n",
        "# tracks_df = tracks_df.withColumn(\"rating\", (rand() * 10 + 1).cast(\"integer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzl4_KPigJyN"
      },
      "outputs": [],
      "source": [
        "tracks_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrNo-NacpGXC"
      },
      "outputs": [],
      "source": [
        "# # Explode the tracks array column into multiple rows\n",
        "# # tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\"))\n",
        "# # tracks_df = slice_df.select(\"pid\", \"tracks\", \"tracks\")\n",
        "# tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"pid\", \"track.track_uri\", \"track.pos\")\n",
        "\n",
        "# # Select relevant columns and add a rating column with value 1\n",
        "# playlist_track_df = tracks_df.withColumn(\"rating\", lit(1))\n",
        "\n",
        "# # Get distinct track_uri values and join with playlist_track_df\n",
        "# all_tracks_df = slice_df.select(explode(\"tracks\").alias(\"track\")).select(\"track.track_uri\").distinct()\n",
        "# all_playlists_df = slice_df.select(\"pid\").distinct()\n",
        "\n",
        "# all_against_all = all_tracks_df.join(all_playlists_df).distinct()\n",
        "\n",
        "# from pyspark.sql.functions import when, col\n",
        "\n",
        "# # playlist_track_rating_df = playlist_track_df.join(all_against_all, [\"pid\", \"track_uri\"], \"left_outer\") \\\n",
        "# #     .withColumn(\"rating\", when(col(\"pos\").isNull(), 0).otherwise(1))\n",
        "\n",
        "# playlist_track_rating_df = all_against_all.join(playlist_track_df, [\"pid\", \"track_uri\"], \"left_outer\") \\\n",
        "#     .withColumn(\"rating\", when(col(\"pos\").isNull(), 0).otherwise(1)) \\\n",
        "#     .drop(\"pos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_g71QpbzP9G"
      },
      "outputs": [],
      "source": [
        "playlist_track_rating_df = tracks_df.withColumn(\"song_id\", dense_rank().over(Window.orderBy(\"track_uri\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFR59iIwsuAv"
      },
      "outputs": [],
      "source": [
        "playlist_track_rating_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E86sNXYxTJP"
      },
      "outputs": [],
      "source": [
        "als = ALS(userCol=\"pid\", itemCol=\"song_id\", ratingCol=\"rating\", nonnegative=True, coldStartStrategy=\"drop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKxgkuM0GsVf"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "def train_test_split(df: DataFrame, split_ratio: float, seed: Optional[int] = None) -> Tuple[DataFrame, DataFrame]:\n",
        "  random.seed(seed)\n",
        "  distinct_pids = df.select(\"pid\").distinct().rdd.map(lambda x: x[0]).collect()\n",
        "  random.shuffle(distinct_pids)\n",
        "  split_index = int(len(distinct_pids) * split_ratio)\n",
        "  train_pids = distinct_pids[:split_index]\n",
        "  test_pids = distinct_pids[split_index:]\n",
        "  train_df = df.filter(col(\"pid\").isin(train_pids))\n",
        "  test_df = df.filter(col(\"pid\").isin(test_pids))\n",
        "  return train_df, test_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf82Hstmxb7i"
      },
      "outputs": [],
      "source": [
        "training, test = playlist_track_rating_df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r0g7uPzH19c"
      },
      "outputs": [],
      "source": [
        "model = als.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S3oNC7AIFXY"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwciStIaibF-"
      },
      "outputs": [],
      "source": [
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_FWXza9ycMU"
      },
      "outputs": [],
      "source": [
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJfWDUMSh8Nd"
      },
      "outputs": [],
      "source": [
        "predictions.filter(col(\"prediction\") != \"NaN\").count(), predictions.filter(col(\"prediction\") == \"NaN\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47oQcRQdm-qy"
      },
      "outputs": [],
      "source": [
        "rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2eoCHXqLNU8"
      },
      "outputs": [],
      "source": [
        "subset = playlist_track_rating_df.select(\"pid\").distinct().limit(1)\n",
        "subUserRecs = model.recommendForUserSubset(subset, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tya3FyOxLn0S"
      },
      "outputs": [],
      "source": [
        "subset.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5lfCDhiLZjo"
      },
      "outputs": [],
      "source": [
        "subUserRecs.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyvBEVF8MP30"
      },
      "outputs": [],
      "source": [
        "def song_name_from_id(song_id: int, reverse_lookup: DataFrame) -> str:\n",
        "  return \n",
        "  \n",
        "def interpretRecommendation(recommended_result: DataFrame) -> str:\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMvzQsXnykMX"
      },
      "outputs": [],
      "source": [
        "userRecs = model.recommendForAllUsers(1).orderBy(\"recommendations\")\n",
        "userRecs.show(truncate=False)\n",
        "userRecs.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKrGgg35mjmS"
      },
      "outputs": [],
      "source": [
        "slice_df.filter(col(\"pid\") == 1710).select(explode(\"tracks.track_name\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mF1ERfgmyFU"
      },
      "outputs": [],
      "source": [
        "track_uris = playlist_track_rating_df.filter(col(\"song_id\") == 588).select(\"track_uri\")\n",
        "track_uris.first()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VJeY9PpvaHUJ",
        "SqlejJ_MFrB9",
        "el7BsOEsjSgN"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
