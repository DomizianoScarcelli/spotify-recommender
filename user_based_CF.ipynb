{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomizianoScarcelli/big-data-project/blob/main/user_based_CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySOgYrVKaDjm"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOojFseRjVnN"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N3wVgRfHYoOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcfe361-4a3f-4b3f-9c13-2a154cd10e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Download necessary libraries\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive \n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CGHPv9OqY9MI"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import gc\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF8LUBZeYiWP"
      },
      "outputs": [],
      "source": [
        "#@title Set up variables\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Big Data/datasets\"\n",
        "DATASET_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_friendly_spotify_playlist_dataset\")\n",
        "AUDIO_FEATURES_FILE = os.path.join(GDRIVE_DATA_DIR, \"pyspark_track_features\")\n",
        "LITTLE_SLICE_FILE = os.path.join(GDRIVE_DATA_DIR, \"little_slice\")\n",
        "SMALL_SLICE_FLIE = os.path.join(GDRIVE_DATA_DIR, \"small_slice\")\n",
        "LITTLE_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"little_slice_audio_features\")\n",
        "MICRO_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"micro_slice_audio_features\")\n",
        "SPLITTED_SLICE_AUDIO_FEATURES = os.path.join(GDRIVE_DATA_DIR, \"splitted_pyspark_track_features\")\n",
        "SAVED_DFS_PATH = os.path.join(GDRIVE_DATA_DIR, \"saved_dfs\")\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4m7VztzdZgm6"
      },
      "outputs": [],
      "source": [
        "#@title Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '100G').\\\n",
        "                set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\").\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsX5d-YXZ2Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c733d6e1-dba8-4ae8-d65a-a86a6f69e8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-s_eNiVaxhn"
      },
      "source": [
        "## Setup ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LrnLYquoarPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c516393-628a-459a-8937-ad8a5d85f30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3IEuiyDawo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e8c2d9-d973-4de2-d90f-db98d4b8f3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2NVN8kdoOnMVtlDGGWtwsbT5M3Q_2EJv2HE77FEXkz978Qtnq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0qJN-lfta1ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75f0ffe-21e5-41eb-f218-909dd9ac2af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-05-22T22:36:23+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x15LhY-5a3Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf4603a-9714-48fa-f662-a8c6093a0e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://e56d-34-125-179-2.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "jlxfJiBSZ6ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11455598-e1d1-4424-e9e1-93c4fc718cfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pyspark.sql.session.SparkSession at 0x7f7c5979a1d0>,\n",
              " [('spark.executor.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC'),\n",
              "  ('spark.app.name', 'PySparkTutorial'),\n",
              "  ('spark.executor.id', 'driver'),\n",
              "  ('spark.driver.host', 'c441443342df'),\n",
              "  ('spark.driver.extraJavaOptions',\n",
              "   '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              "  ('spark.ui.port', '4050'),\n",
              "  ('spark.driver.port', '42953'),\n",
              "  ('spark.app.submitTime', '1684794963044'),\n",
              "  ('spark.app.startTime', '1684794963606'),\n",
              "  ('spark.rdd.compress', 'True'),\n",
              "  ('spark.driver.memory', '12G'),\n",
              "  ('spark.app.id', 'local-1684794967509'),\n",
              "  ('spark.serializer.objectStreamReset', '100'),\n",
              "  ('spark.master', 'local[*]'),\n",
              "  ('spark.submit.pyFiles', ''),\n",
              "  ('spark.driver.maxResultSize', '100G'),\n",
              "  ('spark.submit.deployMode', 'client'),\n",
              "  ('spark.executor.memory', '12G'),\n",
              "  ('spark.ui.showConsoleProgress', 'true')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title Check if everything is ok\n",
        "spark, sc._conf.getAll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeY9PpvaHUJ"
      },
      "source": [
        "# Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "icd2lj-RRvhU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "song_schema = StructType([\n",
        "    StructField(\"pos\", IntegerType(), True),\n",
        "    StructField(\"artist_name\", StringType(), True),\n",
        "    StructField(\"track_uri\", StringType(), True),\n",
        "    StructField(\"artist_uri\", StringType(), True),\n",
        "    StructField(\"track_name\", StringType(), True),\n",
        "    StructField(\"album_uri\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"album_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "playlist_schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", ArrayType(song_schema), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "playlist_schema_mapped = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"collaborative\", StringType(), True),\n",
        "    StructField(\"pid\", IntegerType(), True),\n",
        "    StructField(\"modified_at\", IntegerType(), True),\n",
        "    StructField(\"num_tracks\", IntegerType(), True),\n",
        "    StructField(\"num_albums\", IntegerType(), True),\n",
        "    StructField(\"num_followers\", IntegerType(), True),\n",
        "    StructField(\"tracks\", VectorUDT(), True),\n",
        "    StructField(\"num_edits\", IntegerType(), True),\n",
        "    StructField(\"duration_ms\", IntegerType(), True),\n",
        "    StructField(\"num_artists\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "audio_features_schema = StructType([\n",
        "    StructField(\"danceability\", FloatType(), True),\n",
        "    StructField(\"energy\", FloatType(), True),\n",
        "    StructField(\"key\", IntegerType(), True),\n",
        "    StructField(\"loudness\", FloatType(), True),\n",
        "    StructField(\"mode\", IntegerType(), True),\n",
        "    StructField(\"speechiness\", FloatType(), True),\n",
        "    StructField(\"acousticness\", FloatType(), True),\n",
        "    StructField(\"instrumentalness\", FloatType(), True),\n",
        "    StructField(\"liveness\", FloatType(), True),\n",
        "    StructField(\"valence\", FloatType(), True),\n",
        "    StructField(\"tempo\", FloatType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"uri\", StringType(), True),\n",
        "    StructField(\"track_href\", StringType(), True),\n",
        "    StructField(\"analysis_url\", StringType(), True),\n",
        "    StructField(\"duration_ms\", LongType(), True),\n",
        "    StructField(\"time_signature\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BY_szFyLTps4"
      },
      "outputs": [],
      "source": [
        "playlist_df = spark.read.schema(playlist_schema).json(DATASET_FILE, multiLine=True)\n",
        "slice_df = spark.read.schema(playlist_schema).json(SMALL_SLICE_FLIE, multiLine=True)\n",
        "# slice_df = spark.read.schema(playlist_schema).json(LITTLE_SLICE_FILE, multiLine=True)\n",
        "audio_df = spark.read.schema(audio_features_schema).json(SPLITTED_SLICE_AUDIO_FEATURES, multiLine=True) #has less songs than expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PEI_1vcSPxOG"
      },
      "outputs": [],
      "source": [
        "# slice_df.select(\"tracks\").first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gnNiOotq3p3f"
      },
      "outputs": [],
      "source": [
        "# slice_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaSyTUepySNu"
      },
      "source": [
        "# User-Based Collaborative Filtering\n",
        "Note: The users are the playlists, the items are the songs and the ratings are 0 if the song is not in the playlist, 1 otherwise.\n",
        "\n",
        "We have to define a function $sim(u,v)$ that defines the similarity between two users based on their ratings.\n",
        "\n",
        "We represent the ratings $r_u \\in \\mathbb{R}^n$ as the $n$ dimensional vector that represents the ratings of the user $u$, where $n$ is the number of total songs in the dataset.\n",
        "\n",
        "As the similarity function we can use Jaccard similarity.\n",
        "\\begin{equation}\n",
        "sim(u,v) = J(r_u, r_v) = \\frac{|r_u \\cap r_v|}{|r_u \\cup r_v|}\n",
        "\\end{equation}\n",
        "\n",
        "Jaccard similarity ignores rating values, but we don't care here since the ratings are binary. In case of discrete value ratings we can use cosine similarity, or better pearson's correlation.\n",
        "\n",
        "Done that, and defined as ${U^k}$ the neighborhood of $u$ ($k$ most similar users to $u$), we define the set of items rated by $u$'s neighborhood as\n",
        "\n",
        "\\begin{equation}\n",
        "I^k = \\{i \\in I : \\mathbf{r_{u,i}} \\downarrow \\land u \\in U^k\\}\n",
        "\\end{equation}\n",
        "\n",
        "The rating for the item $i$ to the user $u$ will just be $\\mathbf{r_u[i]}$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import wraps\n",
        "import time\n",
        "\n",
        "DEBUG = False\n",
        "def timeit(func):\n",
        "    @wraps(func)\n",
        "    def timeit_wrapper(*args, **kwargs):\n",
        "        start_time = time.perf_counter()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.perf_counter()\n",
        "        total_time = end_time - start_time\n",
        "        print(f'Function {func.__name__} Took {total_time:.4f} seconds')\n",
        "        return result\n",
        "    return timeit_wrapper"
      ],
      "metadata": {
        "id": "dawV4e6yOCo4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ym9TWjvkGGOa"
      },
      "outputs": [],
      "source": [
        "RATING_VECTOR_FILE_PATH = os.path.join(SAVED_DFS_PATH, \"playlist_rating_df.parquet\")\n",
        "PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"playlist_embeddings.json\")\n",
        "FULL_PLAYLIST_EMBEDDINGS = os.path.join(SAVED_DFS_PATH, \"full_playlist_embeddings.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-XqhUqgS1Qp0"
      },
      "outputs": [],
      "source": [
        "def dense_to_sparse(dense: DenseVector) -> SparseVector:\n",
        "  nonzero_indices = np.nonzero(np.array(dense))[0]\n",
        "  nonzero_values = np.array(dense)[nonzero_indices]\n",
        "  sparse_vector = SparseVector(len(dense), nonzero_indices.tolist(), nonzero_values.tolist())\n",
        "  return sparse_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6tvZAyYD4NXA"
      },
      "outputs": [],
      "source": [
        "def get_all_songs(playlist_df: DataFrame, set_in_playlist: bool = False) -> DataFrame:\n",
        "   all_songs = playlist_df.select(explode(\"tracks.track_uri\").alias(\"track_uri\")).distinct()\n",
        "   if set_in_playlist:\n",
        "     all_songs = all_songs.withColumn(\"in_playlist\", lit(1))\n",
        "   return all_songs\n",
        "  \n",
        "def get_songs_info(playlist_df: DataFrame, set_in_playlist: bool = False) -> DataFrame:\n",
        "   all_songs = playlist_df.select(explode(\"tracks\")).select(\"col.*\").drop(\"pos\").distinct()\n",
        "   if set_in_playlist:\n",
        "     all_songs = all_songs.withColumn(\"in_playlist\", lit(1))\n",
        "   return all_songs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "songs_info_df = get_songs_info(slice_df)\n",
        "songs_info_df.createOrReplaceTempView(\"SONGS_INFO\")\n",
        "\n",
        "songs_info_df = spark.sql(\"\"\"\n",
        "SELECT \n",
        "    row_number() OVER (\n",
        "        PARTITION BY '' \n",
        "        ORDER BY '' \n",
        "    ) as pos,\n",
        "    *\n",
        "FROM \n",
        "    SONGS_INFO\n",
        "\"\"\")\n",
        "\n",
        "songs_info_df = songs_info_df.sort(\"track_uri\")\n",
        "\n",
        "songs_df = songs_info_df.select(\"pos\", \"track_uri\")\n",
        "\n",
        "RATING_VECTOR_LENGTH = songs_df.count()"
      ],
      "metadata": {
        "id": "OEE3KZZfypSY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# songs_info_df.show()\n",
        "# songs_df.show()"
      ],
      "metadata": {
        "id": "oVf4OZFDeq3J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the dataframe in order to associate to each track_uri an integer, that will represent the position of the track in the rating_vector. This is useful in order to avoid doing a lot of joins when generating the rating_vectors."
      ],
      "metadata": {
        "id": "X5iG9Mus6VWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def map_track_df_to_pos(playlist_df: DataFrame, mapping: DataFrame) -> List[DataFrame]:\n",
        "#   songs_df_list = [get_all_songs(spark.createDataFrame([row])) for row in tqdm(slice_df.collect(), desc=\"Creating list of dataframes\")]\n",
        "#   track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap()\n",
        "#   track_uri_to_id_udf = udf(lambda x: track_uri_to_id.get(x), IntegerType())\n",
        "#   songs_df_mapped_list = []\n",
        "\n",
        "#   for df in tqdm(songs_df_list, desc=\"Mapping uris to pos\"):\n",
        "#       df = df.withColumn('pos', track_uri_to_id_udf(col('track_uri')))\n",
        "#       songs_df_mapped_list.append(df)\n",
        "  \n",
        "#   return songs_df_mapped_list\n",
        "\n",
        "# songs_df_mapped_list = map_track_df_to_pos(slice_df, songs_df)"
      ],
      "metadata": {
        "id": "Zu7sLSmn_QW9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType, ArrayType\n",
        "from functools import reduce\n",
        "\n",
        "track_uri_to_id = songs_df.select('track_uri', 'pos').rdd.collectAsMap() # TODO: Pass it as a parameter maybe?\n",
        "#TODO: Since the .rdd is very slow, I can embed the position information of the track inside the track itself,\n",
        "# So then I can just do pos_list.add(row.rating_position) in a few miliseconds. \n",
        "@timeit\n",
        "def map_track_df_to_pos(playlist_df: DataFrame, mapping: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrames containing the playlists, but the tracks are represented as a binary sparse vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_vector(tracks):\n",
        "      pos_list = set()\n",
        "\n",
        "      def reduce_fn(pos_list, row):\n",
        "          pos_list.add(track_uri_to_id.get(row.track_uri))\n",
        "          return pos_list\n",
        "      \n",
        "      pos_list = reduce(reduce_fn, tracks, pos_list)\n",
        "      \n",
        "      return SparseVector(RATING_VECTOR_LENGTH + 1, sorted(list(pos_list)), [1 for _ in pos_list])\n",
        "\n",
        "    map_track_uri_udf = udf(lambda tracks: extract_vector(tracks), returnType=VectorUDT())\n",
        "\n",
        "    # Apply the mapping UDF on the \"tracks\" column of the slice_df dataframe\n",
        "    mapped_df = playlist_df.withColumn('tracks', map_track_uri_udf(col('tracks')))\n",
        "\n",
        "    return mapped_df\n",
        "\n",
        "if not os.path.exists(PLAYLIST_EMBEDDINGS):\n",
        "  mapped_slice_df = map_track_df_to_pos(slice_df, songs_df)\n",
        "  mapped_slice_df.write.json(PLAYLIST_EMBEDDINGS)\n",
        "else:\n",
        "  mapped_slice_df = spark.read.json(PLAYLIST_EMBEDDINGS)"
      ],
      "metadata": {
        "id": "MFwQWIZ1Bddv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RATING_VECTOR_LENGTH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTnMLH5XFeDt",
        "outputId": "bd7b6f74-2776-4e1b-fb88-161f91ab87b6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "681805"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The size of the track_uri -> position mapping dictionary is {} bytes\".format(sys.getsizeof(track_uri_to_id)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWyQZJ5JrDjR",
        "outputId": "cfaaadd5-9d24-4591-a3a3-0806579fdb78"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the track_uri -> position mapping dictionary is 20971608 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GDOER74y5WEJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Old way of creating the rating vector dataframe\n",
        "# def _create_rating_df(playlist_row: Row, songs_df: DataFrame) -> DataFrame:\n",
        "#   \"\"\"\n",
        "#   Creates a dataframe that represents the \"ratings\" for a playlist in the dataframe\n",
        "#   \"\"\"\n",
        "#   playlist_row = spark.createDataFrame([playlist_row], playlist_schema)\n",
        "#   playlist_uris = get_all_songs(playlist_row)\n",
        "\n",
        "#   joined = songs_df.join(playlist_uris, on=\"track_uri\", how=\"right\")\n",
        "#   return joined\n",
        "\n",
        "\n",
        "# def _check_songs_ordering(playlist_row: DataFrame, songs_df: DataFrame) -> bool:\n",
        "#   \"\"\"\n",
        "#   Returns a boolean that indicates if the ordering in the songs_df and rating_df is the same\n",
        "#   \"\"\"\n",
        "#   playlist_row = spark.createDataFrame([playlist_row], playlist_schema)\n",
        "#   playlist_uris = get_all_songs(playlist_row, True).withColumnRenamed(\"in_playlist\", \"isin\")\n",
        "\n",
        "#   joined = songs_df.join(playlist_uris, on=\"track_uri\", how=\"right\")\n",
        "#   joined_left = songs_df.join(playlist_uris, on=\"track_uri\", how=\"left\").filter(\"isin == 1\")\n",
        "#   assert joined.collect() == joined_left.collect(), f\"The order of songs_df is different from the order of rating_df!\"\n",
        "\n",
        "# # def _extract_rating_vector(rating_df: DataFrame) -> SparseVector:\n",
        "# #   \"\"\"\n",
        "# #   Extracts the rating vectors for each playlist \n",
        "# #   \"\"\"\n",
        "# #   dense_vector = DenseVector([row.isin for row in rating_df.select(\"isin\").collect()])\n",
        "# #   return dense_to_sparse(dense_vector)\n",
        "\n",
        "# def _extrac_sparse_rating_vector(rating_df: DataFrame) -> SparseVector:\n",
        "#   indices = np.sort([row.pos for row in rating_df.collect()])\n",
        "#   return SparseVector(RATING_VECTOR_LENGTH, indices, np.ones(indices.shape[0]) )\n",
        "\n",
        "# def rating_vector_from_row(playlist_row: Row, songs_df: DataFrame):\n",
        "#   \"\"\"\n",
        "#   Pipelines togheter create_rating_df and extract_rating_vector.\n",
        "#   \"\"\"\n",
        "#   rating_df_1 = _create_rating_df(playlist_row, songs_df)\n",
        "#   rating_vector_1 = _extrac_sparse_rating_vector(rating_df_1)\n",
        "#   return rating_vector_1\n",
        "\n",
        "# # t1 = time.time() \n",
        "\n",
        "# # rating_vector_1 = rating_vector_from_row(slice_df.first(), songs_df)\n",
        "\n",
        "# # t2 = time.time()\n",
        "\n",
        "# # t2 - t1, rating_vector_1, type(rating_vector_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5Nhc0hwgDQ6X"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(vector_1: SparseVector, vector_2: SparseVector) -> float:\n",
        "  \"\"\"\n",
        "  Computes the Jaccard Similarity between two sparse binary vectors\n",
        "  \"\"\"\n",
        "  # Convert SparseVectors to sets\n",
        "  set1 = set(vector_1.indices)\n",
        "  set2 = set(vector_2.indices)\n",
        "\n",
        "  # Calculate the intersection and union of the sets\n",
        "  intersection = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "\n",
        "  # Calculate the similarity\n",
        "  similarity = intersection / union\n",
        "\n",
        "  return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yuDooTJ4_7Pz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Old way of creating the ratinf vector dataframe pt.2\n",
        "# def create_rating_vectors_df(playlists_df: DataFrame) -> DataFrame:\n",
        "#   rating_vectors = []\n",
        "\n",
        "#   for playlist_row in tqdm(playlists_df.collect(), desc=\"Creating rating vectors\"):\n",
        "#     rating_vector = rating_vector_from_row(playlist_row, songs_df)\n",
        "#     new_row = Row(playlist_id=playlist_row.pid, rating_vector=rating_vector)\n",
        "#     rating_vectors.append([new_row])\n",
        "#   return spark.createDataFrame(rating_vectors)\n",
        "\n",
        "# if os.path.exists(RATING_VECTOR_FILE_PATH):\n",
        "#   # rv_schema = StructType([StructField('playlist_id', LongType(), True), StructField('rating_vector', pyspark.ml.linalg.VectorUDT(), True)])\n",
        "#   rating_vectors_df = spark.read.parquet(RATING_VECTOR_FILE_PATH)\n",
        "#   rv_df = rating_vectors_df.select(col(\"_1.playlist_id\").alias(\"playlist_id\"), col(\"_1.rating_vector\").alias(\"rating_vector\"))\n",
        "# else:\n",
        "#   rating_vectors_df = create_rating_vectors_df(slice_df)\n",
        "#   rating_vectors_df.write.parquet(RATING_VECTOR_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a function that gets in input the playlist to continue, and returns a Dataframe that indicates its similarity with each other playlist in the dataset."
      ],
      "metadata": {
        "id": "dWMpoMHrpxxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mapped_slice_df.first()"
      ],
      "metadata": {
        "id": "PQYLZnm3pxGj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SGjEbkYTAI3h"
      },
      "outputs": [],
      "source": [
        "@timeit\n",
        "def create_similarity_df(input_vector: DataFrame, rating_vectors_df: DataFrame, similarityFunction: Callable) -> DataFrame:\n",
        "  rv_df_input = rating_vectors_df.crossJoin(input_vector)\n",
        "  similarity_udf = udf(similarityFunction, returnType='double')\n",
        "  result_df = rv_df_input.withColumn(\"similarity\", similarity_udf(rv_df_input[\"input_vector\"], rv_df_input[\"rating_vector\"]))\n",
        "  return result_df\n",
        "\n",
        "if DEBUG:\n",
        "  rv_df = mapped_slice_df.withColumnRenamed(\"tracks\", \"rating_vector\")\n",
        "  # TODO: Just for test, we take the first playlist as the playlist to be continued \n",
        "  first_playlist_vector = rv_df.limit(1).select(\"rating_vector\").withColumnRenamed(\"rating_vector\",\"input_vector\")\n",
        "  result_df = create_similarity_df(first_playlist_vector, rv_df, jaccard_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12CY_fuNFXKr"
      },
      "source": [
        "Curse of dimensionality! We can see that each playlist is very dissimilar from each other playlist.\n",
        "\n",
        "If we filter the playlists that have a strictly positive similarity with the input playlist, and order them by descending similarity, we can see that the name (that we assume is very informative for the content of the playlist) is very similar, meaning that the algorithm seems to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NrPRMP-rINkc"
      },
      "outputs": [],
      "source": [
        "# result_df.filter(\"similarity > 0\").orderBy(col(\"similarity\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to suggest some songs to continuate the input playlist, let's take the $k$ top most similar playlists"
      ],
      "metadata": {
        "id": "Ppy5-Y19swvG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wNoSOnIqCLm8"
      },
      "outputs": [],
      "source": [
        "@timeit\n",
        "def get_top_k_results(playlist_pid: int, similarity_df: DataFrame, k: int = 20) -> DataFrame:\n",
        "  return similarity_df.filter( (col('pid') != playlist_pid)).orderBy(col(\"similarity\").desc()).limit(k)\n",
        "\n",
        "if DEBUG:\n",
        "  first_playlist_pid = rv_df.limit(1).select(\"pid\").first().pid\n",
        "  top_k_results = get_top_k_results(first_playlist_pid, result_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top_k_results.show()"
      ],
      "metadata": {
        "id": "aajFVyNEs73j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.linalg import VectorUDT\n",
        "\n",
        "# def add_sparse_vectors(accumulator: SparseVector, vector: SparseVector, weight: float) -> SparseVector:\n",
        "#     accumulator_vec = accumulator.toArray() \n",
        "#     array_2 = vector.toArray() * weight\n",
        "\n",
        "#     summed_array = accumulator_vec + array_2\n",
        "\n",
        "#     values = [value for value in summed_array if value != 0]\n",
        "#     sorted_indices = [index for index, value in enumerate(summed_array) if value != 0]\n",
        "#     return SparseVector(accumulator_vec.size, sorted_indices, values)\n",
        "\n",
        "# @udf(returnType=VectorUDT())\n",
        "# def accumulate_sparse_vectors(accumulator, rating_vector, similarity):\n",
        "#     summed_vector = add_sparse_vectors(accumulator, rating_vector, similarity)\n",
        "#     return summed_vector\n",
        "\n",
        "# df = top_k_results.withColumn('accumulated_vector', accumulate_sparse_vectors(top_k_results[\"rating_vector\"], top_k_results[\"rating_vector\"], top_k_results['similarity']))"
      ],
      "metadata": {
        "id": "ePbkSVa3ONgo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to obtain a single embedding for all the $K$ top most similar playlists, that will be the rating vector. We can then pick the indices of the $n$ top greatest values form this vector, and those will be the $n$ songs that we will reccomend.\n",
        "\n",
        "In order to aggregate the $k$ embeddings into a single one, I decided to take an average, weighted by the similarity value."
      ],
      "metadata": {
        "id": "nGUtRgGutIgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timeit\n",
        "def get_input_rating_vector(similarity_df: DataFrame) -> SparseVector:\n",
        "  return similarity_df.limit(1).select(\"input_vector\").collect()[0].input_vector\n",
        "\n",
        "def row_to_sparse_vector(row: Row) -> SparseVector:\n",
        "    \"\"\"\n",
        "    Because of json serialization, the SparseVector is converted into a Row(indices=..., values=...),\n",
        "    this function converts it back to a pyspark.SparseVector with length RATING_VECTOR_LENGHT+1 as default.\n",
        "    \"\"\"\n",
        "    return SparseVector(RATING_VECTOR_LENGTH+1, row.indices, row.values)\n",
        "\n",
        "@timeit\n",
        "def accumulate_top_k_results(top_k_results: DataFrame, input_vector: np.ndarray) -> SparseVector:\n",
        "  \n",
        "  @udf(returnType=VectorUDT())\n",
        "  def sum_vector(sparse_vectors, similarities):\n",
        "    acc = np.zeros(sparse_vectors[0].size)\n",
        "    similarity_sum = 0\n",
        "    for sparse_vector, similarity in zip(sparse_vectors, similarities):\n",
        "      acc += (row_to_sparse_vector(sparse_vector).toArray() * similarity)\n",
        "      similarity_sum += similarity\n",
        "    acc /= similarity_sum\n",
        "\n",
        "    #If a song is present in the input playlist, don't consider it\n",
        "    acc -= (input_vector * acc)\n",
        "\n",
        "    dense = DenseVector(acc.tolist())\n",
        "    return dense_to_sparse(dense)\n",
        "\n",
        "\n",
        "  return top_k_results.agg(sum_vector(collect_list('rating_vector'), collect_list(\"similarity\")).alias('summed')).first()[0]\n",
        "\n",
        "if DEBUG:\n",
        "  input_vector = row_to_sparse_vector(get_input_rating_vector(result_df)).toArray()\n",
        "  accumulated_vector = accumulate_top_k_results(top_k_results, input_vector)"
      ],
      "metadata": {
        "id": "nDEnlGrCcOHp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reccomendations_schema = StructType([\n",
        "     StructField('pos', IntegerType(), True),\n",
        "     StructField('confidence', FloatType(), True),\n",
        "     ])\n",
        "\n",
        "@timeit\n",
        "def get_top_n_values(vector: SparseVector, n:int = 10) -> DataFrame:\n",
        "  elements = list(enumerate(vector.toArray()))\n",
        "  sorted_elements = sorted(elements, key=lambda x: x[1], reverse=True)\n",
        "  top_n_indices = [(index, confidence.item()) for index, confidence in sorted_elements[:n]]\n",
        "  return spark.createDataFrame(top_n_indices, schema=reccomendations_schema)\n",
        "\n",
        "if DEBUG:\n",
        "  top_n_reccomendations = get_top_n_values(accumulated_vector)"
      ],
      "metadata": {
        "id": "keTpUBIXTdHg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top_n_reccomendations"
      ],
      "metadata": {
        "id": "iE9TIsj5UGyM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_schema = StructType([\n",
        "     StructField('artist_name', StringType(), True),\n",
        "     StructField('track_uri', StringType(), True),\n",
        "     StructField('artist_uri', StringType(), True),\n",
        "     StructField('track_name', StringType(), True),\n",
        "     StructField('album_uri', StringType(), True),\n",
        "     StructField('duration_ms', LongType(), True),\n",
        "     StructField('album_name', StringType(), True),\n",
        "    StructField('confidence', FloatType(), True),\n",
        "     ])"
      ],
      "metadata": {
        "id": "F82HZEu6sZuY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "#TODO: For now this works, but it's very slow, and since this has to be executed online,\n",
        "# consider to directly embed the song information inside the dataframe when computing the songs\n",
        "# to recommend.\n",
        "@timeit\n",
        "def recommendation_song_info(recommendation: DataFrame, songs_info_df: DataFrame) -> DataFrame:\n",
        "  return recommendation.join(songs_info_df, \"pos\")\n",
        "\n",
        "if DEBUG:\n",
        "  songs_info = recommendation_song_info(top_n_reccomendations, songs_info_df)"
      ],
      "metadata": {
        "id": "ZJOyep5RUMGV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all togheter\n",
        "We now define a single function that will get a playlist in input and will reccomend $n$ songs."
      ],
      "metadata": {
        "id": "CFmlqOAttu32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: this now takes a playlist and extracts its PID, if a playlist is built from scratch the PID shouldn't be defined\n",
        "# A solution would be to pass the playlist row with the PID = Nan and then have a condition when extracting the PID. If Nan, ignore it\n",
        "@timeit\n",
        "def user_based_recommendation(playlist: DataFrame, \n",
        "                              mapped_slice_df: DataFrame, \n",
        "                              similarity_function: Callable, \n",
        "                              n:int = 50,\n",
        "                              k: int = 20) -> DataFrame:\n",
        "                              \n",
        "  rv_df = mapped_slice_df.withColumnRenamed(\"tracks\", \"rating_vector\") #TODO: Parse the rv_df before and then remove this\n",
        "  \n",
        "  #TODO: define the songs_df as input to the function\n",
        "  playlist_vector = map_track_df_to_pos(playlist, songs_df).select(\"tracks\").withColumnRenamed(\"tracks\", \"input_vector\")\n",
        "  similarity_df = create_similarity_df(playlist_vector, rv_df, jaccard_similarity)\n",
        "  top_k_results = get_top_k_results(playlist.first().pid, similarity_df, k=k)\n",
        "  input_vector = playlist_vector.select(\"input_vector\").first()[0].toArray()\n",
        "  accumulated_vector = accumulate_top_k_results(top_k_results, input_vector)\n",
        "  top_n_indices = get_top_n_values(accumulated_vector, n=n)\n",
        "  recommended_songs_info = recommendation_song_info(top_n_indices, songs_info_df)\n",
        "\n",
        "  return recommended_songs_info\n",
        "\n",
        "if DEBUG:\n",
        "  #Collect and createDataFrame because operations on limit(1) take as long as the entire slice_df, don't know why\n",
        "  playlist = spark.createDataFrame(slice_df.filter(\"pid == 1010\").limit(1).collect())\n",
        "  final_recommendation = user_based_recommendation(playlist, mapped_slice_df, jaccard_similarity, n=5)"
      ],
      "metadata": {
        "id": "jfiQn5_rt827"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Evaluation"
      ],
      "metadata": {
        "id": "eM7ErDxxt69B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split as sklearn_split\n",
        "\n",
        "def train_test_split(playlist: Row) -> Tuple[Row, Row]:\n",
        "    train_rows, test_rows = sklearn_split(playlist.tracks, random_state=42)\n",
        "\n",
        "    playlist_train =  Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=train_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "\n",
        "    playlist_test = Row(\n",
        "            name=playlist.name,\n",
        "            collaborative=playlist.collaborative,\n",
        "            pid=playlist.pid,\n",
        "            modified_at=playlist.modified_at,\n",
        "            num_tracks=playlist.num_tracks,\n",
        "            num_albums=playlist.num_albums,\n",
        "            num_followers=playlist.num_followers,\n",
        "            tracks=test_rows,\n",
        "            num_edits=playlist.num_edits,\n",
        "            duration_ms=playlist.duration_ms,\n",
        "            num_artists=playlist.num_artists,\n",
        "        )\n",
        "    \n",
        "    return playlist_train, playlist_test"
      ],
      "metadata": {
        "id": "pyILOf4Z9DcD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's divide the whole playlist dataset into train and test splits"
      ],
      "metadata": {
        "id": "vW8S9RYTogWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, struct\n",
        "import shutil\n",
        "\n",
        "def divide_whole_dataset(playlist_df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
        "  # Create a UDF to apply the divide_row function to each row\n",
        "  train_test_split_udf = udf(train_test_split, returnType=ArrayType(StructType(playlist_df.schema.fields)))\n",
        "\n",
        "  # Apply the divide_row UDF to each row of the DataFrame\n",
        "  divided_df = playlist_df.withColumn(\"divided\", train_test_split_udf(struct(*playlist_df.columns)))\n",
        "\n",
        "  # Split the divided column into two separate columns: train and test\n",
        "  train_test_df = divided_df.select(col('divided').getItem(0).alias('train'), col('divided').getItem(1).alias('test'))\n",
        "\n",
        "  # Split the train and test columns into separate DataFrames\n",
        "  train_df = train_test_df.select(\"train.*\")\n",
        "  test_df = train_test_df.select(\"test.*\")\n",
        "  return train_df, test_df\n",
        "\n",
        "TRAIN_DF_PATH = os.path.join(SAVED_DFS_PATH, \"train_df.json\")\n",
        "TEST_DF_PATH = os.path.join(SAVED_DFS_PATH, \"test_df.json\")\n",
        "\n",
        "if os.path.exists(TRAIN_DF_PATH) and os.path.exists(TEST_DF_PATH):\n",
        "  train_df = spark.read.json(TRAIN_DF_PATH)\n",
        "  test_df = spark.read.json(TEST_DF_PATH)\n",
        "else:\n",
        "  # In order to avoid [PATH_ALREADY_EXISTS] errors. \n",
        "  if os.path.exists(TRAIN_DF_PATH):\n",
        "    shutil.rmtree(TRAIN_DF_PATH)\n",
        "  if os.path.exists(TEST_DF_PATH):\n",
        "    shutil.rmtree(TEST_DF_PATH)\n",
        "\n",
        "  train_df, test_df = divide_whole_dataset(slice_df)\n",
        "  train_df.write.json(TRAIN_DF_PATH)\n",
        "  test_df.write.json(TEST_DF_PATH)"
      ],
      "metadata": {
        "id": "jy32R6JFwPB1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Super bug! Sometimes there there are duplicate playlists in the training set!!\n",
        "#TODO: Doing this, there are 97,000 playlists in the train_df, and not 100,000 WHAT\n",
        "\n",
        "# Fixed this somehow rerunning the code and overwriting the parquet files, IDK what happened kekw\n",
        "# # train_df, test_df = divide_whole_dataset(slice_df)\n",
        "# train_df.write.mode(\"overwrite\").parquet(TRAIN_DF_PATH)\n",
        "# test_df.write.mode(\"overwrite\").parquet(TEST_DF_PATH)"
      ],
      "metadata": {
        "id": "IkddwozuMdYu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "@timeit\n",
        "def precision_at_k(recommendations, ground_truth, num_of_recommendations):\n",
        "    \"\"\"\n",
        "    Calculates precision at k for the recommendations.\n",
        "    \"\"\"\n",
        "    recommended_relevant_tracks = recommendations.join(ground_truth, \"track_uri\").count() #this can be top_n_results.join in order to be more performant\n",
        "    precision = recommended_relevant_tracks / float(num_of_recommendations)\n",
        "    return precision\n",
        "\n",
        "# TODO: Implemented in 1 sec, Test it\n",
        "@timeit\n",
        "def normalized_discounted_gain(recommendations: DataFrame, ground_truth: DataFrame) -> float:\n",
        "    sorted_recs = recommendations.orderBy(F.desc(\"confidence\")).select(\"track_uri\").collect()\n",
        "\n",
        "    def dcg(scores: List[int]) -> float:\n",
        "        return sum([score / (F.log2(rank + 2)) for rank, score in enumerate(scores)])\n",
        "\n",
        "    rec_scores = [1 if row in sorted_recs else 0 for row in ground_truth]\n",
        "    max_dcg = dcg([1] * len(ground_truth))\n",
        "    ndcg = dcg(rec_scores) / max_dcg if max_dcg != 0 else 0.0\n",
        "\n",
        "    return ndcg"
      ],
      "metadata": {
        "id": "86dFBXK20uqu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@timeit\n",
        "def evaluate(pid: int) -> Tuple[DataFrame, float]:\n",
        "    playlist_train = train_df.filter(f\"pid == {pid}\")\n",
        "    playlist_test = test_df.filter(f\"pid == {pid}\")\n",
        "    ground_truth = playlist_test.select(explode(\"tracks\")).select(\"col.*\")\n",
        "    num_of_recommendations = ground_truth.count()\n",
        "    recommendations = user_based_recommendation(playlist_train, \n",
        "                                                mapped_slice_df, \n",
        "                                                jaccard_similarity, \n",
        "                                                n=num_of_recommendations,\n",
        "                                                k = 10)\n",
        "    precision = precision_at_k(recommendations, ground_truth, num_of_recommendations)\n",
        "    return playlist_train, playlist_test, recommendations, precision"
      ],
      "metadata": {
        "id": "J8kbN-2Emdmz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl_train, pl_test, reccomendations, precision = evaluate(3001)"
      ],
      "metadata": {
        "id": "oFeeuj3l7ro3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a858b0-ea5a-4eda-831a-64005aba9c69"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function map_track_df_to_pos Took 0.5207 seconds\n",
            "Function create_similarity_df Took 0.5590 seconds\n",
            "Function get_top_k_results Took 0.0807 seconds\n",
            "Function accumulate_top_k_results Took 88.2747 seconds\n",
            "Function get_top_n_values Took 0.2868 seconds\n",
            "Function recommendation_song_info Took 0.0404 seconds\n",
            "Function user_based_recommendation Took 95.4083 seconds\n",
            "Function precision_at_k Took 62.7779 seconds\n",
            "Function evaluate Took 160.6996 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pl_train.show(), pl_test.show()"
      ],
      "metadata": {
        "id": "O-REosqXd_GZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081193cc-d73f-4da6-abc1-76295d89a726"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+-----------+-----+----------+-----------+---------+-------------+----------+----+--------------------+\n",
            "|collaborative|duration_ms|modified_at| name|num_albums|num_artists|num_edits|num_followers|num_tracks| pid|              tracks|\n",
            "+-------------+-----------+-----------+-----+----------+-----------+---------+-------------+----------+----+--------------------+\n",
            "|        false|    6432550| 1465084800|Chill|        20|         13|        4|            1|        26|3001|[{New Amerykah Pa...|\n",
            "+-------------+-----------+-----------+-----+----------+-----------+---------+-------------+----------+----+--------------------+\n",
            "\n",
            "+-------------+-----------+-----------+-----+----------+-----------+---------+-------------+----------+----+--------------------+\n",
            "|collaborative|duration_ms|modified_at| name|num_albums|num_artists|num_edits|num_followers|num_tracks| pid|              tracks|\n",
            "+-------------+-----------+-----------+-----+----------+-----------+---------+-------------+----------+----+--------------------+\n",
            "|        false|    6432550| 1465084800|Chill|        20|         13|        4|            1|        26|3001|[{Is Your Love Bi...|\n",
            "+-------------+-----------+-----------+-----+----------+-----------+---------+-------------+----------+----+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pl_train.select(explode(\"tracks\")).select(\"col.*\").show(),\\\n",
        "pl_test.select(explode(\"tracks\")).select(\"col.*\").show()"
      ],
      "metadata": {
        "id": "_QKAcSRZKTyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fb0bf7-7f1d-4a5e-991a-11aa7d8de3b2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----------------+--------------------+-----------+---+--------------------+--------------------+\n",
            "|          album_name|           album_uri|      artist_name|          artist_uri|duration_ms|pos|          track_name|           track_uri|\n",
            "+--------------------+--------------------+-----------------+--------------------+-----------+---+--------------------+--------------------+\n",
            "|New Amerykah Part...|spotify:album:1MO...|      Erykah Badu|spotify:artist:7I...|     289720|  1|         Window Seat|spotify:track:74H...|\n",
            "|       Days & Nights|spotify:album:736...|            Daley|spotify:artist:13...|     239960| 23|      Alone Together|spotify:track:1TK...|\n",
            "|           Take Care|spotify:album:6X1...|            Drake|spotify:artist:3T...|     321080|  5|        The Real Her|spotify:track:74a...|\n",
            "|New Amerykah Part...|spotify:album:0Rq...|      Erykah Badu|spotify:artist:7I...|     320986|  2|               Honey|spotify:track:2BZ...|\n",
            "|  Cheers To The Fall|spotify:album:6Bl...|        Andra Day|spotify:artist:1c...|     253342| 12|             Rise Up|spotify:track:0tV...|\n",
            "|The Light Of The Sun|spotify:album:52U...|       Jill Scott|spotify:artist:6A...|     109493| 15|               Quick|spotify:track:37B...|\n",
            "|                Live|spotify:album:7Cg...|      Erykah Badu|spotify:artist:7I...|     221000|  3|Tyrone - Live Ver...|spotify:track:1MC...|\n",
            "|           Take Care|spotify:album:6X1...|            Drake|spotify:artist:3T...|     265120|  4|      Doing It Wrong|spotify:track:4eS...|\n",
            "|            Epiphany|spotify:album:4aI...|Chrisette Michele|spotify:artist:3Y...|     220826| 21|All I Ever Think ...|spotify:track:4KW...|\n",
            "|Who Is Jill Scott...|spotify:album:620...|       Jill Scott|spotify:artist:6A...|     285746| 17|He Loves Me (Lyze...|spotify:track:2Pz...|\n",
            "|                I Am|spotify:album:0Hi...|Chrisette Michele|spotify:artist:3Y...|     222960| 22|               Be Ok|spotify:track:2Pw...|\n",
            "|Who Is Jill Scott...|spotify:album:620...|       Jill Scott|spotify:artist:6A...|     255733| 18|             The Way|spotify:track:49a...|\n",
            "|Late Nights & Ear...|spotify:album:3l9...| Marsha Ambrosius|spotify:artist:46...|     243653| 25|Hope She Cheats O...|spotify:track:0oc...|\n",
            "|        Pieces Of Me|spotify:album:73X...|           Ledisi|spotify:artist:60...|     202733| 20|        Pieces Of Me|spotify:track:6aV...|\n",
            "|Speakerboxxx/The ...|spotify:album:1Us...|          OutKast|spotify:artist:1G...|     158093|  7|  Take Off Your Cool|spotify:track:2fa...|\n",
            "|               Birdy|spotify:album:1WG...|            Birdy|spotify:artist:2W...|     224866| 10|             Shelter|spotify:track:75H...|\n",
            "|The Light Of The Sun|spotify:album:52U...|       Jill Scott|spotify:artist:6A...|     138373| 14|     Some Other Time|spotify:track:4QF...|\n",
            "|           The Truth|spotify:album:1VD...|           Ledisi|spotify:artist:60...|     254506| 19|         I Blame You|spotify:track:7KK...|\n",
            "|If You're Reading...|spotify:album:0pt...|            Drake|spotify:artist:3T...|     320400|  6|              Jungle|spotify:track:7JX...|\n",
            "+--------------------+--------------------+-----------------+--------------------+-----------+---+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+----------------+--------------------+-----------+---+--------------------+--------------------+\n",
            "|          album_name|           album_uri|     artist_name|          artist_uri|duration_ms|pos|          track_name|           track_uri|\n",
            "+--------------------+--------------------+----------------+--------------------+-----------+---+--------------------+--------------------+\n",
            "|Is Your Love Big ...|spotify:album:520...| Lianne La Havas|spotify:artist:2R...|     267746|  8|        Lost & Found|spotify:track:6FZ...|\n",
            "|The Light Of The Sun|spotify:album:52U...|      Jill Scott|spotify:artist:6A...|     287293| 16|       Rolling Hills|spotify:track:0XH...|\n",
            "|But You Caint Use...|spotify:album:5q1...|     Erykah Badu|spotify:artist:7I...|     319958|  0|               Hello|spotify:track:6Aa...|\n",
            "|Late Nights & Ear...|spotify:album:3l9...|Marsha Ambrosius|spotify:artist:46...|     209573| 24|          Your Hands|spotify:track:5bc...|\n",
            "|Beyond the Lights...|spotify:album:17U...|   Cynthia Erivo|spotify:artist:46...|     183506| 11| Fly Before You Fall|spotify:track:4if...|\n",
            "|Is Your Love Big ...|spotify:album:520...| Lianne La Havas|spotify:artist:2R...|     264973|  9|                Gone|spotify:track:77G...|\n",
            "|                   Z|spotify:album:2qC...|             SZA|spotify:artist:7t...|     350911| 13|Warm Winds (feat....|spotify:track:0Kz...|\n",
            "+--------------------+--------------------+----------------+--------------------+-----------+---+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reccomendations.orderBy(col(\"confidence\").desc()).show() #TODO: Confidence has some strange values, check them out to see if they are correct"
      ],
      "metadata": {
        "id": "bPO3bbSERwkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bdfea1-bafe-4dcd-b5e7-6191678c7564"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+-------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+\n",
            "|   pos|confidence|  artist_name|           track_uri|          artist_uri|          track_name|           album_uri|duration_ms|          album_name|\n",
            "+------+----------+-------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+\n",
            "|221590|0.53681153|        Drake|spotify:track:047...|spotify:artist:3T...|        Marvins Room|spotify:album:6X1...|     347226|           Take Care|\n",
            "|221591| 0.4521853|        Drake|spotify:track:6Z0...|spotify:artist:3T...|         Shot For Me|spotify:album:6X1...|     224720|           Take Care|\n",
            "|537208|0.41606435|        Drake|spotify:track:7t1...|spotify:artist:3T...|Look What You've ...|spotify:album:6X1...|     301960|           Take Care|\n",
            "|319063|0.33143815|        Drake|spotify:track:0jF...|spotify:artist:3T...|            Practice|spotify:album:6X1...|     237733|           Take Care|\n",
            "|434245|0.26219386|        Drake|spotify:track:6cT...|spotify:artist:3T...|      Furthest Thing|spotify:album:2gX...|     267373|Nothing Was The Same|\n",
            "|321552| 0.2445849|PARTYNEXTDOOR|spotify:track:1wZ...|spotify:artist:2H...|Come and See Me (...|spotify:album:2FX...|     235477|PARTYNEXTDOOR 3 (P3)|\n",
            "|634764| 0.2445849|        Drake|spotify:track:6kI...|spotify:artist:3T...|            Too Much|spotify:album:2gX...|     261853|Nothing Was The Same|\n",
            "+------+----------+-------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision"
      ],
      "metadata": {
        "id": "MmoD37iEBxfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057b91bd-a32d-4355-db0f-2081921af7e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlejJ_MFrB9"
      },
      "source": [
        "# Fighting against the curse of dimensionality: Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnioMF6_KGfr"
      },
      "source": [
        "We want to define $\\mathbf{x}_u \\in \\mathbb{R}^d$ $d$-dimensional vector that represents the user $u$, and $\\mathbf{w}_i \\in \\mathbb{R}^d$ vector that represent the item $i$.\n",
        "\n",
        "We then can estimate the rating of user $u$ for the item $i$ by computing\n",
        "\\begin{equation}\n",
        "\\hat{r}_{u, i}=\\mathbf{x}_u^T \\cdot \\mathbf{w}_i=\\sum_{j=1}^d x_{u, j} w_{j, i}\n",
        "\\end{equation}\n",
        "Or, in matrix notation,\n",
        "\n",
        "\\begin{equation}\n",
        "\\underbrace{R}_{m \\times n} =\n",
        "\\underbrace{X}_{m \\times d}\n",
        "\\underbrace{W^T}_{d \\times n}\n",
        "\\end{equation}\n",
        "\n",
        "### How to learn $X$ and $W$\n",
        "The matrix $R$ is partially known and filled with the observations inside the dataset $\\mathcal{D}$. In order to learn the latent factor representations $X$ and $W$, we minimize the following loss function:\n",
        "\\begin{equation}\n",
        "L(X, W)=\\sum_{(u, i) \\in \\mathcal{D}}\\underbrace{\\left(r_{u, i}-\\mathbf{x}_u^T \\cdot \\mathbf{w}_i\\right)^2}_{\\text{squared error term}}+\\underbrace{\\lambda\\left(\\sum_{u \\in \\mathcal{D}}\\left\\|\\mathbf{x}_u\\right\\|^2+\\sum_{i \\in \\mathcal{D}}\\left\\|\\mathbf{w}_i\\right\\|^2\\right)}_{\\text{regularization term}}\n",
        "\\end{equation}\n",
        "\n",
        "We can then minimize the loss using Stochastic Gradient Descent or Alternating Least Squares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el7BsOEsjSgN"
      },
      "source": [
        "# Matrix Factorization\n",
        "Generate a matrix Y where each column represent a playlist and each row represent a song, the (i,j) entry will be 1 if the playlist contains the song, 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-KsENF4IfRks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d894a426-f959-4f3d-8163-0ec75893390c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-92255c027331>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Throw error in order to not execute the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Throw error in order to not execute the following code\n",
        "raise ValueError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1PEWA1Xjbb2"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import explode\n",
        "spark.conf.set(\"spark.sql.pivotMaxValues\", 1000000)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import expr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCEit6ZXgClg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode\n",
        "import random\n",
        "tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"pid\", \"track.track_uri\")\n",
        "tracks_df = tracks_df.withColumn(\"rating\", lit(1))\n",
        "# tracks_df = tracks_df.withColumn(\"rating\", (rand() * 10 + 1).cast(\"integer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzl4_KPigJyN"
      },
      "outputs": [],
      "source": [
        "tracks_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrNo-NacpGXC"
      },
      "outputs": [],
      "source": [
        "# # Explode the tracks array column into multiple rows\n",
        "# # tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\"))\n",
        "# # tracks_df = slice_df.select(\"pid\", \"tracks\", \"tracks\")\n",
        "# tracks_df = slice_df.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"pid\", \"track.track_uri\", \"track.pos\")\n",
        "\n",
        "# # Select relevant columns and add a rating column with value 1\n",
        "# playlist_track_df = tracks_df.withColumn(\"rating\", lit(1))\n",
        "\n",
        "# # Get distinct track_uri values and join with playlist_track_df\n",
        "# all_tracks_df = slice_df.select(explode(\"tracks\").alias(\"track\")).select(\"track.track_uri\").distinct()\n",
        "# all_playlists_df = slice_df.select(\"pid\").distinct()\n",
        "\n",
        "# all_against_all = all_tracks_df.join(all_playlists_df).distinct()\n",
        "\n",
        "# from pyspark.sql.functions import when, col\n",
        "\n",
        "# # playlist_track_rating_df = playlist_track_df.join(all_against_all, [\"pid\", \"track_uri\"], \"left_outer\") \\\n",
        "# #     .withColumn(\"rating\", when(col(\"pos\").isNull(), 0).otherwise(1))\n",
        "\n",
        "# playlist_track_rating_df = all_against_all.join(playlist_track_df, [\"pid\", \"track_uri\"], \"left_outer\") \\\n",
        "#     .withColumn(\"rating\", when(col(\"pos\").isNull(), 0).otherwise(1)) \\\n",
        "#     .drop(\"pos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_g71QpbzP9G"
      },
      "outputs": [],
      "source": [
        "playlist_track_rating_df = tracks_df.withColumn(\"song_id\", dense_rank().over(Window.orderBy(\"track_uri\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFR59iIwsuAv"
      },
      "outputs": [],
      "source": [
        "playlist_track_rating_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E86sNXYxTJP"
      },
      "outputs": [],
      "source": [
        "als = ALS(userCol=\"pid\", itemCol=\"song_id\", ratingCol=\"rating\", nonnegative=True, coldStartStrategy=\"drop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKxgkuM0GsVf"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "def train_test_split(df: DataFrame, split_ratio: float, seed: Optional[int] = None) -> Tuple[DataFrame, DataFrame]:\n",
        "  random.seed(seed)\n",
        "  distinct_pids = df.select(\"pid\").distinct().rdd.map(lambda x: x[0]).collect()\n",
        "  random.shuffle(distinct_pids)\n",
        "  split_index = int(len(distinct_pids) * split_ratio)\n",
        "  train_pids = distinct_pids[:split_index]\n",
        "  test_pids = distinct_pids[split_index:]\n",
        "  train_df = df.filter(col(\"pid\").isin(train_pids))\n",
        "  test_df = df.filter(col(\"pid\").isin(test_pids))\n",
        "  return train_df, test_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf82Hstmxb7i"
      },
      "outputs": [],
      "source": [
        "training, test = playlist_track_rating_df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r0g7uPzH19c"
      },
      "outputs": [],
      "source": [
        "model = als.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S3oNC7AIFXY"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwciStIaibF-"
      },
      "outputs": [],
      "source": [
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_FWXza9ycMU"
      },
      "outputs": [],
      "source": [
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJfWDUMSh8Nd"
      },
      "outputs": [],
      "source": [
        "predictions.filter(col(\"prediction\") != \"NaN\").count(), predictions.filter(col(\"prediction\") == \"NaN\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47oQcRQdm-qy"
      },
      "outputs": [],
      "source": [
        "rmse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = playlist_track_rating_df.select(\"pid\").distinct().limit(1)\n",
        "subUserRecs = model.recommendForUserSubset(subset, 10)"
      ],
      "metadata": {
        "id": "q2eoCHXqLNU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset.show()"
      ],
      "metadata": {
        "id": "Tya3FyOxLn0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subUserRecs.show(truncate=False)"
      ],
      "metadata": {
        "id": "u5lfCDhiLZjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def song_name_from_id(song_id: int, reverse_lookup: DataFrame) -> str:\n",
        "  return \n",
        "  \n",
        "def interpretRecommendation(recommended_result: DataFrame) -> str:\n",
        "  return"
      ],
      "metadata": {
        "id": "uyvBEVF8MP30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMvzQsXnykMX"
      },
      "outputs": [],
      "source": [
        "userRecs = model.recommendForAllUsers(1).orderBy(\"recommendations\")\n",
        "userRecs.show(truncate=False)\n",
        "userRecs.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKrGgg35mjmS"
      },
      "outputs": [],
      "source": [
        "slice_df.filter(col(\"pid\") == 1710).select(explode(\"tracks.track_name\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mF1ERfgmyFU"
      },
      "outputs": [],
      "source": [
        "track_uris = playlist_track_rating_df.filter(col(\"song_id\") == 588).select(\"track_uri\")\n",
        "track_uris.first()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SqlejJ_MFrB9",
        "el7BsOEsjSgN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}